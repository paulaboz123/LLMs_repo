import json
import logging
import os
import re
import math
from typing import Any, Dict, List, Tuple, Optional

import pandas as pd

try:
    from openai import AzureOpenAI
except Exception:
    AzureOpenAI = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("score")

# -------------------------
# Tunables (env)
# -------------------------
USE_MOCK = os.getenv("USE_MOCK", "0").strip() == "1"

MIN_CHUNK_LEN = int(os.getenv("MIN_CHUNK_LEN", "10"))
MAX_LABELS_PER_CHUNK = int(os.getenv("MAX_LABELS_PER_CHUNK", "3"))

# sentence extraction
MAX_SENTENCES_PER_CHUNK = int(os.getenv("MAX_SENTENCES_PER_CHUNK", "12"))
MAX_SENTENCES_TO_EVAL = int(os.getenv("MAX_SENTENCES_TO_EVAL", "10"))

# retrieval + LLM
RETRIEVE_TOPK = int(os.getenv("RETRIEVE_TOPK", "10"))
LABEL_MIN_PROB = float(os.getenv("LABEL_MIN_PROB", "0.75"))
HIGHLIGHT_MIN_PROB = float(os.getenv("HIGHLIGHT_MIN_PROB", "0.80"))
RELEVANCE_MIN = float(os.getenv("RELEVANCE_MIN", "0.60"))

# minimal filters (keep them simple)
MIN_WORDS_PER_SENTENCE = int(os.getenv("MIN_WORDS_PER_SENTENCE", "4"))
REJECT_TOC = os.getenv("REJECT_TOC", "1").strip() == "1"
REJECT_CAPTIONS = os.getenv("REJECT_CAPTIONS", "1").strip() == "1"

# LLM requirement gate is integrated in classifier prompt (single call)
USE_LLM = os.getenv("USE_LLM", "1").strip() == "1" and not USE_MOCK

MAX_DEBUG_CHUNKS = int(os.getenv("MAX_DEBUG_CHUNKS", "0"))

client = None
CHAT_DEPLOYMENT = None
EMBED_DEPLOYMENT = None

# -------------------------
# Demands cache
# -------------------------
DEMANDS: List[Dict[str, str]] = []
DEMAND_BY_ID: Dict[str, Dict[str, str]] = {}
DEMAND_TEXT_FOR_EMBED: Dict[str, str] = {}
DEMAND_EMB: Dict[str, List[float]] = {}

# -------------------------
# Minimal regex filters for TOC/captions
# -------------------------
TOC_DOTS_RE = re.compile(r"^\s*(\d+(\.\d+)*\s+)?[A-Za-z].{2,}\.{5,}\s*\d+\s*$")
TOC_SECTION_PAGE_RE = re.compile(r"^\s*\d+(\.\d+)+\s+[A-Za-z].{2,}\s+\d+\s*$")
TABLE_FIG_CAPTION_RE = re.compile(r"^\s*(table|fig\.?|figure)\s*\d+(\.\d+)*\b", re.IGNORECASE)
SECTION_HEADING_RE = re.compile(r"^\s*\d+(\.\d+)*\s+[A-Za-z][A-Za-z0-9 \-_/(),]{2,}\s*$")

REQ_MODAL_RE = re.compile(
    r"\b(shall|must|required|requirement|should|need to|has to|may not|must not|shall not)\b",
    re.IGNORECASE,
)
REQ_IMPERATIVE_RE = re.compile(
    r"^\s*(use|provide|ensure|verify|test|install|maintain|replace|include|apply|set|adjust|calibrate|limit)\b",
    re.IGNORECASE,
)
REQ_NUM_RE = re.compile(r"\b\d+([.,]\d+)?\b")
UNIT_RE = re.compile(r"\b(mm|cm|m|kg|w|kw|mw|v|kv|a|ma|hz|rpm|bar|pa|degc|Â°c|c|ip\d{2})\b", re.IGNORECASE)


def _json_load_maybe(x: Any) -> Any:
    if isinstance(x, (bytes, bytearray)):
        x = x.decode("utf-8", errors="replace")
    if isinstance(x, str):
        s = x.strip()
        if not s:
            return None
        try:
            return json.loads(s)
        except Exception:
            return x
    return x


def _find_content_byid(document: Dict[str, Any]) -> Dict[str, Any]:
    cd = document.get("contentDomain", {})
    by_id = cd.get("byId", {})
    return by_id if isinstance(by_id, dict) else {}


def _load_demands_xlsx() -> List[Dict[str, str]]:
    here = os.path.dirname(os.path.abspath(__file__))
    candidates = [
        os.path.join(here, "demands.xlsx"),
        os.path.join(here, "assets", "demands.xlsx"),
    ]
    model_dir = os.getenv("AZUREML_MODEL_DIR")
    if model_dir:
        candidates += [
            os.path.join(model_dir, "demands.xlsx"),
            os.path.join(model_dir, "assets", "demands.xlsx"),
        ]

    for p in candidates:
        if os.path.exists(p):
            df = pd.read_excel(p)
            df.columns = [c.strip().lower() for c in df.columns]
            if "demand_id" not in df.columns and "id" in df.columns:
                df = df.rename(columns={"id": "demand_id"})
            if "description" not in df.columns and "demand_description" in df.columns:
                df = df.rename(columns={"demand_description": "description"})

            required = {"demand_id", "demand", "description"}
            missing = required - set(df.columns)
            if missing:
                raise RuntimeError(f"demands.xlsx missing columns: {sorted(list(missing))}. Found: {list(df.columns)}")

            out: List[Dict[str, str]] = []
            for _, r in df.fillna("").iterrows():
                did = str(r["demand_id"]).strip()
                if not did or did.lower() == "nan":
                    continue
                out.append(
                    {"id": did, "demand": str(r["demand"]).strip(), "description": str(r["description"]).strip()}
                )
            logger.info("Loaded %d demands from %s", len(out), p)
            return out

    raise RuntimeError(f"demands.xlsx not found. Tried: {candidates}")


def _word_count(s: str) -> int:
    return len([w for w in re.findall(r"[A-Za-z0-9]+", s or "") if w.strip()])


def _is_toc_line(s: str) -> bool:
    if not REJECT_TOC:
        return False
    t = (s or "").strip()
    if len(t) < 6:
        return False
    if TOC_DOTS_RE.match(t) or TOC_SECTION_PAGE_RE.match(t):
        return True
    if re.search(r"\.{5,}", t) and re.search(r"\d+\s*$", t):
        return True
    return False


def _is_caption_or_heading(s: str) -> bool:
    if not REJECT_CAPTIONS:
        return False
    t = (s or "").strip()
    if not t:
        return True

    # keep captions only if they look like requirement (rare)
    if (REQ_MODAL_RE.search(t) or REQ_IMPERATIVE_RE.search(t)) and (REQ_NUM_RE.search(t) or UNIT_RE.search(t)):
        return False

    if TABLE_FIG_CAPTION_RE.match(t):
        return True
    if SECTION_HEADING_RE.match(t) and _word_count(t) <= 12:
        return True
    return False


def _should_reject_sentence(s: str) -> bool:
    t = (s or "").strip()
    if not t:
        return True
    if _word_count(t) < MIN_WORDS_PER_SENTENCE:
        return True
    if _is_toc_line(t):
        return True
    if _is_caption_or_heading(t):
        return True
    return False


def _split_into_sentences_or_lines(chunk_text: str) -> List[str]:
    t = (chunk_text or "").strip()
    if not t:
        return []
    # keep simple
    parts = re.split(r"(?<=[.!?])\s+|\n+", t)
    parts = [p.strip() for p in parts if p and p.strip()]
    return parts[:MAX_SENTENCES_PER_CHUNK]


def _is_requirement_fast(text: str) -> Tuple[bool, float]:
    t = (text or "").strip()
    if len(t) < MIN_CHUNK_LEN:
        return False, 0.0
    has_modal = bool(REQ_MODAL_RE.search(t))
    has_imp = bool(REQ_IMPERATIVE_RE.search(t))
    has_num_unit = bool(REQ_NUM_RE.search(t) and UNIT_RE.search(t))
    if (has_modal or has_imp) and has_num_unit:
        return True, 0.86
    if has_modal or has_imp:
        return True, 0.72
    if has_num_unit:
        return True, 0.62
    return False, 0.25


def _cosine(a: List[float], b: List[float]) -> float:
    # assumes non-empty
    dot = 0.0
    na = 0.0
    nb = 0.0
    for x, y in zip(a, b):
        dot += x * y
        na += x * x
        nb += y * y
    if na <= 0.0 or nb <= 0.0:
        return 0.0
    return dot / (math.sqrt(na) * math.sqrt(nb))


def _embed(text: str) -> Optional[List[float]]:
    if client is None or not EMBED_DEPLOYMENT:
        return None
    try:
        resp = client.embeddings.create(model=EMBED_DEPLOYMENT, input=text)
        return list(resp.data[0].embedding)
    except Exception:
        return None


def _build_demand_text_for_embed(demand: str, desc: str) -> str:
    # IMPORTANT: ignore "CST" as a signal; keep only meaningful parts
    # We do not assume fixed format; we simply pass the demand+desc as is,
    # embeddings handle casing.
    demand = (demand or "").strip()
    desc = (desc or "").strip()
    return f"Label: {demand}\nDescription: {desc}".strip()


def _build_indexes():
    global DEMAND_BY_ID, DEMAND_TEXT_FOR_EMBED, DEMAND_EMB

    DEMAND_BY_ID = {d["id"]: d for d in DEMANDS}
    DEMAND_TEXT_FOR_EMBED = {}
    DEMAND_EMB = {}

    # prepare texts
    for d in DEMANDS:
        did = d["id"]
        DEMAND_TEXT_FOR_EMBED[did] = _build_demand_text_for_embed(d.get("demand", ""), d.get("description", ""))

    # precompute embeddings if available
    if client is None or not EMBED_DEPLOYMENT:
        logger.info("Embeddings disabled (missing client or EMBED_DEPLOYMENT). Will use lexical fallback.")
        return

    for did, txt in DEMAND_TEXT_FOR_EMBED.items():
        emb = _embed(txt)
        if emb:
            DEMAND_EMB[did] = emb

    logger.info("Prepared demand embeddings: %d/%d", len(DEMAND_EMB), len(DEMAND_TEXT_FOR_EMBED))


def _retrieve_candidates(sentence: str, topk: int) -> List[Tuple[str, float]]:
    """
    Returns list of (demand_id, similarity_or_score) sorted desc.
    Uses embeddings if available; else uses trivial lexical fallback.
    """
    s = (sentence or "").strip()
    if not s:
        return []

    s_emb = _embed(s)
    if s_emb and DEMAND_EMB:
        sims = []
        for did, demb in DEMAND_EMB.items():
            sims.append((did, _cosine(s_emb, demb)))
        sims.sort(key=lambda x: x[1], reverse=True)
        return sims[:max(1, topk)]

    # lexical fallback: count shared keywords
    toks = set(re.findall(r"[A-Za-z]{3,}", s.lower()))
    if not toks:
        return []
    scored = []
    for did, txt in DEMAND_TEXT_FOR_EMBED.items():
        dtoks = set(re.findall(r"[A-Za-z]{3,}", txt.lower()))
        overlap = len(toks & dtoks)
        if overlap >= 2:
            scored.append((did, float(overlap)))
    scored.sort(key=lambda x: x[1], reverse=True)
    return scored[:max(1, topk)]


def _llm_classify(sentence: str, candidates: List[Tuple[str, float]]) -> Dict[str, Any]:
    """
    One call:
      - isRequirement
      - relevance
      - demandIds with probabilities (multi-label allowed)
    """
    is_req_fast, rel_fast = _is_requirement_fast(sentence)
    if client is None or not CHAT_DEPLOYMENT or not USE_LLM:
        # fallback: convert retrieval scores to probs
        out = []
        if candidates:
            best = candidates[0][1]
            for did, sc in candidates[:MAX_LABELS_PER_CHUNK]:
                # map similarity ~[0..1] to prob roughly
                p = 0.55 + 0.40 * max(0.0, min(1.0, (sc if sc <= 1.0 else sc / max(1.0, best))))
                p = min(PROB_CAP, p)
                if p >= LABEL_MIN_PROB:
                    out.append({"id": did, "probability": float(p)})
        rel = max(rel_fast, out[0]["probability"] if out else 0.0)
        return {"isRequirement": bool(is_req_fast), "relevance": float(rel), "labels": out, "notes": "fallback_no_llm"}

    # build short list text
    cand_lines = []
    for i, (did, sim) in enumerate(candidates[:RETRIEVE_TOPK], start=1):
        d = DEMAND_BY_ID.get(did, {})
        cand_lines.append(
            f"{i}. id={did}\n"
            f"   demand={d.get('demand','')}\n"
            f"   description={d.get('description','')}\n"
        )
    cand_block = "\n".join(cand_lines).strip()

    system = "Return STRICT JSON only. No prose."
    user = f"""
You classify ONE sentence from a technical specification into 0..{MAX_LABELS_PER_CHUNK} demands.
You MUST use the candidate list only (do not invent ids).
A sentence can match multiple demands (multi-label).

Return JSON only:
{{
  "isRequirement": true|false,
  "relevance": 0.0,
  "demandIds": [{{"id":"...","probability":0.0}}]
}}

Rules:
- If sentence is a heading/caption/table title (e.g. 'Table 2.1 ...', section title) and has no explicit requirement language, set isRequirement=false and demandIds=[].
- Prefer semantic meaning over keyword overlap.
- If two demands both apply, include both.
- Probabilities should reflect confidence, 0..1. Do not set all to 1.0.
- If not a requirement, keep relevance < {RELEVANCE_MIN} and return empty demandIds.

SENTENCE:
{sentence}

CANDIDATES:
{cand_block}
""".strip()

    try:
        resp = client.chat.completions.create(
            model=CHAT_DEPLOYMENT,
            messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
            temperature=0.0,
            max_tokens=450,
        )
        raw = (resp.choices[0].message.content or "").strip()
        data = json.loads(raw)

        is_req = bool(data.get("isRequirement", False))
        rel = float(data.get("relevance", 0.0) or 0.0)
        dem = data.get("demandIds", []) or []

        labels = []
        for x in dem:
            did = str(x.get("id", "")).strip()
            p = float(x.get("probability", 0.0) or 0.0)
            if did and did in DEMAND_BY_ID:
                p = min(PROB_CAP, max(0.0, p))
                labels.append({"id": did, "probability": p})

        labels.sort(key=lambda z: z["probability"], reverse=True)
        labels = labels[:MAX_LABELS_PER_CHUNK]

        # enforce min prob
        labels = [lb for lb in labels if lb["probability"] >= LABEL_MIN_PROB]

        # if model says not requirement, do not return labels
        if not is_req:
            labels = []

        rel_out = max(rel, labels[0]["probability"] if labels else 0.0)
        return {"isRequirement": is_req, "relevance": float(rel_out), "labels": labels, "notes": "ok_llm"}

    except Exception:
        # hard fallback
        return _llm_classify(sentence, candidates=[])


# -------------------------
# AML entrypoints
# -------------------------
def init():
    global client, CHAT_DEPLOYMENT, EMBED_DEPLOYMENT, DEMANDS, USE_MOCK

    USE_MOCK = os.getenv("USE_MOCK", "0").strip() == "1"
    DEMANDS = _load_demands_xlsx()

    if not USE_MOCK and AzureOpenAI is not None:
        aoai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "").strip()
        aoai_key = os.getenv("AZURE_OPENAI_API_KEY", "").strip()
        aoai_version = os.getenv("AZURE_OPENAI_API_VERSION", "").strip() or "2024-06-01"

        CHAT_DEPLOYMENT = (
            os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT", "").strip()
            or os.getenv("AZURE_OPENAI_DEPLOYMENT", "").strip()
        )
        EMBED_DEPLOYMENT = os.getenv("AZURE_OPENAI_EMBED_DEPLOYMENT", "").strip()

        if aoai_endpoint and aoai_key:
            client = AzureOpenAI(azure_endpoint=aoai_endpoint, api_key=aoai_key, api_version=aoai_version)

    _build_indexes()

    logger.info(
        "init(): USE_MOCK=%s USE_LLM=%s demands=%d chat=%s embed=%s",
        USE_MOCK, USE_LLM, len(DEMANDS), str(bool(CHAT_DEPLOYMENT)), str(bool(EMBED_DEPLOYMENT))
    )


def run(raw_data):
    """
    Contract (do not break):
      { "predictions": <document> }

    document MUST include:
      documentDemandPredictions: ["id1","id2", ...]  (ARRAY OF STRINGS)

    per chunk:
      relevantProba: float
      cdTransformerPredictions: [{label, proba}]
      cdLogregPredictions: SAME SHAPE (mirror)
      highlightText: string
    """
    try:
        req = _json_load_maybe(raw_data)
        if not isinstance(req, dict):
            return {"error": "Bad request: body must be JSON object", "predictions": {"documentDemandPredictions": []}}

        if "document" not in req:
            return {"error": "Bad request: missing 'document'", "predictions": {"documentDemandPredictions": []}}

        document = _json_load_maybe(req["document"])
        if not isinstance(document, dict):
            return {"error": "Bad request: 'document' must be JSON object", "predictions": {"documentDemandPredictions": []}}

        num_preds = int(req.get("num_preds", MAX_LABELS_PER_CHUNK))
        num_preds = max(1, min(10, num_preds))
        num_preds = min(num_preds, MAX_LABELS_PER_CHUNK)

        by_id = _find_content_byid(document)

        best_doc_probs: Dict[str, float] = {}
        debug_left = MAX_DEBUG_CHUNKS

        for cid, content in by_id.items():
            if not isinstance(content, dict):
                continue

            chunk_text = (str(content.get("text", "") or "")).strip()

            # Set expected keys
            content["relevantProba"] = 0.0
            content["cdTransformerPredictions"] = []
            content["cdLogregPredictions"] = []
            content["highlightText"] = ""
            content["labelExplanations"] = []

            if debug_left > 0:
                logger.info("chunk=%s sample=%r", cid, chunk_text[:180])
                debug_left -= 1

            sentences_raw = _split_into_sentences_or_lines(chunk_text)
            if not sentences_raw:
                continue

            sentences = [s for s in sentences_raw if not _should_reject_sentence(s)]
            if not sentences:
                continue

            # pick best sentence in chunk by (LLM relevance * best label prob)
            best_metric = -1.0
            best_sentence = ""
            best_labels = []
            best_rel = 0.0
            best_is_req = False

            # Evaluate a few sentences (simple: prefer longer and requirement-ish)
            scored_sents: List[Tuple[float, str]] = []
            for s in sentences:
                ss = s.strip()
                if len(ss) < MIN_CHUNK_LEN:
                    continue
                is_req, rel_fast = _is_requirement_fast(ss)
                wc = _word_count(ss)
                len_bonus = 0.0 if wc < 8 else (0.05 if wc < 16 else 0.08)
                score = (0.40 * rel_fast) + (0.03 if is_req else 0.0) + len_bonus
                scored_sents.append((score, ss))
            scored_sents.sort(key=lambda x: x[0], reverse=True)
            to_eval = [s for _, s in scored_sents[:MAX_SENTENCES_TO_EVAL]]

            for s in to_eval:
                cands = _retrieve_candidates(s, RETRIEVE_TOPK)
                pred = _llm_classify(s, cands)

                labels = pred.get("labels", []) or []
                if not labels:
                    continue

                top1 = float(labels[0]["probability"])
                rel = float(pred.get("relevance", 0.0) or 0.0)
                is_req = bool(pred.get("isRequirement", False))

                metric = top1 * (1.0 + 0.25 * rel) * (1.10 if is_req else 1.0)
                if metric > best_metric:
                    best_metric = metric
                    best_sentence = s
                    best_labels = labels
                    best_rel = rel
                    best_is_req = is_req

            if not best_labels:
                continue

            top1 = float(best_labels[0]["probability"])
            should_highlight = (top1 >= HIGHLIGHT_MIN_PROB) and bool(best_is_req)

            if not should_highlight:
                continue

            final_labels = [{"label": lb["id"], "proba": float(lb["probability"])} for lb in best_labels[:num_preds]]

            content["relevantProba"] = float(max(best_rel, top1))
            content["cdTransformerPredictions"] = final_labels
            content["cdLogregPredictions"] = list(final_labels)
            content["highlightText"] = best_sentence

            # explanations (simple)
            expls = []
            for p in final_labels:
                did = p["label"]
                d = DEMAND_BY_ID.get(did, {})
                expls.append({"label": did, "explanation": f"Matched using description: {d.get('description','')[:160]}".strip()})
            content["labelExplanations"] = expls

            for p in final_labels:
                did = p["label"]
                prob = float(p["proba"])
                if did not in best_doc_probs or prob > best_doc_probs[did]:
                    best_doc_probs[did] = prob

        ids_sorted = [k for k, v in sorted(best_doc_probs.items(), key=lambda kv: kv[1], reverse=True)]
        document["documentDemandPredictions"] = ids_sorted[:num_preds]
        return {"predictions": document}

    except Exception as e:
        logger.exception("run() failed")
        return {"error": str(e), "predictions": {"documentDemandPredictions": []}}
