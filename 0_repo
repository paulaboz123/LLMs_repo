import json
import logging
import os
import re
import math
from typing import Any, Dict, List, Tuple, Optional

import pandas as pd

try:
    from openai import AzureOpenAI
except Exception:
    AzureOpenAI = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("score")

# -------------------------
# Tunables (env)
# -------------------------
USE_MOCK = os.getenv("USE_MOCK", "0").strip() == "1"

MIN_CHUNK_LEN = int(os.getenv("MIN_CHUNK_LEN", "10"))
MAX_LABELS_PER_CHUNK = int(os.getenv("MAX_LABELS_PER_CHUNK", "3"))

# Sentence selection in chunk
MAX_SENTENCES_PER_CHUNK = int(os.getenv("MAX_SENTENCES_PER_CHUNK", "12"))
MAX_SENTENCES_TO_EVAL = int(os.getenv("MAX_SENTENCES_TO_EVAL", "10"))

# thresholds
LABEL_MIN_PROB = float(os.getenv("LABEL_MIN_PROB", "0.75"))
HIGHLIGHT_MIN_PROB = float(os.getenv("HIGHLIGHT_MIN_PROB", "0.80"))
RELEVANCE_MIN = float(os.getenv("RELEVANCE_MIN", "0.60"))

# content filters
MIN_WORDS_PER_SENTENCE = int(os.getenv("MIN_WORDS_PER_SENTENCE", "4"))
REJECT_TOC = os.getenv("REJECT_TOC", "1").strip() == "1"
REJECT_PAGE_ARTIFACTS = os.getenv("REJECT_PAGE_ARTIFACTS", "1").strip() == "1"
REJECT_HEADINGS = os.getenv("REJECT_HEADINGS", "1").strip() == "1"
REJECT_TABLE_CAPTIONS = os.getenv("REJECT_TABLE_CAPTIONS", "1").strip() == "1"

# Optional LLM requirement-gate (OFF by default)
USE_LLM_GATE = os.getenv("USE_LLM_GATE", "0").strip() == "1" and not USE_MOCK

# topic-first behavior
TOPIC_FIRST = os.getenv("TOPIC_FIRST", "1").strip() == "1"
TOPIC_MIN_HITS = int(os.getenv("TOPIC_MIN_HITS", "1"))  # how many topic matches needed to activate topic-only candidate set

# scoring weights
W_TOPIC_HIT = float(os.getenv("W_TOPIC_HIT", "8.0"))  # strong anchor
W_DESC_OVERLAP = float(os.getenv("W_DESC_OVERLAP", "3.0"))
W_NAME_OVERLAP = float(os.getenv("W_NAME_OVERLAP", "1.0"))  # lower than desc
W_ASSET_HIT = float(os.getenv("W_ASSET_HIT", "0.5"))

# probability calibration (multi-label)
# Probability from score: p = 1 - exp(-score / SCORE_SCALE)
SCORE_SCALE = float(os.getenv("SCORE_SCALE", "8.0"))
PROB_CAP = float(os.getenv("PROB_CAP", "0.95"))

# highlight fallback
ALLOW_HIGHLIGHT_WITH_STRONG_MATCH = os.getenv("ALLOW_HIGHLIGHT_WITH_STRONG_MATCH", "1").strip() == "1"
STRONG_MATCH_PROB = float(os.getenv("STRONG_MATCH_PROB", "0.88"))
STRONG_MATCH_DESC_OVERLAP = int(os.getenv("STRONG_MATCH_DESC_OVERLAP", "2"))

MAX_DEBUG_CHUNKS = int(os.getenv("MAX_DEBUG_CHUNKS", "0"))

client = None
CHAT_DEPLOYMENT = None

# -------------------------
# Demands cache
# -------------------------
DEMANDS: List[Dict[str, str]] = []

DEMAND_NAME_RAW: Dict[str, str] = {}
DEMAND_DESC_RAW: Dict[str, str] = {}

DEMAND_NAME_TOKS: Dict[str, set] = {}
DEMAND_DESC_TOKS: Dict[str, set] = {}
DEMAND_ALL_TOKS: Dict[str, set] = {}

# hierarchy: asset + topic (ignore CST/prefix)
DEMAND_ASSET: Dict[str, str] = {}  # motor|pump|""
DEMAND_TOPIC: Dict[str, str] = {}  # e.g., drawings, vibration, vendor, painting
TOPIC_TO_DEMANDS: Dict[str, List[str]] = {}  # topic -> [demand ids]

# IDF
TOKEN_DF: Dict[str, int] = {}
TOKEN_IDF: Dict[str, float] = {}

# -------------------------
# Stopwords (minimal EN)
# -------------------------
STOPWORDS = {
    "the", "and", "or", "to", "of", "in", "on", "for", "with", "a", "an", "as", "at", "by",
    "from", "into", "over", "under", "than", "then", "that", "this", "these", "those",
    "be", "is", "are", "was", "were", "been", "being",
    "it", "its", "they", "them", "their", "we", "our", "you", "your",
    "can", "could", "may", "might", "will", "would",
    "not", "no", "yes",
}

SHORT_TOKEN_WHITELIST = {"ip", "hz", "kw", "mw", "mm", "cm", "kg", "pa", "bar", "rpm", "°c", "dc", "ac", "od", "id"}

# -------------------------
# Requirement heuristics
# -------------------------
REQ_MODAL_RE = re.compile(
    r"\b(shall|must|required|requirement|should|need to|has to|may not|must not|shall not|prohibit|forbidden|recommended|recommend)\b",
    re.IGNORECASE,
)
REQ_IMPERATIVE_RE = re.compile(
    r"^\s*(use|provide|ensure|verify|test|install|maintain|replace|protect|include|apply|set|adjust|calibrate|measure|limit)\b",
    re.IGNORECASE,
)
REQ_COND_RE = re.compile(r"\b(if|when|unless|in case)\b", re.IGNORECASE)

REQ_NUM_RE = re.compile(r"\b\d+([.,]\d+)?\b")
UNIT_RE = re.compile(r"\b(mm|cm|m|kg|w|kw|mw|v|kv|a|ma|hz|rpm|bar|pa|degc|°c|c|ip\d{2})\b", re.IGNORECASE)

# -------------------------
# TOC / artifacts / headings
# -------------------------
TOC_DOTS_RE = re.compile(r"^\s*(\d+(\.\d+)*\s+)?[A-Za-z][A-Za-z0-9 \-_/(),]{2,}\s*\.{5,}\s*\d+\s*$")
TOC_SECTION_PAGE_RE = re.compile(r"^\s*\d+(\.\d+)+\s+[A-Za-z].{2,}\s+\d+\s*$")

PAGE_ONLY_RE = re.compile(r"^\s*\d+\s*$")
PAGE_DASH_RE = re.compile(r"^\s*[-–—]+\s*\d+\s*[-–—]+\s*$")
PAGE_WORD_RE = re.compile(r"^\s*(page)\s*\d+\s*$", re.IGNORECASE)
PAGE_FRACTION_RE = re.compile(r"^\s*\d+\s*/\s*\d+\s*$")

TABLE_FIG_CAPTION_RE = re.compile(r"^\s*(table|fig\.?|figure)\s*\d+(\.\d+)*\b", re.IGNORECASE)
SECTION_HEADING_RE = re.compile(r"^\s*\d+(\.\d+)*\s+[A-Za-z][A-Za-z0-9 \-_/(),]{2,}\s*$")
ANNEX_HEADING_RE = re.compile(r"^\s*(annex|appendix)\s+[A-Za-z0-9]+\b", re.IGNORECASE)
TITLE_LIKE_RE = re.compile(r"^[A-Za-z0-9][A-Za-z0-9 \-_/(),]{0,80}$")

# robust dash splitter for demand strings
_DASH_SPLIT_RE = re.compile(r"\s*[-–—]\s*")


def _json_load_maybe(x: Any) -> Any:
    if isinstance(x, (bytes, bytearray)):
        x = x.decode("utf-8", errors="replace")
    if isinstance(x, str):
        s = x.strip()
        if not s:
            return None
        try:
            return json.loads(s)
        except Exception:
            return x
    return x


def _find_content_byid(document: Dict[str, Any]) -> Dict[str, Any]:
    cd = document.get("contentDomain", {})
    by_id = cd.get("byId", {})
    return by_id if isinstance(by_id, dict) else {}


def _load_demands_xlsx() -> List[Dict[str, str]]:
    here = os.path.dirname(os.path.abspath(__file__))
    candidates = [
        os.path.join(here, "demands.xlsx"),
        os.path.join(here, "assets", "demands.xlsx"),
    ]
    model_dir = os.getenv("AZUREML_MODEL_DIR")
    if model_dir:
        candidates += [
            os.path.join(model_dir, "demands.xlsx"),
            os.path.join(model_dir, "assets", "demands.xlsx"),
        ]

    for p in candidates:
        if os.path.exists(p):
            df = pd.read_excel(p)
            df.columns = [c.strip().lower() for c in df.columns]
            if "demand_id" not in df.columns and "id" in df.columns:
                df = df.rename(columns={"id": "demand_id"})
            if "description" not in df.columns and "demand_description" in df.columns:
                df = df.rename(columns={"demand_description": "description"})

            required = {"demand_id", "demand", "description"}
            missing = required - set(df.columns)
            if missing:
                raise RuntimeError(f"demands.xlsx missing columns: {sorted(list(missing))}. Found: {list(df.columns)}")

            out: List[Dict[str, str]] = []
            for _, r in df.fillna("").iterrows():
                did = str(r["demand_id"]).strip()
                if not did or did.lower() == "nan":
                    continue
                out.append({"id": did, "demand": str(r["demand"]).strip(), "description": str(r["description"]).strip()})
            logger.info("Loaded %d demands from %s", len(out), p)
            return out

    raise RuntimeError(f"demands.xlsx not found. Tried: {candidates}")


def _tokenize(s: str) -> List[str]:
    s = (s or "").lower()
    s = re.sub(r"[^a-z0-9°]+", " ", s)
    toks = []
    for t in s.split():
        if len(t) < 3 and t not in SHORT_TOKEN_WHITELIST:
            continue
        if t in STOPWORDS:
            continue
        toks.append(t)
    return toks


def _tokset(s: str) -> set:
    return set(_tokenize(s))


def _word_count(s: str) -> int:
    return len([w for w in re.findall(r"[A-Za-z0-9]+", s or "") if w.strip()])


def _is_toc_line(s: str) -> bool:
    if not REJECT_TOC:
        return False
    t = (s or "").strip()
    if len(t) < 6:
        return False
    if TOC_DOTS_RE.match(t):
        return True
    if TOC_SECTION_PAGE_RE.match(t):
        return True
    if re.search(r"\.{5,}", t) and re.search(r"\d+\s*$", t):
        return True
    return False


def _is_page_artifact(s: str) -> bool:
    if not REJECT_PAGE_ARTIFACTS:
        return False
    t = (s or "").strip()
    if not t:
        return True
    if PAGE_ONLY_RE.match(t) or PAGE_DASH_RE.match(t) or PAGE_WORD_RE.match(t) or PAGE_FRACTION_RE.match(t):
        return True
    return False


def _is_caption_or_heading(s: str) -> bool:
    if not REJECT_HEADINGS:
        return False

    t = (s or "").strip()
    if not t:
        return True

    # If explicit requirement language exists, keep it
    if REQ_MODAL_RE.search(t) or REQ_IMPERATIVE_RE.search(t):
        return False

    if REJECT_TABLE_CAPTIONS and TABLE_FIG_CAPTION_RE.match(t):
        return True

    if SECTION_HEADING_RE.match(t) and _word_count(t) <= 12:
        return True

    if ANNEX_HEADING_RE.match(t) and _word_count(t) <= 12:
        return True

    if TITLE_LIKE_RE.match(t) and len(t) <= 70 and not re.search(r"[.!?;:]\s*$", t):
        if re.search(r"\b\d+(\.\d+)*\b", t):
            return True
        if _word_count(t) <= 6:
            return True

    return False


def _should_reject_sentence(s: str) -> bool:
    t = (s or "").strip()
    if not t:
        return True
    if _word_count(t) < MIN_WORDS_PER_SENTENCE:
        return True
    if _is_page_artifact(t):
        return True
    if _is_toc_line(t):
        return True
    if _is_caption_or_heading(t):
        return True
    return False


def _detect_asset(sentence: str) -> str:
    s = (sentence or "").lower()
    has_motor = re.search(r"\bmotor\b", s) is not None
    has_pump = re.search(r"\bpump\b", s) is not None
    if has_motor and not has_pump:
        return "motor"
    if has_pump and not has_motor:
        return "pump"
    return ""


def _parse_demand_parts(demand_name: str) -> Tuple[str, str]:
    """
    Input examples:
      "CST - Motor - efficiency"
      "CST - Pump - drawings"
    IMPORTANT:
      - ignore CST / prefix completely
      - asset is 2nd segment
      - topic is last segment
    Returns: (asset, topic) lowercased.
    """
    raw = (demand_name or "").strip()
    raw = re.sub(r"\s+", " ", raw)
    parts = [p.strip().lower() for p in _DASH_SPLIT_RE.split(raw) if p.strip()]
    if not parts:
        return "", ""
    asset = parts[1] if len(parts) >= 2 else ""
    topic = parts[-1] if len(parts) >= 1 else ""
    if "motor" in asset:
        asset = "motor"
    elif "pump" in asset:
        asset = "pump"
    topic = re.sub(r"\s+", " ", topic).strip()
    return asset, topic


def _build_demand_indexes():
    global DEMAND_NAME_RAW, DEMAND_DESC_RAW, DEMAND_NAME_TOKS, DEMAND_DESC_TOKS, DEMAND_ALL_TOKS
    global DEMAND_ASSET, DEMAND_TOPIC, TOPIC_TO_DEMANDS, TOKEN_DF, TOKEN_IDF

    DEMAND_NAME_RAW = {}
    DEMAND_DESC_RAW = {}
    DEMAND_NAME_TOKS = {}
    DEMAND_DESC_TOKS = {}
    DEMAND_ALL_TOKS = {}

    DEMAND_ASSET = {}
    DEMAND_TOPIC = {}
    TOPIC_TO_DEMANDS = {}

    TOKEN_DF = {}
    TOKEN_IDF = {}

    for d in DEMANDS:
        did = d["id"]
        name = (d.get("demand", "") or "").strip()
        desc = (d.get("description", "") or "").strip()

        DEMAND_NAME_RAW[did] = name.lower()
        DEMAND_DESC_RAW[did] = desc.lower()

        asset, topic = _parse_demand_parts(name)
        DEMAND_ASSET[did] = asset
        DEMAND_TOPIC[did] = topic

        # tokens: ignore CST/prefix by NOT injecting it at all
        name_toks = _tokset(topic)  # name tokens = topic only (key change)
        # but keep some extra from full name excluding prefix for robustness
        # (asset+topic) only:
        if asset:
            name_toks.add(asset)

        desc_toks = _tokset(desc)

        DEMAND_NAME_TOKS[did] = name_toks
        DEMAND_DESC_TOKS[did] = desc_toks
        DEMAND_ALL_TOKS[did] = set(name_toks | desc_toks)

        if topic:
            TOPIC_TO_DEMANDS.setdefault(topic, []).append(did)

    # IDF over demands (topic+desc tokens)
    N = max(1, len(DEMANDS))
    for toks in DEMAND_ALL_TOKS.values():
        for tok in toks:
            TOKEN_DF[tok] = TOKEN_DF.get(tok, 0) + 1
    for tok, df in TOKEN_DF.items():
        TOKEN_IDF[tok] = math.log((N + 1.0) / (df + 1.0)) + 1.0


def _idf_sum(tokens: set) -> float:
    s = 0.0
    for tok in tokens:
        s += float(TOKEN_IDF.get(tok, 1.0))
    return s


def _topic_variants(topic: str) -> List[str]:
    """
    Minimal variants without exploding synonyms:
    - exact
    - plural/singular naive
    """
    t = (topic or "").strip().lower()
    if not t:
        return []
    vars_ = {t}
    if t.endswith("s") and len(t) > 3:
        vars_.add(t[:-1])
    else:
        vars_.add(t + "s")
    return sorted(vars_)


def _find_topics_in_sentence(sentence: str) -> List[str]:
    """
    Find topics using strict word-boundary matching.
    This is the key 'topic-first' anchor (no semantic guessing, no synonym explosion).
    """
    s = (sentence or "").lower()
    found = set()
    # fast prefilter by tokens
    stoks = set(_tokenize(sentence))
    if not stoks:
        return []

    # Only try topics that could plausibly match: token intersection with topic tokens
    # (topic might be multi-word; we still use phrase match later)
    for topic in TOPIC_TO_DEMANDS.keys():
        t_toks = set(_tokenize(topic))
        if not t_toks:
            continue
        if len(stoks & t_toks) == 0:
            # if multi-word phrase, tokens won't intersect -> skip
            continue

        # phrase match on variants
        for v in _topic_variants(topic):
            if re.search(rf"\b{re.escape(v)}\b", s):
                found.add(topic)
                break

    return sorted(found)


def _is_requirement_fast(text: str) -> Tuple[bool, float]:
    t = (text or "").strip()
    if len(t) < MIN_CHUNK_LEN:
        return False, 0.0

    if _is_caption_or_heading(t):
        # headings are usually not requirements unless explicit modal/imperative + numeric/unit
        has_num = bool(REQ_NUM_RE.search(t))
        has_unit = bool(UNIT_RE.search(t))
        if (REQ_MODAL_RE.search(t) or REQ_IMPERATIVE_RE.search(t)) and (has_num or has_unit):
            return True, 0.70
        return False, 0.10

    has_modal = bool(REQ_MODAL_RE.search(t))
    has_imp = bool(REQ_IMPERATIVE_RE.search(t))
    has_num = bool(REQ_NUM_RE.search(t))
    has_unit = bool(UNIT_RE.search(t))
    has_cond = bool(REQ_COND_RE.search(t))

    if (has_modal or has_imp) and (has_num or has_unit):
        return True, 0.86
    if has_modal or has_imp:
        return True, 0.72
    if has_num and has_unit:
