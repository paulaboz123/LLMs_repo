import json
import logging
import os
import re
import math
from typing import Any, Dict, List, Tuple, Optional

import pandas as pd

try:
    from openai import AzureOpenAI
except Exception:
    AzureOpenAI = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("score")

# -------------------------
# Tunables (env)
# -------------------------
USE_MOCK = os.getenv("USE_MOCK", "0").strip() == "1"  # if 1, never call LLM

MIN_CHUNK_LEN = int(os.getenv("MIN_CHUNK_LEN", "10"))
MAX_LABELS_PER_CHUNK = int(os.getenv("MAX_LABELS_PER_CHUNK", "3"))
CANDIDATES_TOPK = int(os.getenv("CANDIDATES_TOPK", "12"))

LABEL_MIN_PROB = float(os.getenv("LABEL_MIN_PROB", "0.75"))
HIGHLIGHT_MIN_PROB = float(os.getenv("HIGHLIGHT_MIN_PROB", "0.80"))
RELEVANCE_MIN = float(os.getenv("RELEVANCE_MIN", "0.60"))

MAX_SENTENCES_PER_CHUNK = int(os.getenv("MAX_SENTENCES_PER_CHUNK", "10"))
MAX_SENTENCES_TO_LLM = int(os.getenv("MAX_SENTENCES_TO_LLM", "6"))  # allow more to avoid "0 results"

# Content filters
MIN_WORDS_PER_SENTENCE = int(os.getenv("MIN_WORDS_PER_SENTENCE", "4"))  # reject <=3 words
REJECT_TOC = os.getenv("REJECT_TOC", "1").strip() == "1"
REJECT_PAGE_ARTIFACTS = os.getenv("REJECT_PAGE_ARTIFACTS", "1").strip() == "1"
REJECT_HEADINGS = os.getenv("REJECT_HEADINGS", "1").strip() == "1"  # NEW: captions/section headings
REJECT_TABLE_CAPTIONS = os.getenv("REJECT_TABLE_CAPTIONS", "1").strip() == "1"  # NEW

# Optional LLM gate (OFF by default)
USE_LLM_GATE = os.getenv("USE_LLM_GATE", "0").strip() == "1" and not USE_MOCK

# Deterministic guards (description-first but NOT absolute-killer)
MIN_CANDIDATE_TOKEN_OVERLAP = int(os.getenv("MIN_CANDIDATE_TOKEN_OVERLAP", "1"))
MIN_DESC_OVERLAP_FOR_LABEL = int(os.getenv("MIN_DESC_OVERLAP_FOR_LABEL", "0"))  # default 0 to avoid "0 highlights"
MIN_LABEL_TOKEN_OVERLAP = int(os.getenv("MIN_LABEL_TOKEN_OVERLAP", "2"))
ZERO_LEXICAL_REJECT = os.getenv("ZERO_LEXICAL_REJECT", "1").strip() == "1"

# Scoring weights (deterministic)
W_NAME_OVERLAP = float(os.getenv("W_NAME_OVERLAP", "2.0"))
W_DESC_OVERLAP = float(os.getenv("W_DESC_OVERLAP", "3.0"))   # description more important
W_NAME_PHRASE = float(os.getenv("W_NAME_PHRASE", "4.0"))

# NEW: structure-aware boosts (topic/asset from "CST - motor - earthing")
W_TOPIC_HIT = float(os.getenv("W_TOPIC_HIT", "5.0"))
W_ASSET_HIT = float(os.getenv("W_ASSET_HIT", "0.5"))

# Probability shaping (IMPORTANT: reduce false "high confidence")
SOFTMAX_TEMP = float(os.getenv("SOFTMAX_TEMP", "4.0"))
PROB_CAP = float(os.getenv("PROB_CAP", "0.95"))
PROB_BASE = float(os.getenv("PROB_BASE", "0.35"))   # was effectively 0.55 hardcoded before
PROB_SCALE = float(os.getenv("PROB_SCALE", "0.65")) # base+scale=1.0 recommended

# Highlight fallback (prevents "0 results" when requirement-gate misses imperative requirements)
ALLOW_HIGHLIGHT_WITH_STRONG_MATCH = os.getenv("ALLOW_HIGHLIGHT_WITH_STRONG_MATCH", "1").strip() == "1"
STRONG_MATCH_PROB = float(os.getenv("STRONG_MATCH_PROB", "0.88"))
STRONG_MATCH_DESC_OVERLAP = int(os.getenv("STRONG_MATCH_DESC_OVERLAP", "2"))

MAX_DEBUG_CHUNKS = int(os.getenv("MAX_DEBUG_CHUNKS", "0"))

client = None
CHAT_DEPLOYMENT = None

# -------------------------
# Demands cache
# -------------------------
DEMANDS: List[Dict[str, str]] = []
DEMAND_TEXT: Dict[str, str] = {}
DEMAND_NAME_RAW: Dict[str, str] = {}
DEMAND_DESC_RAW: Dict[str, str] = {}
DEMAND_NAME_TOKS: Dict[str, set] = {}
DEMAND_DESC_TOKS: Dict[str, set] = {}
DEMAND_ALL_TOKS: Dict[str, set] = {}

# NEW: structured demand parts + IDF
DEMAND_ASSET: Dict[str, str] = {}
DEMAND_TOPIC: Dict[str, str] = {}
TOKEN_DF: Dict[str, int] = {}
TOKEN_IDF: Dict[str, float] = {}

# -------------------------
# Stopwords (English ONLY, minimal)
# Keep requirement words; keep domain words (you asked to NOT remove them)
# -------------------------
STOPWORDS = {
    "the", "and", "or", "to", "of", "in", "on", "for", "with", "a", "an", "as", "at", "by",
    "from", "into", "over", "under", "than", "then", "that", "this", "these", "those",
    "be", "is", "are", "was", "were", "been", "being",
    "it", "its", "they", "them", "their", "we", "our", "you", "your",
    "can", "could", "may", "might", "will", "would",
    "not", "no", "yes",
}

# Keep short engineering tokens (do NOT overdo; whitelist only)
SHORT_TOKEN_WHITELIST = {
    "ip", "hz", "kw", "mw", "mm", "cm", "kg", "pa", "bar", "rpm", "°c", "dc", "ac", "od", "id"
}

# -------------------------
# Requirement heuristics (EN)
# -------------------------
REQ_MODAL_RE = re.compile(
    r"\b(shall|must|required|requirement|should|need to|has to|may not|must not|shall not|prohibit|forbidden|recommended|recommend)\b",
    re.IGNORECASE,
)
REQ_IMPERATIVE_RE = re.compile(
    r"^\s*(use|provide|ensure|verify|test|install|maintain|replace|protect|include|apply|set|adjust|calibrate|measure|limit)\b",
    re.IGNORECASE,
)
REQ_COND_RE = re.compile(r"\b(if|when|unless|in case)\b", re.IGNORECASE)

REQ_NUM_RE = re.compile(r"\b\d+([.,]\d+)?\b")
UNIT_RE = re.compile(
    r"\b(mm|cm|m|kg|w|kw|mw|v|kv|a|ma|hz|rpm|bar|pa|degc|°c|c|ip\d{2})\b",
    re.IGNORECASE,
)

# -------------------------
# TOC / page artifacts / headings
# -------------------------
TOC_DOTS_RE = re.compile(r"^\s*(\d+(\.\d+)*\s+)?[A-Za-z][A-Za-z0-9 \-_/(),]{2,}\s*\.{5,}\s*\d+\s*$")
TOC_SECTION_PAGE_RE = re.compile(r"^\s*\d+(\.\d+)+\s+[A-Za-z].{2,}\s+\d+\s*$")

PAGE_ONLY_RE = re.compile(r"^\s*\d+\s*$")
PAGE_DASH_RE = re.compile(r"^\s*[-–—]+\s*\d+\s*[-–—]+\s*$")
PAGE_WORD_RE = re.compile(r"^\s*(page)\s*\d+\s*$", re.IGNORECASE)
PAGE_FRACTION_RE = re.compile(r"^\s*\d+\s*/\s*\d+\s*$")

# NEW: typical captions/headings that should NOT be highlighted (unless they contain clear requirement language)
TABLE_FIG_CAPTION_RE = re.compile(r"^\s*(table|fig\.?|figure)\s*\d+(\.\d+)*\b", re.IGNORECASE)
SECTION_HEADING_RE = re.compile(r"^\s*\d+(\.\d+)*\s+[A-Za-z][A-Za-z0-9 \-_/(),]{2,}\s*$")
ANNEX_HEADING_RE = re.compile(r"^\s*(annex|appendix)\s+[A-Za-z0-9]+\b", re.IGNORECASE)

# e.g., "Pump vibration specification" (short, title-like)
TITLE_LIKE_RE = re.compile(r"^[A-Za-z0-9][A-Za-z0-9 \-_/(),]{0,80}$")


def _json_load_maybe(x: Any) -> Any:
    if isinstance(x, (bytes, bytearray)):
        x = x.decode("utf-8", errors="replace")
    if isinstance(x, str):
        s = x.strip()
        if not s:
            return None
        try:
            return json.loads(s)
        except Exception:
            return x
    return x


def _find_content_byid(document: Dict[str, Any]) -> Dict[str, Any]:
    cd = document.get("contentDomain", {})
    by_id = cd.get("byId", {})
    return by_id if isinstance(by_id, dict) else {}


def _load_demands_xlsx() -> List[Dict[str, str]]:
    here = os.path.dirname(os.path.abspath(__file__))
    candidates = [
        os.path.join(here, "demands.xlsx"),
        os.path.join(here, "assets", "demands.xlsx"),
    ]
    model_dir = os.getenv("AZUREML_MODEL_DIR")
    if model_dir:
        candidates += [
            os.path.join(model_dir, "demands.xlsx"),
            os.path.join(model_dir, "assets", "demands.xlsx"),
        ]

    for p in candidates:
        if os.path.exists(p):
            df = pd.read_excel(p)
            df.columns = [c.strip().lower() for c in df.columns]
            if "demand_id" not in df.columns and "id" in df.columns:
                df = df.rename(columns={"id": "demand_id"})
            if "description" not in df.columns and "demand_description" in df.columns:
                df = df.rename(columns={"demand_description": "description"})

            required = {"demand_id", "demand", "description"}
            missing = required - set(df.columns)
            if missing:
                raise RuntimeError(f"demands.xlsx missing columns: {sorted(list(missing))}. Found: {list(df.columns)}")

            out: List[Dict[str, str]] = []
            for _, r in df.fillna("").iterrows():
                did = str(r["demand_id"]).strip()
                if not did or did.lower() == "nan":
                    continue
                out.append({"id": did, "demand": str(r["demand"]).strip(), "description": str(r["description"]).strip()})
            logger.info("Loaded %d demands from %s", len(out), p)
            return out

    raise RuntimeError(f"demands.xlsx not found. Tried: {candidates}")


def _tokenize(s: str) -> List[str]:
    s = (s or "").lower()
    # keep degree sign
    s = re.sub(r"[^a-z0-9°]+", " ", s)
    toks = []
    for t in s.split():
        if len(t) < 3 and t not in SHORT_TOKEN_WHITELIST:
            continue
        if t in STOPWORDS:
            continue
        toks.append(t)
    return toks


def _tokset(s: str) -> set:
    return set(_tokenize(s))


def _word_count(s: str) -> int:
    return len([w for w in re.findall(r"[A-Za-z0-9]+", s or "") if w.strip()])


def _is_toc_line(s: str) -> bool:
    if not REJECT_TOC:
        return False
    t = (s or "").strip()
    if len(t) < 6:
        return False
    if TOC_DOTS_RE.match(t):
        return True
    if TOC_SECTION_PAGE_RE.match(t):
        return True
    # heuristic: many dots and ends with number
    if re.search(r"\.{5,}", t) and re.search(r"\d+\s*$", t):
        return True
    return False


def _is_page_artifact(s: str) -> bool:
    if not REJECT_PAGE_ARTIFACTS:
        return False
    t = (s or "").strip()
    if not t:
        return True
    if PAGE_ONLY_RE.match(t) or PAGE_DASH_RE.match(t) or PAGE_WORD_RE.match(t) or PAGE_FRACTION_RE.match(t):
        return True
    return False


def _is_caption_or_heading(s: str) -> bool:
    """
    Reject:
    - TOC lines already handled elsewhere
    - table/figure captions like "Table 2.1 Pump vibration specification"
    - numbered headings like "2.1 Spare parts"
    - annex/appendix headings
    - short title-like lines (no punctuation) that look like headings
    BUT: do NOT reject if it clearly contains requirement language.
    """
    if not REJECT_HEADINGS:
        return False

    t = (s or "").strip()
    if not t:
        return True

    # If it contains explicit requirement language, keep it
    if REQ_MODAL_RE.search(t) or REQ_IMPERATIVE_RE.search(t):
        return False

    # Table/Figure captions
    if REJECT_TABLE_CAPTIONS and TABLE_FIG_CAPTION_RE.match(t):
        return True

    # Classic section headings (numbered)
    # "2.1 Spare parts" or "1.1.3 Measurements"
    if SECTION_HEADING_RE.match(t) and _word_count(t) <= 12:
        return True

    # Annex/Appendix headings
    if ANNEX_HEADING_RE.match(t) and _word_count(t) <= 12:
        return True

    # Title-like short line:
    # - no sentence end punctuation
    # - relatively short
    # - often appears as "Pump vibration specification" / "Spare parts"
    if TITLE_LIKE_RE.match(t) and len(t) <= 70 and not re.search(r"[.!?;:]\s*$", t):
        # if it has numbers (e.g., 2.1) or looks like caption-ish, treat as heading
        if re.search(r"\b\d+(\.\d+)*\b", t):
            return True
        # If very short (<=6 words), almost always a heading
        if _word_count(t) <= 6:
            return True

    return False


def _should_reject_sentence(s: str) -> bool:
    t = (s or "").strip()
    if not t:
        return True
    if _word_count(t) < MIN_WORDS_PER_SENTENCE:
        return True
    if _is_page_artifact(t):
        return True
    if _is_toc_line(t):
        return True
    if _is_caption_or_heading(t):
        return True
    return False


def _parse_demand_parts(name: str) -> Tuple[str, str, str]:
    # "CST - Motor - documentation" -> ("cst", "motor", "documentation")
    parts = [p.strip().lower() for p in re.split(r"\s*-\s*", (name or "").strip()) if p.strip()]
    if not parts:
        return "", "", ""
    prefix = parts[0] if len(parts) >= 1 else ""
    asset = parts[1] if len(parts) >= 2 else ""
    topic = parts[-1] if len(parts) >= 3 else (parts[-1] if parts else "")

    # normalize asset variants
    if "motor" in asset:
        asset = "motor"
    elif "pump" in asset:
        asset = "pump"

    # normalize topic whitespace
    topic = re.sub(r"\s+", " ", topic).strip()
    return prefix, asset, topic


def _build_demand_indexes():
    global DEMAND_TEXT, DEMAND_NAME_RAW, DEMAND_DESC_RAW
    global DEMAND_NAME_TOKS, DEMAND_DESC_TOKS, DEMAND_ALL_TOKS
    global DEMAND_ASSET, DEMAND_TOPIC, TOKEN_DF, TOKEN_IDF

    DEMAND_TEXT = {}
    DEMAND_NAME_RAW = {}
    DEMAND_DESC_RAW = {}
    DEMAND_NAME_TOKS = {}
    DEMAND_DESC_TOKS = {}
    DEMAND_ALL_TOKS = {}
    DEMAND_ASSET = {}
    DEMAND_TOPIC = {}
    TOKEN_DF = {}
    TOKEN_IDF = {}

    # first pass: build tokens
    for d in DEMANDS:
        did = d["id"]
        name = (d.get("demand", "") or "").strip()
        desc = (d.get("description", "") or "").strip()

        DEMAND_NAME_RAW[did] = name.lower()
        DEMAND_DESC_RAW[did] = desc.lower()

        _, asset, topic = _parse_demand_parts(name)
        DEMAND_ASSET[did] = asset
        DEMAND_TOPIC[did] = topic

        name_toks = _tokset(name)
        desc_toks = _tokset(desc)

        # Add asset/topic as anchor tokens (helps retrieval without making synonyms explode)
        if asset:
            name_toks.add(asset)
        if topic and len(topic) >= 3:
            name_toks.add(topic)

        DEMAND_NAME_TOKS[did] = name_toks
        DEMAND_DESC_TOKS[did] = desc_toks
        DEMAND_ALL_TOKS[did] = set(name_toks | desc_toks)

        desc_short = desc if len(desc) <= 220 else (desc[:220] + "…")
        DEMAND_TEXT[did] = f"{name} | {desc_short}"

    # second pass: DF/IDF over demands (each demand = "doc")
    N = max(1, len(DEMANDS))
    for did, toks in DEMAND_ALL_TOKS.items():
        for tok in toks:
            TOKEN_DF[tok] = TOKEN_DF.get(tok, 0) + 1
    for tok, df in TOKEN_DF.items():
        TOKEN_IDF[tok] = math.log((N + 1.0) / (df + 1.0)) + 1.0


def _detect_asset(sentence: str) -> str:
    s = (sentence or "").lower()
    has_motor = re.search(r"\bmotor\b", s) is not None
    has_pump = re.search(r"\bpump\b", s) is not None
    if has_motor and not has_pump:
        return "motor"
    if has_pump and not has_motor:
        return "pump"
    return ""


def _is_requirement_fast(text: str) -> Tuple[bool, float]:
    t = (text or "").strip()
    if len(t) < MIN_CHUNK_LEN:
        return False, 0.0

    # Headings/captions are almost never requirements unless modal/imperative exists
    if _is_caption_or_heading(t):
        # If it has numeric+unit, still could be requirement-ish, but keep low
        has_num = bool(REQ_NUM_RE.search(t))
        has_unit = bool(UNIT_RE.search(t))
        if (REQ_MODAL_RE.search(t) or REQ_IMPERATIVE_RE.search(t)) and (has_num or has_unit):
            return True, 0.70
        return False, 0.10

    has_modal = bool(REQ_MODAL_RE.search(t))
    has_imp = bool(REQ_IMPERATIVE_RE.search(t))
    has_num = bool(REQ_NUM_RE.search(t))
    has_unit = bool(UNIT_RE.search(t))
    has_cond = bool(REQ_COND_RE.search(t))

    if (has_modal or has_imp) and (has_num or has_unit):
        return True, 0.86
    if has_modal or has_imp:
        return True, 0.72
    if has_num and has_unit:
        return True, 0.66
    if has_cond and (has_num or has_unit):
        return True, 0.62
    return False, 0.25


def _split_into_sentences_or_lines(chunk_text: str) -> List[str]:
    t = (chunk_text or "").strip()
    if not t:
        return []

    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    tableish = (len(lines) >= 6) or any(("|" in ln or "\t" in ln) for ln in lines)
    if tableish:
        out = [ln for ln in lines if len(ln) >= MIN_CHUNK_LEN]
        return out[:MAX_SENTENCES_PER_CHUNK]

    parts = re.split(r"(?<=[.!?])\s+|\n+", t)
    parts = [p.strip() for p in parts if p and p.strip()]
    return parts[:MAX_SENTENCES_PER_CHUNK]


def _name_phrase_hit(sentence_lower: str, did: str) -> bool:
    """
    Previous behavior ("full demand string in sentence") almost never hits.
    New behavior: treat the *topic* (last segment) as the primary phrase anchor.
    This avoids exploding to synonyms while still matching "documentation/spare parts/vibration/earthing/coating".
    """
    topic = (DEMAND_TOPIC.get(did, "") or "").strip()
    if topic and len(topic) >= 3:
        if re.search(rf"\b{re.escape(topic)}\b", sentence_lower):
            return True

    # Also allow a short phrase from demand name if it's reasonable (e.g., "spare parts")
    name = (DEMAND_NAME_RAW.get(did, "") or "").strip()
    if name:
        # pick last 1-3 words from the demand string as a fallback phrase (excluding CST/asset noise)
        parts = [p.strip().lower() for p in re.split(r"\s*-\s*", name) if p.strip()]
        if len(parts) >= 3:
            tail = parts[-1]
            # If tail is multiword like "spare parts", require exact match
            if " " in tail and len(tail) >= 6 and tail in sentence_lower:
                return True
    return False


def _topic_hit(sentence_lower: str, did: str) -> bool:
    topic = (DEMAND_TOPIC.get(did, "") or "").strip()
    if not topic or len(topic) < 3:
        return False
    return re.search(rf"\b{re.escape(topic)}\b", sentence_lower) is not None


def _idf_sum(overlap_tokens: set) -> float:
    s = 0.0
    for tok in overlap_tokens:
        s += float(TOKEN_IDF.get(tok, 1.0))
    return s


def _candidate_retrieval(sentence: str, k: int) -> List[str]:
    stoks = set(_tokenize(sentence))
    s_lower = (sentence or "").lower()
    if not stoks:
        return []

    # Hard asset gate (motor vs pump) when sentence clearly signals it.
    sent_asset = _detect_asset(sentence)

    scored: List[Tuple[str, float]] = []
    anchors: List[str] = []

    for did, all_toks in DEMAND_ALL_TOKS.items():
        if not all_toks:
            continue

        if sent_asset:
            da = DEMAND_ASSET.get(did, "")
            if da and da != sent_asset:
                continue

        overlap = len(stoks & all_toks)
        if _name_phrase_hit(s_lower, did):
            anchors.append(did)
        if overlap >= MIN_CANDIDATE_TOKEN_OVERLAP:
            scored.append((did, float(overlap)))

    if not scored and not anchors:
        return []

    anchor_set = set(anchors)
    boosted: List[Tuple[str, float]] = []
    present = set()

    for did, sc in scored:
        if did in anchor_set:
            sc += W_NAME_PHRASE
        boosted.append((did, sc))
        present.add(did)

    for did in anchors:
        if did not in present:
            boosted.append((did, W_NAME_PHRASE))

    boosted.sort(key=lambda x: x[1], reverse=True)
    return [did for did, _ in boosted[: max(1, k)]]


def _score_label(sentence: str, did: str) -> Tuple[float, int, int, int, bool]:
    stoks = set(_tokenize(sentence))
    if not stoks:
        return 0.0, 0, 0, 0, False

    name_toks = DEMAND_NAME_TOKS.get(did, set())
    desc_toks = DEMAND_DESC_TOKS.get(did, set())
    if not name_toks and not desc_toks:
        return 0.0, 0, 0, 0, False

    inter_name = stoks & name_toks
    inter_desc = stoks & desc_toks

    # keep original integer overlaps for meta/guards
    oname = len(inter_name)
    odesc = len(inter_desc)
    ototal = oname + odesc

    s_lower = (sentence or "").lower()
    phrase_hit = _name_phrase_hit(s_lower, did)
    topic_hit = _topic_hit(s_lower, did)

    # IDF-weighted score: reduces generic-token domination
    score = W_NAME_OVERLAP * _idf_sum(inter_name) + W_DESC_OVERLAP * _idf_sum(inter_desc)

    if topic_hit:
        score += W_TOPIC_HIT
    if phrase_hit:
        score += W_NAME_PHRASE

    sent_asset = _detect_asset(sentence)
    if sent_asset and DEMAND_ASSET.get(did, "") == sent_asset:
        score += W_ASSET_HIT

    return float(score), int(ototal), int(oname), int(odesc), bool(phrase_hit)


def _softmax_probs(scores: List[float], temp: float) -> List[float]:
    if not scores:
        return []
    t = max(0.1, float(temp))
    m = max(scores)
    exps = [math.exp((s - m) / t) for s in scores]
    z = sum(exps) if exps else 1.0
    return [e / z for e in exps]


def _make_explanation(sentence: str, did: str) -> str:
    """
    Deterministic 1-2 sentence explanation based on token overlaps.
    (No LLM; does not affect predictions.)
    """
    stoks = set(_tokenize(sentence))
    name_toks = DEMAND_NAME_TOKS.get(did, set())
    desc_toks = DEMAND_DESC_TOKS.get(did, set())

    hit_name = sorted(list(stoks & name_toks))[:4]
    hit_desc = sorted(list(stoks & desc_toks))[:4]

    parts = []
    if hit_desc:
        parts.append(f"Matches description cues: {', '.join(hit_desc)}.")
    if hit_name:
        parts.append(f"Also aligns with label terms: {', '.join(hit_name)}.")
    if not parts:
        parts.append("Token overlap supports this label per demand table.")
    return " ".join(parts[:2]).strip()


def _deterministic_predict(sentence: str, top_k: int) -> Dict[str, Any]:
    is_req_fast, rel_fast = _is_requirement_fast(sentence)

    candidate_ids = _candidate_retrieval(sentence, CANDIDATES_TOPK)
    if not candidate_ids:
        return {"isRequirement": bool(is_req_fast), "relevance": float(rel_fast), "labels": [], "meta": {}, "notes": "no_candidates"}

    scored: List[Tuple[str, float, int, int, int, bool]] = []
    for did in candidate_ids:
        sc, ototal, oname, odesc, phrase_hit = _score_label(sentence, did)
        scored.append((did, sc, ototal, oname, odesc, phrase_hit))

    scored.sort(key=lambda x: x[1], reverse=True)

    filtered: List[Tuple[str, float, int, int, int, bool]] = []
    for did, sc, ototal, oname, odesc, phrase_hit in scored:
        if ZERO_LEXICAL_REJECT and ototal == 0:
            continue
        if ototal < MIN_LABEL_TOKEN_OVERLAP:
            continue

        if MIN_DESC_OVERLAP_FOR_LABEL > 0 and odesc < MIN_DESC_OVERLAP_FOR_LABEL:
            # allow phrase hit or strong name overlap as fallback
            if not phrase_hit and oname < 2:
                continue

        filtered.append((did, sc, ototal, oname, odesc, phrase_hit))
        if len(filtered) >= max(1, top_k):
            break

    if not filtered:
        return {"isRequirement": bool(is_req_fast), "relevance": float(rel_fast), "labels": [], "meta": {}, "notes": "no_labels_after_guards"}

    dids = [d for d, *_ in filtered]
    scs = [s for _, s, *_ in filtered]
    probs = _softmax_probs(scs, SOFTMAX_TEMP)

    labels_out: List[Dict[str, Any]] = []
    meta: Dict[str, Any] = {}
    for (did, sc, ototal, oname, odesc, phrase_hit), p in zip(filtered, probs):
        p_adj = min(PROB_CAP, max(0.0, float(p)))
        # New shaping: avoid inflating mediocre candidates to "high confidence"
        p_adj = min(PROB_CAP, max(0.0, PROB_BASE + PROB_SCALE * p_adj))

        if p_adj < LABEL_MIN_PROB:
            continue

        labels_out.append({"id": did, "probability": float(p_adj)})
        meta[did] = {
            "oname": int(oname),
            "odesc": int(odesc),
            "phrase": bool(phrase_hit),
            "topic": bool(_topic_hit((sentence or "").lower(), did)),
        }

    labels_out.sort(key=lambda x: x["probability"], reverse=True)
    labels_out = labels_out[:MAX_LABELS_PER_CHUNK]

    if not labels_out:
        return {"isRequirement": bool(is_req_fast), "relevance": float(rel_fast), "labels": [], "meta": meta, "notes": "probs_below_threshold"}

    rel = max(float(rel_fast), float(labels_out[0]["probability"]))
    return {"isRequirement": bool(is_req_fast), "relevance": float(rel), "labels": labels_out, "meta": meta, "notes": "ok"}


def _llm_gate(sentence: str) -> Tuple[bool, float]:
    if client is None or not CHAT_DEPLOYMENT:
        return _is_requirement_fast(sentence)

    system = "Return STRICT JSON only. No prose."
    user = f"""
Decide whether the SENTENCE contains a concrete requirement/specification that should be highlighted.
Return JSON only:
{{
  "isRequirement": true|false,
  "relevance": 0.0
}}
SENTENCE:
{sentence}
Guidance:
- signals: must/shall/need to/has to, constraints, tests, prohibitions, explicit conditions, imperative requirements.
- do NOT treat headings/captions (e.g., 'Table 2.1 ...', section titles) as requirements unless they contain explicit requirement language.
- relevance 0..1. If NOT requirement, keep it < {RELEVANCE_MIN}.
""".strip()

    resp = client.chat.completions.create(
        model=CHAT_DEPLOYMENT,
        messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
        temperature=0.0,
        max_tokens=180,
    )
    raw = (resp.choices[0].message.content or "").strip()
    try:
        data = json.loads(raw)
        is_req = bool(data.get("isRequirement", False))
        rel = float(data.get("relevance", 0.0) or 0.0)
        return is_req, rel
    except Exception:
        return _is_requirement_fast(sentence)


def _predict_sentence(sentence: str, top_k: int) -> Dict[str, Any]:
    if _should_reject_sentence(sentence):
        return {"isRequirement": False, "relevance": 0.0, "labels": [], "meta": {}, "notes": "rejected_artifact"}

    pred = _deterministic_predict(sentence, top_k=top_k)

    if USE_LLM_GATE:
        is_req, rel = _llm_gate(sentence)
        pred["isRequirement"] = bool(is_req)
        pred["relevance"] = float(rel)

    return pred


# -------------------------
# AML entrypoints
# -------------------------
def init():
    global client, CHAT_DEPLOYMENT, DEMANDS, USE_MOCK

    USE_MOCK = os.getenv("USE_MOCK", "0").strip() == "1"
    DEMANDS = _load_demands_xlsx()
    _build_demand_indexes()

    logger.info(
        "init(): USE_MOCK=%s USE_LLM_GATE=%s demands=%d LABEL_MIN_PROB=%.2f HIGHLIGHT_MIN_PROB=%.2f "
        "MIN_DESC_OVERLAP_FOR_LABEL=%d SOFTMAX_TEMP=%.2f PROB_BASE=%.2f PROB_SCALE=%.2f "
        "REJECT_HEADINGS=%s REJECT_TABLE_CAPTIONS=%s",
        USE_MOCK, USE_LLM_GATE, len(DEMANDS),
        LABEL_MIN_PROB, HIGHLIGHT_MIN_PROB,
        MIN_DESC_OVERLAP_FOR_LABEL, SOFTMAX_TEMP, PROB_BASE, PROB_SCALE,
        str(REJECT_HEADINGS), str(REJECT_TABLE_CAPTIONS)
    )

    if USE_LLM_GATE and not USE_MOCK:
        if AzureOpenAI is None:
            raise RuntimeError("openai package not available but USE_LLM_GATE=1")

        aoai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "").strip()
        aoai_key = os.getenv("AZURE_OPENAI_API_KEY", "").strip()
        aoai_version = os.getenv("AZURE_OPENAI_API_VERSION", "").strip() or "2024-06-01"

        CHAT_DEPLOYMENT = (
            os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT", "").strip()
            or os.getenv("AZURE_OPENAI_DEPLOYMENT", "").strip()
        )

        if not (aoai_endpoint and aoai_key and CHAT_DEPLOYMENT):
            raise RuntimeError(
                "Missing AOAI env vars. Need: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, "
                "AZURE_OPENAI_CHAT_DEPLOYMENT (or AZURE_OPENAI_DEPLOYMENT)."
            )

        client = AzureOpenAI(
            azure_endpoint=aoai_endpoint,
            api_key=aoai_key,
            api_version=aoai_version,
        )


def run(raw_data):
    """
    Contract (do not break):
      { "predictions": <document> }

    document MUST include:
      documentDemandPredictions: ["id1","id2", ...]  (ARRAY OF STRINGS)

    per chunk:
      relevantProba: float
      cdTransformerPredictions: [{label, proba}]
      cdLogregPredictions: SAME SHAPE (mirror)
    """
    try:
        req = _json_load_maybe(raw_data)
        if not isinstance(req, dict):
            return {"error": "Bad request: body must be JSON object", "predictions": {"documentDemandPredictions": []}}

        if "document" not in req:
            return {"error": "Bad request: missing 'document'", "predictions": {"documentDemandPredictions": []}}

        document = _json_load_maybe(req["document"])
        if not isinstance(document, dict):
            return {"error": "Bad request: 'document' must be JSON object", "predictions": {"documentDemandPredictions": []}}

        num_preds = int(req.get("num_preds", MAX_LABELS_PER_CHUNK))
        num_preds = max(1, min(10, num_preds))
        num_preds = min(num_preds, MAX_LABELS_PER_CHUNK)

        by_id = _find_content_byid(document)

        best_doc_probs: Dict[str, float] = {}
        debug_left = MAX_DEBUG_CHUNKS

        for cid, content in by_id.items():
            if not isinstance(content, dict):
                continue

            chunk_text = (str(content.get("text", "") or "")).strip()

            # Always set expected keys/types
            content["relevantProba"] = 0.0
            content["cdTransformerPredictions"] = []
            content["cdLogregPredictions"] = []
            content["highlightText"] = ""
            # Extra optional info (UI can ignore)
            content["labelExplanations"] = []

            if debug_left > 0:
                logger.info("chunk=%s sample=%r", cid, chunk_text[:180])
                debug_left -= 1

            sentences_raw = _split_into_sentences_or_lines(chunk_text)
            if not sentences_raw:
                continue

            # Filter artifacts/headings/TOC
            sentences = [s for s in sentences_raw if not _should_reject_sentence(s)]
            if not sentences:
                continue

            # Rank sentences: prefer requirement-ish + content-rich lines
            scored_sents: List[Tuple[float, str]] = []
            for s in sentences:
                ss = s.strip()
                if len(ss) < MIN_CHUNK_LEN:
                    continue
                is_req, rel_fast = _is_requirement_fast(ss)

                # avoid picking a leftover caption-like line (belt & suspenders)
                if _is_caption_or_heading(ss):
                    continue

                cands = _candidate_retrieval(ss, k=5)
                # Slightly favor lines that have more words (captions are short)
                wc = _word_count(ss)
                len_bonus = 0.0
                if wc >= 10:
                    len_bonus = 0.05
                if wc >= 16:
                    len_bonus = 0.08

                score = (0.40 * rel_fast) + (0.08 * float(len(cands))) + (0.03 if is_req else 0.0) + len_bonus
                scored_sents.append((score, ss))

            scored_sents.sort(key=lambda x: x[0], reverse=True)
            candidates_for_model = [s for _, s in scored_sents[:MAX_SENTENCES_TO_LLM] if s.strip()]

            best_chunk_pred: Optional[Dict[str, Any]] = None
            best_metric = -1.0
            best_sentence = ""

            for s in candidates_for_model:
                pred = _predict_sentence(s, top_k=num_preds)
                labels = pred.get("labels", []) or []
                if not isinstance(labels, list) or not labels:
                    continue

                labels_sorted = sorted(labels, key=lambda x: float(x.get("probability", 0.0) or 0.0), reverse=True)
                top1 = float(labels_sorted[0].get("probability", 0.0) or 0.0)
                rel = float(pred.get("relevance", 0.0) or 0.0)
                is_req = bool(pred.get("isRequirement", False))

                metric = top1 * (1.0 + 0.25 * rel) * (1.10 if is_req else 1.0)
                if metric > best_metric:
                    best_metric = metric
                    best_chunk_pred = pred
                    best_sentence = s

            if best_chunk_pred is None:
                continue

            labels = best_chunk_pred.get("labels", []) or []
            labels_sorted = sorted(labels, key=lambda x: float(x.get("probability", 0.0) or 0.0), reverse=True)
            labels_sorted = labels_sorted[:num_preds]
            final_labels = [{"label": str(lb["id"]), "proba": float(lb["probability"])} for lb in labels_sorted]

            top1 = float(final_labels[0]["proba"]) if final_labels else 0.0
            is_req = bool(best_chunk_pred.get("isRequirement", False))
            rel = float(best_chunk_pred.get("relevance", 0.0) or 0.0)

            # Strong match fallback (but never allow headings/captions)
            strong_match = False
            if ALLOW_HIGHLIGHT_WITH_STRONG_MATCH and final_labels and not _is_caption_or_heading(best_sentence):
                did0 = final_labels[0]["label"]
                meta0 = (best_chunk_pred.get("meta", {}) or {}).get(did0, {})
                odesc0 = int(meta0.get("odesc", 0) or 0)
                strong_match = (top1 >= STRONG_MATCH_PROB and odesc0 >= STRONG_MATCH_DESC_OVERLAP)

            # Final highlight decision:
            # - must NOT be caption/heading/TOC/page artifact
            # - must meet confidence threshold
            # - must be requirement OR strong-match
            should_highlight = bool(
                (top1 >= HIGHLIGHT_MIN_PROB)
                and (is_req or strong_match)
                and (not _is_caption_or_heading(best_sentence))
                and (not _is_toc_line(best_sentence))
                and (not _is_page_artifact(best_sentence))
            )

            if should_highlight and final_labels:
                content["relevantProba"] = float(max(rel, top1))
                content["cdTransformerPredictions"] = final_labels
                content["cdLogregPredictions"] = list(final_labels)
                content["highlightText"] = best_sentence

                # optional explanations
                expls = []
                for p in final_labels:
                    did = p["label"]
                    expls.append({"label": did, "explanation": _make_explanation(best_sentence, did)})
                content["labelExplanations"] = expls

                for p in final_labels:
                    did = p["label"]
                    prob = float(p["proba"])
                    if did not in best_doc_probs or prob > best_doc_probs[did]:
                        best_doc_probs[did] = prob

        ids_sorted = [k for k, v in sorted(best_doc_probs.items(), key=lambda kv: kv[1], reverse=True)]
        document["documentDemandPredictions"] = ids_sorted[:num_preds]
        return {"predictions": document}

    except Exception as e:
        logger.exception("run() failed")
        return {"error": str(e), "predictions": {"documentDemandPredictions": []}}


