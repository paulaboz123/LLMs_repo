{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure ML deployment (one notebook)\n",
        "This notebook generates deployment code and deploys it under an existing Online Endpoint using the Python SDK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%pip install -U azure-ai-ml azure-identity openai pandas numpy openpyxl python-dotenv requests pyyaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "SUBSCRIPTION_ID = \"REPLACE_ME\"\n",
        "RESOURCE_GROUP  = \"REPLACE_ME\"\n",
        "WORKSPACE_NAME  = \"REPLACE_ME\"\n",
        "\n",
        "ENDPOINT_NAME   = \"REPLACE_ME\"   # existing Azure ML Online Endpoint name\n",
        "NEW_DEPLOYMENT  = \"llm\"          # new deployment name inside the endpoint\n",
        "\n",
        "INSTANCE_TYPE   = \"Standard_DS3_v2\"\n",
        "INSTANCE_COUNT  = 1\n",
        "\n",
        "# Azure OpenAI (Azure Portal -> Azure OpenAI resource -> Keys and Endpoint)\n",
        "AZURE_OPENAI_ENDPOINT = \"https://<your-resource>.openai.azure.com/\"\n",
        "AZURE_OPENAI_API_KEY  = \"REPLACE_ME\"\n",
        "AZURE_OPENAI_API_VERSION = \"2024-12-01-preview\"  # if fails, try 2024-10-01-preview\n",
        "\n",
        "# Azure OpenAI Studio -> Deployments (deployment names)\n",
        "AZURE_OPENAI_CHAT_DEPLOYMENT  = \"gpt-5-mini\"\n",
        "AZURE_OPENAI_EMBED_DEPLOYMENT = \"text-embedding-3-small\"\n",
        "\n",
        "TOP_K_RAG = 8\n",
        "MAX_LABELS_PER_CHUNK = 3\n",
        "MIN_KEEP_PROBA = 0.30\n",
        "MAX_TEXT_CHARS = 4000\n",
        "\n",
        "# Folder where the notebook will write score.py + demands.xlsx for deployment\n",
        "CODE_DIR = \"./_deploy_code\"\n",
        "SCORING_SCRIPT = \"score.py\"\n",
        "\n",
        "SWITCH_TRAFFIC_TO_NEW = False  # True -> route 100% traffic to NEW_DEPLOYMENT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate `score.py` + sample `demands.xlsx` into `CODE_DIR`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "\n",
        "os.makedirs(CODE_DIR, exist_ok=True)\n",
        "\n",
        "demands_path = os.path.join(CODE_DIR, \"demands.xlsx\")\n",
        "if not os.path.exists(demands_path):\n",
        "    df = pd.DataFrame([\n",
        "        {\"demand_id\": \"motor-earthing\", \"demand\": \"CST - Motor - earthing\", \"description\": \"Explicit requirement to connect motor/frame to protective earth (PE) / grounding.\"},\n",
        "        {\"demand_id\": \"motor-ip\", \"demand\": \"CST - Motor - IP\", \"description\": \"Ingress protection class requirement (e.g., IP55, IP66) for motor/enclosure.\"},\n",
        "        {\"demand_id\": \"motor-iso9001\", \"demand\": \"CST - Motor - ISO 9001 certificate\", \"description\": \"Requirement to provide ISO 9001 certification or certificate/documentation.\"},\n",
        "        {\"demand_id\": \"pump-material\", \"demand\": \"CST - Pump - material\", \"description\": \"Requirement for pump material (e.g., AISI 316L) and restrictions for wetted parts/housing.\"},\n",
        "    ])\n",
        "    df.to_excel(demands_path, index=False)\n",
        "    print(\"Created sample demands.xlsx:\", demands_path)\n",
        "else:\n",
        "    print(\"Using existing demands.xlsx:\", demands_path)\n",
        "\n",
        "score_py = r'''import os\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "client: Optional[AzureOpenAI] = None\n",
        "DEMANDS: List[Dict[str, str]] = []\n",
        "DEMAND_EMB: Optional[np.ndarray] = None\n",
        "\n",
        "AZURE_CHAT_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\", \"\").strip()\n",
        "AZURE_EMBED_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBED_DEPLOYMENT\", \"\").strip()\n",
        "\n",
        "TOP_K_RAG = int(os.getenv(\"TOP_K_RAG\", \"8\"))\n",
        "MAX_LABELS_PER_CHUNK = int(os.getenv(\"MAX_LABELS_PER_CHUNK\", \"3\"))\n",
        "MIN_KEEP_PROBA = float(os.getenv(\"MIN_KEEP_PROBA\", \"0.30\"))\n",
        "MAX_TEXT_CHARS = int(os.getenv(\"MAX_TEXT_CHARS\", \"4000\"))\n",
        "\n",
        "def _l2_normalize(x: np.ndarray) -> np.ndarray:\n",
        "    denom = (np.linalg.norm(x, axis=1, keepdims=True) + 1e-12)\n",
        "    return x / denom\n",
        "\n",
        "def _safe_float(v: Any, default: float = 0.0) -> float:\n",
        "    try:\n",
        "        return float(v)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def _safe_json_loads(s: str) -> Optional[dict]:\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def init():\n",
        "    global client, DEMANDS, DEMAND_EMB, AZURE_CHAT_DEPLOYMENT, AZURE_EMBED_DEPLOYMENT\n",
        "    logger.info(\"INIT: starting...\")\n",
        "\n",
        "    endpoint = (os.getenv(\"AZURE_OPENAI_ENDPOINT\") or \"\").strip()\n",
        "    api_key = (os.getenv(\"AZURE_OPENAI_API_KEY\") or \"\").strip()\n",
        "    api_version = (os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2024-12-01-preview\").strip()\n",
        "\n",
        "    AZURE_CHAT_DEPLOYMENT = (os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\") or \"\").strip()\n",
        "    AZURE_EMBED_DEPLOYMENT = (os.getenv(\"AZURE_OPENAI_EMBED_DEPLOYMENT\") or \"\").strip()\n",
        "\n",
        "    if not endpoint or not api_key:\n",
        "        logger.error(\"Missing AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY.\")\n",
        "        return\n",
        "    if not AZURE_CHAT_DEPLOYMENT or not AZURE_EMBED_DEPLOYMENT:\n",
        "        logger.error(\"Missing AZURE_OPENAI_CHAT_DEPLOYMENT or AZURE_OPENAI_EMBED_DEPLOYMENT.\")\n",
        "        return\n",
        "\n",
        "    client = AzureOpenAI(azure_endpoint=endpoint, api_key=api_key, api_version=api_version)\n",
        "\n",
        "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    demands_path = os.path.join(base_dir, \"demands.xlsx\")\n",
        "    if not os.path.exists(demands_path):\n",
        "        logger.error(f\"demands.xlsx not found at: {demands_path}\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_excel(demands_path)\n",
        "    required_cols = {\"demand_id\", \"demand\", \"description\"}\n",
        "    if not required_cols.issubset(df.columns):\n",
        "        logger.error(f\"demands.xlsx missing columns. Required: {required_cols}, got: {set(df.columns)}\")\n",
        "        return\n",
        "\n",
        "    DEMANDS = []\n",
        "    embed_inputs: List[str] = []\n",
        "    for _, row in df.iterrows():\n",
        "        did = str(row[\"demand_id\"]).strip()\n",
        "        name = str(row[\"demand\"]).strip()\n",
        "        desc = str(row[\"description\"]).strip()\n",
        "        if not did or did.lower() == \"nan\":\n",
        "            continue\n",
        "        if not name or name.lower() == \"nan\":\n",
        "            continue\n",
        "        DEMANDS.append({\"id\": did, \"name\": name, \"description\": desc})\n",
        "        embed_inputs.append(f\"Name: {name}\\nClarification: {desc}\")\n",
        "\n",
        "    if not DEMANDS:\n",
        "        logger.error(\"No demands loaded from Excel (DEMANDS is empty).\")\n",
        "        return\n",
        "\n",
        "    t0 = time.time()\n",
        "    emb = client.embeddings.create(model=AZURE_EMBED_DEPLOYMENT, input=embed_inputs)\n",
        "    DEMAND_EMB = _l2_normalize(np.array([e.embedding for e in emb.data], dtype=np.float32))\n",
        "    logger.info(f\"INIT: loaded {len(DEMANDS)} demands. Embedding time: {time.time() - t0:.2f}s\")\n",
        "\n",
        "def _retrieve_candidates(chunk_text: str, k: int) -> List[Dict[str, str]]:\n",
        "    if client is None or DEMAND_EMB is None or not DEMANDS:\n",
        "        return []\n",
        "    emb = client.embeddings.create(model=AZURE_EMBED_DEPLOYMENT, input=[chunk_text])\n",
        "    q = _l2_normalize(np.array([emb.data[0].embedding], dtype=np.float32))\n",
        "    sims = DEMAND_EMB @ q[0]\n",
        "    idx = np.argsort(-sims)[:k]\n",
        "    return [DEMANDS[i] for i in idx]\n",
        "\n",
        "def _llm_classify_chunk(chunk_id: str, chunk_text: str, candidates: List[Dict[str, str]]) -> Dict[str, Any]:\n",
        "    if client is None:\n",
        "        return {\"chunkId\": chunk_id, \"demandIds\": [], \"explanation\": \"AzureOpenAI client not initialized.\"}\n",
        "    if not candidates:\n",
        "        return {\"chunkId\": chunk_id, \"demandIds\": [], \"explanation\": \"No candidates from retrieval.\"}\n",
        "\n",
        "    demands_context = \"\\n\".join(\n",
        "        [f\"- id: {d['id']}\\n  name: {d['name']}\\n  clarification: {d['description']}\" for d in candidates]\n",
        "    )\n",
        "\n",
        "    system = (\n",
        "        \"You label customer requirements in product documentation. \"\n",
        "        \"You MUST only choose from the provided demands. \"\n",
        "        \"Match ONLY when the chunk clearly satisfies the demand's clarification. \"\n",
        "        \"Return at most 3 demands. \"\n",
        "        \"If nothing matches, return an empty demandIds array. \"\n",
        "        \"Probabilities must be in [0.0, 1.0]. \"\n",
        "        \"Return JSON only.\"\n",
        "    )\n",
        "\n",
        "    user = f\"\"\"Demands (id, name, clarification):\n",
        "{demands_context}\n",
        "\n",
        "ChunkId: {chunk_id}\n",
        "Chunk text:\n",
        "{chunk_text}\n",
        "\n",
        "Return JSON in this format exactly:\n",
        "{{\n",
        "  \"chunkId\": \"{chunk_id}\",\n",
        "  \"demandIds\": [{{\"id\":\"<one of provided ids>\",\"probability\":0.85}}],\n",
        "  \"explanation\": \"brief reason\"\n",
        "}}\"\"\".strip()\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=AZURE_CHAT_DEPLOYMENT,\n",
        "        messages=[{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}],\n",
        "    )\n",
        "    content = resp.choices[0].message.content or \"\"\n",
        "    parsed = _safe_json_loads(content)\n",
        "    return parsed or {\"chunkId\": chunk_id, \"demandIds\": [], \"explanation\": \"LLM returned non-JSON.\"}\n",
        "\n",
        "def run(raw_data: Any) -> Dict[str, Any]:\n",
        "    request = json.loads(raw_data) if isinstance(raw_data, str) else raw_data\n",
        "    if not isinstance(request, dict):\n",
        "        raise ValueError(\"Request must be a JSON object/dict.\")\n",
        "    if \"document\" not in request or \"num_preds\" not in request:\n",
        "        raise ValueError(\"Invalid input: expected 'document' and 'num_preds'.\")\n",
        "\n",
        "    document = request[\"document\"]\n",
        "    num_pred = int(request[\"num_preds\"])\n",
        "\n",
        "    by_id = document.get(\"contentDomain\", {}).get(\"byId\", {})\n",
        "    if not isinstance(by_id, dict):\n",
        "        raise ValueError(\"document.contentDomain.byId must be an object/dict.\")\n",
        "\n",
        "    document_demand_predictions = set()\n",
        "\n",
        "    for chunk_id, content in by_id.items():\n",
        "        text = str(content.get(\"text\", \"\") or \"\")[:MAX_TEXT_CHARS]\n",
        "\n",
        "        if client is None or DEMAND_EMB is None or not DEMANDS:\n",
        "            content.update({\"relevantProba\": 0.0, \"cdLogregPredictions\": [], \"cdTransformerPredictions\": []})\n",
        "            continue\n",
        "\n",
        "        candidates = _retrieve_candidates(text, TOP_K_RAG)\n",
        "        llm_out = _llm_classify_chunk(chunk_id, text, candidates)\n",
        "\n",
        "        preds = []\n",
        "        for item in (llm_out.get(\"demandIds\", []) or [])[:MAX_LABELS_PER_CHUNK]:\n",
        "            did = str(item.get(\"id\", \"\")).strip()\n",
        "            proba = _safe_float(item.get(\"probability\", 0.0))\n",
        "            if did and proba >= MIN_KEEP_PROBA:\n",
        "                preds.append({\"label\": did, \"proba\": proba})\n",
        "                document_demand_predictions.add(did)\n",
        "\n",
        "        preds = sorted(preds, key=lambda x: x[\"proba\"], reverse=True)[:num_pred]\n",
        "        relevant_proba = max([p[\"proba\"] for p in preds], default=0.0)\n",
        "\n",
        "        content.update({\"relevantProba\": relevant_proba, \"cdLogregPredictions\": [], \"cdTransformerPredictions\": preds})\n",
        "\n",
        "    document[\"documentDemandPredictions\"] = list(document_demand_predictions)\n",
        "    return {\"predictions\": document}\n",
        "'''\n",
        "pathlib.Path(os.path.join(CODE_DIR, \"score.py\")).write_text(score_py, encoding=\"utf-8\")\n",
        "print(\"Wrote\", os.path.join(CODE_DIR, \"score.py\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy to existing Online Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.entities import Environment, ManagedOnlineDeployment, CodeConfiguration, OnlineRequestSettings\n",
        "\n",
        "credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)\n",
        "\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=SUBSCRIPTION_ID,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WORKSPACE_NAME,\n",
        ")\n",
        "\n",
        "endpoint = ml_client.online_endpoints.get(ENDPOINT_NAME)\n",
        "print(\"Endpoint:\", endpoint.name, \"Traffic:\", endpoint.traffic)\n",
        "\n",
        "ENV_NAME = \"openai-rag-demand-labeler-env\"\n",
        "env = Environment(\n",
        "    name=ENV_NAME,\n",
        "    description=\"PoC env for Azure OpenAI + pandas/openpyxl\",\n",
        "    image=\"mcr.microsoft.com/azureml/minimal-ubuntu20.04-py310-cpu-inference:latest\",\n",
        "    conda_file=\"conda_env.yml\",\n",
        ")\n",
        "env = ml_client.environments.create_or_update(env)\n",
        "print(\"Environment:\", env.name, env.version)\n",
        "\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=NEW_DEPLOYMENT,\n",
        "    endpoint_name=ENDPOINT_NAME,\n",
        "    environment=f\"azureml:{ENV_NAME}@latest\",\n",
        "    code_configuration=CodeConfiguration(code=CODE_DIR, scoring_script=SCORING_SCRIPT),\n",
        "    instance_type=INSTANCE_TYPE,\n",
        "    instance_count=INSTANCE_COUNT,\n",
        "    environment_variables={\n",
        "        \"AZURE_OPENAI_ENDPOINT\": AZURE_OPENAI_ENDPOINT,\n",
        "        \"AZURE_OPENAI_API_KEY\": AZURE_OPENAI_API_KEY,\n",
        "        \"AZURE_OPENAI_API_VERSION\": AZURE_OPENAI_API_VERSION,\n",
        "        \"AZURE_OPENAI_CHAT_DEPLOYMENT\": AZURE_OPENAI_CHAT_DEPLOYMENT,\n",
        "        \"AZURE_OPENAI_EMBED_DEPLOYMENT\": AZURE_OPENAI_EMBED_DEPLOYMENT,\n",
        "        \"TOP_K_RAG\": str(TOP_K_RAG),\n",
        "        \"MAX_LABELS_PER_CHUNK\": str(MAX_LABELS_PER_CHUNK),\n",
        "        \"MIN_KEEP_PROBA\": str(MIN_KEEP_PROBA),\n",
        "        \"MAX_TEXT_CHARS\": str(MAX_TEXT_CHARS),\n",
        "    },\n",
        "    request_settings=OnlineRequestSettings(request_timeout_ms=180000),\n",
        ")\n",
        "\n",
        "dep = ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
        "print(\"Deployment:\", dep.name, dep.provisioning_state)\n",
        "\n",
        "print(\"Deployment logs (tail):\")\n",
        "print(ml_client.online_deployments.get_logs(name=NEW_DEPLOYMENT, endpoint_name=ENDPOINT_NAME, lines=200))\n",
        "\n",
        "if SWITCH_TRAFFIC_TO_NEW:\n",
        "    ep = ml_client.online_endpoints.get(ENDPOINT_NAME)\n",
        "    ep.traffic = {NEW_DEPLOYMENT: 100}\n",
        "    ml_client.online_endpoints.begin_create_or_update(ep).result()\n",
        "    print(\"Traffic updated.\")\n",
        "\n",
        "print(\"Final traffic:\", ml_client.online_endpoints.get(ENDPOINT_NAME).traffic)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}