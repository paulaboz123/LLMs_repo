{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PoC: Klasyfikacja zdań do 150 demandów z użyciem opisów labeli (bez RAG, bez OpenAI)\n",
        "\n",
        "**Cel**: wykorzystać nową kolumnę `demand_desc` jako \"prototypy klas\" i klasyfikować `text` poprzez największe podobieństwo cosine do embeddingów opisów labeli.\n",
        "Opcjonalnie: self-training na danych nielabelowanych.\n",
        "\n",
        "Założenia o danych:\n",
        "- kolumny: `text`, `demand_id`, `demand_desc`\n",
        "- `demand_id` jest puste/NaN dla wierszy niepolabelowanych\n",
        "- `demand_desc` zawiera opis znaczenia demanda (może powtarzać się w wielu wierszach dla tego samego `demand_id`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip -q install -U \"sentence-transformers>=3.0.0\" \"scikit-learn>=1.4.0\" \"pandas>=2.0.0\" \"numpy>=1.24.0\" \"tqdm\" \"openpyxl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importy + konfiguracja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wczytanie danych (CSV lub XLSX)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "DATA_PATH = \"dataset.csv\"  # albo: \"dataset.xlsx\"\n",
        "\n",
        "if DATA_PATH.lower().endswith(\".xlsx\"):\n",
        "    df = pd.read_excel(DATA_PATH)\n",
        "else:\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "required_cols = {\"text\", \"demand_id\", \"demand_desc\"}\n",
        "missing = required_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Brak wymaganych kolumn: {missing}\")\n",
        "\n",
        "df[\"text\"] = df[\"text\"].astype(str)\n",
        "df[\"demand_id\"] = df[\"demand_id\"].replace(\"\", np.nan)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Labeled:\", df[\"demand_id\"].notna().sum(), \"Unlabeled:\", df[\"demand_id\"].isna().sum())\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mapa: demand_id -> demand_desc (prototypy klas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "labeled = df[df[\"demand_id\"].notna()].copy()\n",
        "unlabeled = df[df[\"demand_id\"].isna()].copy()\n",
        "\n",
        "label_desc = (\n",
        "    labeled[[\"demand_id\", \"demand_desc\"]]\n",
        "    .dropna()\n",
        "    .drop_duplicates(subset=[\"demand_id\"])\n",
        "    .set_index(\"demand_id\")[\"demand_desc\"]\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "all_labels = sorted(label_desc.keys())\n",
        "print(\"Unique labeled classes:\", len(all_labels))\n",
        "\n",
        "missing_desc = set(labeled[\"demand_id\"].unique()) - set(label_desc.keys())\n",
        "print(\"Labels missing desc:\", len(missing_desc))\n",
        "if missing_desc:\n",
        "    print(\"Examples:\", list(missing_desc)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split train/val/test\n",
        "\n",
        "Uwaga: przy 150 klasach i 3k przykładów niektóre klasy mogą być bardzo rzadkie; w razie problemów ze stratyfikacją robimy fallback."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "X = labeled[\"text\"].values\n",
        "y = labeled[\"demand_id\"].values\n",
        "\n",
        "def safe_stratified_split(X, y, test_size, seed):\n",
        "    try:\n",
        "        return train_test_split(X, y, test_size=test_size, random_state=seed, stratify=y)\n",
        "    except ValueError:\n",
        "        return train_test_split(X, y, test_size=test_size, random_state=seed, stratify=None)\n",
        "\n",
        "X_train, X_tmp, y_train, y_tmp = safe_stratified_split(X, y, test_size=0.30, seed=SEED)\n",
        "X_val, X_test, y_val, y_test = safe_stratified_split(X_tmp, y_tmp, test_size=0.50, seed=SEED)\n",
        "\n",
        "print(len(X_train), len(X_val), len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model embeddingowy\n",
        "\n",
        "Wybór startowy: `intfloat/multilingual-e5-base` (dobry dla PL/EN).  \n",
        "Możesz podmienić na np. `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "BASE_MODEL = \"intfloat/multilingual-e5-base\"\n",
        "model = SentenceTransformer(BASE_MODEL, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embeddingi opisów labeli i predykcja przez max cosine similarity\n",
        "\n",
        "Tu *nie* porównujemy opisu z \"innym opisem\" w sensie treningu na parach z tego samego rzędu.\n",
        "Robimy klasyfikację: **tekst zdania** (query) porównujemy do **wszystkich opisów klas** (passage) i wybieramy klasę o największym podobieństwie.\n",
        "\n",
        "To jest klasyfikator typu \"prototypowego\": opis labela jest reprezentantem klasy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def embed_texts(st_model, texts, batch_size=128):\n",
        "    embs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        embs.append(st_model.encode(batch, convert_to_tensor=True, normalize_embeddings=True))\n",
        "    return torch.cat(embs, dim=0)\n",
        "\n",
        "label_texts = [f\"passage: {label_desc[l]}\" for l in all_labels]\n",
        "label_emb = embed_texts(model, label_texts, batch_size=128)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_labels(st_model, texts, label_emb, all_labels, batch_size=128):\n",
        "    preds = []\n",
        "    scores = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predict\"):\n",
        "        batch = [f\"query: {t}\" for t in texts[i:i+batch_size]]\n",
        "        text_emb = st_model.encode(batch, convert_to_tensor=True, normalize_embeddings=True)\n",
        "        sims = cos_sim(text_emb, label_emb)  # [B, L]\n",
        "        best = torch.argmax(sims, dim=1).cpu().numpy()\n",
        "        best_scores = torch.max(sims, dim=1).values.cpu().numpy()\n",
        "        preds.extend([all_labels[j] for j in best])\n",
        "        scores.extend(best_scores.tolist())\n",
        "    return np.array(preds), np.array(scores)\n",
        "\n",
        "val_pred, _ = predict_labels(model, X_val, label_emb, all_labels)\n",
        "test_pred, _ = predict_labels(model, X_test, label_emb, all_labels)\n",
        "\n",
        "print(\"VAL macro F1 :\", f1_score(y_val, val_pred, average=\"macro\"))\n",
        "print(\"VAL micro F1 :\", f1_score(y_val, val_pred, average=\"micro\"))\n",
        "print(\"TEST macro F1:\", f1_score(y_test, test_pred, average=\"macro\"))\n",
        "print(\"TEST micro F1:\", f1_score(y_test, test_pred, average=\"micro\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Raport (może być długi dla 150 klas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(classification_report(y_test, test_pred, digits=3, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Opcjonalnie) Self-training na unlabeled (pseudo-labeling)\n",
        "\n",
        "Bierzemy tylko pewne predykcje (próg podobieństwa oraz margines top1-top2), dokładamy do treningu lub do ręcznej walidacji.\n",
        "W tym prostym notebooku robimy tylko **prelabeling + zapis wyników**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "THRESH = 0.40\n",
        "MARGIN = 0.05\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_top2(st_model, texts, label_emb, all_labels, batch_size=128):\n",
        "    top1_label, top1_score, top2_score = [], [], []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predict top2\"):\n",
        "        batch = [f\"query: {t}\" for t in texts[i:i+batch_size]]\n",
        "        text_emb = st_model.encode(batch, convert_to_tensor=True, normalize_embeddings=True)\n",
        "        sims = cos_sim(text_emb, label_emb)  # [B, L]\n",
        "        vals, idxs = torch.topk(sims, k=2, dim=1)\n",
        "        vals = vals.cpu().numpy()\n",
        "        idxs = idxs.cpu().numpy()\n",
        "        top1_label.extend([all_labels[j] for j in idxs[:,0]])\n",
        "        top1_score.extend(vals[:,0].tolist())\n",
        "        top2_score.extend(vals[:,1].tolist())\n",
        "    return np.array(top1_label), np.array(top1_score), np.array(top2_score)\n",
        "\n",
        "if len(unlabeled) > 0:\n",
        "    ul_texts = unlabeled[\"text\"].astype(str).tolist()\n",
        "    ul_pred, ul_s1, ul_s2 = predict_top2(model, ul_texts, label_emb, all_labels)\n",
        "    keep = (ul_s1 >= THRESH) & ((ul_s1 - ul_s2) >= MARGIN)\n",
        "    print(\"Pseudo accepted:\", int(keep.sum()), \"out of\", len(unlabeled))\n",
        "else:\n",
        "    print(\"No unlabeled rows found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zapis predykcji dla całego datasetu (labeled + unlabeled)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "all_texts = df[\"text\"].astype(str).tolist()\n",
        "pred_all, score_all = predict_labels(model, all_texts, label_emb, all_labels)\n",
        "\n",
        "out = df.copy()\n",
        "out[\"pred_demand_id\"] = pred_all\n",
        "out[\"pred_score\"] = score_all\n",
        "\n",
        "OUT_PATH = \"predictions.parquet\"\n",
        "out.to_parquet(OUT_PATH, index=False)\n",
        "\n",
        "print(\"Saved:\", OUT_PATH)\n",
        "out.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Co dalej (praktycznie)\n",
        "\n",
        "1) Jeśli baseline (bez fine-tuningu) jest słaby, następnym krokiem jest fine-tuning bi-encodera na parach (text, demand_desc) z MultipleNegativesRankingLoss.  \n",
        "2) Można też trenować klasyczny transformer-classifier, ale **dokleić** opis labela do wejścia (np. text + [SEP] desc) i uczyć „czy pasuje” (cross-encoder) — to bywa mocniejsze, ale wolniejsze w inferencji.  \n",
        "W tym notebooku zostawiliśmy wariant najprostszy i najszybszy w PoC."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}