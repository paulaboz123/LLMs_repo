import json
import logging
import os
import time
from typing import Any, Dict, List, Tuple

import pandas as pd
from openai import AzureOpenAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("score")

# ---------- Globals ----------
client: AzureOpenAI = None
CHAT_DEPLOYMENT: str = None
DEMANDS: List[Dict[str, str]] = []  # [{id, demand, description}]


# ---------- Utils ----------
def _env(name: str, required: bool = True, default: str = None) -> str:
    v = os.getenv(name, default)
    if required and not v:
        raise RuntimeError(f"Missing env var: {name}")
    return v

def _json_load_maybe(x: Any) -> Any:
    """If x is JSON string -> parse. If dict/list -> return. Else return as-is."""
    if isinstance(x, (bytes, bytearray)):
        x = x.decode("utf-8", errors="replace")
    if isinstance(x, str):
        s = x.strip()
        if not s:
            return None
        try:
            return json.loads(s)
        except Exception:
            # not json, return raw string
            return x
    return x

def _safe_content_items(document: Dict[str, Any]) -> List[Dict[str, Any]]:
    cd = (document or {}).get("contentDomain", {})
    by_id = cd.get("byId", {})
    if isinstance(by_id, dict):
        return list(by_id.values())
    return []

def _load_demands_xlsx() -> List[Dict[str, str]]:
    here = os.path.dirname(os.path.abspath(__file__))
    candidates = [
        os.path.join(here, "demands.xlsx"),
        os.path.join(here, "assets", "demands.xlsx"),
    ]
    model_dir = os.getenv("AZUREML_MODEL_DIR")
    if model_dir:
        candidates.extend([
            os.path.join(model_dir, "demands.xlsx"),
            os.path.join(model_dir, "assets", "demands.xlsx"),
        ])

    for p in candidates:
        if os.path.exists(p):
            df = pd.read_excel(p)
            df.columns = [c.strip().lower() for c in df.columns]
            need = {"demand_id", "demand", "description"}
            miss = need - set(df.columns)
            if miss:
                raise RuntimeError(f"demands.xlsx missing columns: {miss}. Found: {list(df.columns)}")
            out = []
            for _, r in df.iterrows():
                out.append({
                    "id": str(r["demand_id"]).strip(),
                    "demand": str(r["demand"]).strip(),
                    "description": str(r["description"]).strip(),
                })
            logger.info(f"Loaded demands: {len(out)} from {p}")
            return out

    raise RuntimeError(f"demands.xlsx not found. Tried: {candidates}")

def _compact_catalog(max_desc_chars: int = 180) -> str:
    lines = []
    for d in DEMANDS:
        desc = d["description"] or ""
        if len(desc) > max_desc_chars:
            desc = desc[:max_desc_chars] + "…"
        lines.append(f'{d["id"]}: {d["demand"]} | {desc}')
    return "\n".join(lines)

def _llm_predict(text: str, top_k: int) -> Tuple[float, List[Dict[str, Any]]]:
    """
    Returns:
      relevant_proba: float
      cdTransformerPredictions: [{label:<demand_id>, proba:<float>}]
    """
    t = (text or "").strip()
    if len(t) < 4:
        return 0.0, []

    catalog = _compact_catalog()

    system = (
        "Return STRICT JSON only. "
        "You classify whether TEXT is a customer requirement and map it to demand IDs."
    )
    user = f"""
TEXT:
{t}

DEMANDS (id: demand | description):
{catalog}

Return JSON ONLY:
{{
  "relevantProba": 0.0,
  "predictions": [{{"id":"<demand_id>", "probability":0.0}}]
}}

Rules:
- relevantProba is 0..1
- If relevantProba < 0.5 => predictions must be []
- Otherwise return up to {top_k} predictions, sorted by probability desc
- prediction.id must be one of DEMANDS ids
""".strip()

    resp = client.chat.completions.create(
        model=CHAT_DEPLOYMENT,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
        # NIE ustawiam temperature=0, bo widziałaś błąd "temperature does not support 0"
    )

    content = (resp.choices[0].message.content or "").strip()
    try:
        data = json.loads(content)
    except Exception:
        logger.warning("LLM returned non-JSON. content=%r", content[:500])
        return 0.0, []

    rel = float(data.get("relevantProba", 0.0) or 0.0)
    raw = data.get("predictions", []) or []

    allowed = {d["id"] for d in DEMANDS}
    preds: List[Dict[str, Any]] = []
    if rel >= 0.5:
        for p in raw[:top_k]:
            did = str(p.get("id", "")).strip()
            try:
                pr = float(p.get("probability"))
            except Exception:
                continue
            if did and did in allowed:
                preds.append({"label": did, "proba": max(0.0, min(1.0, pr))})

    rel = max(0.0, min(1.0, rel))
    return rel, preds

def _inference(document: Dict[str, Any], num_preds: int) -> Dict[str, Any]:
    items = _safe_content_items(document)
    logger.info("content items=%d", len(items))

    doc_ids = set()

    for content in items:
        text = content.get("text", "")
        rel, preds = _llm_predict(text, top_k=num_preds)

        # Zostawiamy stare pola (kompatybilność)
        content["relevantProba"] = rel
        content["cdLogregPredictions"] = []              # placeholder
        content["cdTransformerPredictions"] = preds       # label=demand_id

        if preds:
            doc_ids.add(preds[0]["label"])

    # Krytyczne: apka parsuje to jako JsonArray
    document["documentDemandPredictions"] = sorted(doc_ids)  # list[str]
    return document


# ---------- Azure ML entrypoints ----------
def init():
    global client, CHAT_DEPLOYMENT, DEMANDS

    # UWAGA: nazwy env vars muszą być IDENTYCZNE jak w deployment.environment_variables
    endpoint = _env("AZURE_OPENAI_ENDPOINT")
    api_key = _env("AZURE_OPENAI_API_KEY")
    api_version = _env("AZURE_OPENAI_API_VERSION")
    CHAT_DEPLOYMENT = _env("AZURE_OPENAI_CHAT_DEPLOYMENT")  # nazwa deploymentu w Azure OpenAI

    client = AzureOpenAI(
        azure_endpoint=endpoint,
        api_key=api_key,
        api_version=api_version,
    )

    DEMANDS = _load_demands_xlsx()
    logger.info("init OK. demands=%d", len(DEMANDS))

def run(raw_data):
    """
    Expected body from app:
    {
      "document": <object OR json-string>,
      "model": "<string>",
      "num_preds": <int>
    }

    Must return something the app can read at:
    json["predictions"]["documentDemandPredictions"] OR json["documentDemandPredictions"]
    """
    t0 = time.time()
    try:
        req = _json_load_maybe(raw_data)
        if not isinstance(req, dict):
            return {"error": "Bad request: body must be JSON object"}

        if "document" not in req or "num_preds" not in req:
            return {"error": "Bad request: expected fields: document, num_preds"}

        document = _json_load_maybe(req["document"])
        if isinstance(document, str):
            # if it was a non-json string, treat as error
            return {"error": "Bad request: 'document' must be JSON (object or JSON string)"}
        if not isinstance(document, dict):
            return {"error": f"Bad request: 'document' wrong type: {type(document)}"}

        num_preds = int(req["num_preds"])
        out_doc = _inference(document, num_preds)

        logger.info("run OK in %.2fs | docPreds=%d",
                    time.time() - t0, len(out_doc.get("documentDemandPredictions", [])))

        # Najbezpieczniej pod apkę: opakowanie w "predictions"
        return {"predictions": out_doc}

    except Exception as e:
        logger.exception("run() failed")
        # To często ratuje Ci życie: response body pokaże błąd, jeśli apka go loguje
        return {"error": str(e)}

