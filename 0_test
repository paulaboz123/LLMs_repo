import os
import json
import time
import logging
from typing import Dict, Any, List, Tuple

import numpy as np
import pandas as pd
from openai import AzureOpenAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===== Globals (initialized in init) =====
client: AzureOpenAI = None
EMBED_DEPLOYMENT: str = None

DEMANDS: List[Dict[str, str]] = []
DEMAND_EMB: np.ndarray = None  # (N, D)

# relevance prototypes
REL_EMB: np.ndarray = None     # (D,)
IRREL_EMB: np.ndarray = None   # (D,)


# -----------------------------
# Helpers
# -----------------------------
def _safe_norm(x: np.ndarray) -> float:
    return float(np.linalg.norm(x) + 1e-12)

def _cos_sim(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b) / (_safe_norm(a) * _safe_norm(b)))

def _cos_sim_vec_to_mat(v: np.ndarray, mat: np.ndarray) -> np.ndarray:
    v = v / _safe_norm(v)
    mat = mat / (np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12)
    return mat @ v  # (N,)

def _sim_to_proba(sim: float) -> float:
    # cosine [-1..1] -> [0..1] + lekkie "wyostrzenie"
    s = (sim + 1.0) / 2.0
    s = s ** 2
    return max(0.0, min(1.0, float(s)))

def _embed(texts: List[str]) -> List[List[float]]:
    resp = client.embeddings.create(model=EMBED_DEPLOYMENT, input=texts)
    return [d.embedding for d in resp.data]

def _load_demands_xlsx(path: str) -> List[Dict[str, str]]:
    df = pd.read_excel(path)
    df = df.rename(columns={c: c.strip() for c in df.columns})

    expected = {"demand_id", "demand", "description"}
    missing = expected - set(df.columns)
    if missing:
        raise ValueError(f"demands.xlsx missing columns: {sorted(list(missing))}. Found: {list(df.columns)}")

    out = []
    for _, r in df.iterrows():
        did = str(r["demand_id"]).strip()
        if not did or did.lower() == "nan":
            continue
        out.append({
            "id": did,
            "demand": str(r["demand"]).strip(),
            "description": "" if pd.isna(r["description"]) else str(r["description"]).strip()
        })
    if not out:
        raise ValueError("demands.xlsx loaded but has 0 valid rows (check demand_id).")
    return out

def _demand_text(d: Dict[str, str]) -> str:
    # To embed label semantics
    if d["description"]:
        return f"{d['demand']}\n{d['description']}"
    return d["demand"]

def _pick_content_store(document: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
    # app może wysyłać contentDomain albo content – obsługujemy oba
    for store in ("contentDomain", "content"):
        node = document.get(store)
        if isinstance(node, dict) and isinstance(node.get("byId"), dict):
            return store, node["byId"]
    raise ValueError("Expected document.contentDomain.byId OR document.content.byId")

def _relevance_proba(text_emb: np.ndarray) -> float:
    """
    Binary relevance via 2 prototypy embeddingowe: relevant vs irrelevant.
    Zwraca [0..1] jako relevantProba.
    """
    s_rel = _cos_sim(text_emb, REL_EMB)
    s_irr = _cos_sim(text_emb, IRREL_EMB)
    # różnica podobieństw -> skala [0..1]
    # im bardziej bliżej REL niż IRR, tym większe proba
    delta = s_rel - s_irr  # zwykle w okolicy [-0.5..0.5]
    # mapowanie sigmoidopodobne bez importu scipy
    # 0.0 -> ~0.5, 0.2 -> ~0.62, 0.4 -> ~0.73
    proba = 1.0 / (1.0 + np.exp(-4.0 * delta))
    return float(max(0.0, min(1.0, proba)))


# -----------------------------
# Old-style API
# -----------------------------
def init():
    global client, EMBED_DEPLOYMENT, DEMANDS, DEMAND_EMB, REL_EMB, IRREL_EMB

    logger.info("Initializing...")

    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT", "").strip()
    api_key = os.environ.get("AZURE_OPENAI_API_KEY", "").strip()
    api_version = os.environ.get("AZURE_OPENAI_API_VERSION", "").strip()
    EMBED_DEPLOYMENT = os.environ.get("AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT", "").strip()

    if not (azure_endpoint and api_key and api_version and EMBED_DEPLOYMENT):
        raise RuntimeError(
            "Missing env vars. Need: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, "
            "AZURE_OPENAI_API_VERSION, AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT"
        )

    client = AzureOpenAI(
        azure_endpoint=azure_endpoint,
        api_key=api_key,
        api_version=api_version,
    )

    base_dir = os.path.dirname(os.path.abspath(__file__))
    demands_path = os.environ.get("DEMANDS_XLSX_PATH", os.path.join(base_dir, "demands.xlsx"))

    DEMANDS = _load_demands_xlsx(demands_path)
    logger.info("Loaded %d demands", len(DEMANDS))

    # Precompute label embeddings
    label_texts = [_demand_text(d) for d in DEMANDS]
    DEMAND_EMB = np.array(_embed(label_texts), dtype=np.float32)

    # Relevance prototypes (tunable; ważne, żeby pasowało do Twoich PDFów)
    relevant_examples = [
        "The motor shall be fitted with local start/stop station.",
        "Vendor shall submit inspection notification to purchaser.",
        "Motor efficiency class shall be IE3 or better.",
        "Ingress protection class IP55 is required.",
        "Bearings shall have minimum lifetime of 40,000 hours.",
        "Vendor shall provide routine test report."
    ]
    irrelevant_examples = [
        "Table of contents.",
        "This document is confidential.",
        "Page 2 of 48.",
        "General information and introduction.",
        "Definitions and abbreviations.",
        "Scope of supply overview."
    ]
    rel = np.array(_embed(relevant_examples), dtype=np.float32).mean(axis=0)
    irr = np.array(_embed(irrelevant_examples), dtype=np.float32).mean(axis=0)
    REL_EMB = rel
    IRREL_EMB = irr

    logger.info("Initialized OK. DEMAND_EMB=%s", DEMAND_EMB.shape)


def is_relevant_customer_demand(text: str, relevant_proba: float, cd_logreg_proba: float, cd_transformer_proba: float) -> bool:
    """
    Zostawiam sygnaturę jak u Ciebie, ale sens teraz jest:
    - relevant_proba: z OpenAI relevance
    - cd_logreg_proba / cd_transformer_proba: z OpenAI label match (top1)
    """
    if not text or len(text.split()) <= 2:
        return False
    # progi do tuningu; na start konserwatywnie (high precision)
    return (
        relevant_proba >= 0.65 and
        cd_transformer_proba >= 0.80
    )


def inference(document: Dict[str, Any], num_cd_predictions: int) -> Dict[str, Any]:
    start_time = time.time()

    store_name, by_id = _pick_content_store(document)
    logger.info("Scoring store=%s items=%d", store_name, len(by_id))

    document_demand_predictions = set()

    # embed per chunk: 1 request per chunk (możesz batchować później)
    for _, content in by_id.items():
        if not isinstance(content, dict):
            continue

        text = content.get("text", "")
        if not isinstance(text, str) or not text.strip():
            # zapewnij pola, żeby UI się nie wywaliło
            content.setdefault("relevantProba", 0.0)
            content.setdefault("cdLogregPredictions", [])
            content.setdefault("cdTransformerPredictions", [])
            continue

        text_emb = np.array(_embed([text])[0], dtype=np.float32)

        # --- 1) Relevance (OpenAI embeddings)
        relevant_proba = _relevance_proba(text_emb)

        # --- 2) Label classification (OpenAI embeddings)
        sims = _cos_sim_vec_to_mat(text_emb, DEMAND_EMB)  # (N,)
        topk = max(1, int(num_cd_predictions))
        idx = np.argsort(-sims)[:topk]

        cd_transformer_predictions = []
        for i in idx:
            proba = _sim_to_proba(float(sims[i]))
            # UWAGA: label musi być demand_id (nie tekst)
            cd_transformer_predictions.append({
                "label": DEMANDS[int(i)]["id"],
                "proba": float(round(proba, 4))
            })

        cd_transformer_predictions.sort(key=lambda x: x["proba"], reverse=True)

        # “pseudo-logreg”: zostawiamy ten sam format listy predykcji
        # (jeśli appka ma gdzieś check na cdLogregPredictions[0]["proba"])
        cd_logreg_predictions = cd_transformer_predictions[:1]

        # stary kod miał dodatkowo gating (is_relevant_customer_demand)
        top1_proba = cd_transformer_predictions[0]["proba"] if cd_transformer_predictions else 0.0
        add_to_doc = is_relevant_customer_demand(
            text=text,
            relevant_proba=relevant_proba,
            cd_logreg_proba=cd_logreg_predictions[0]["proba"] if cd_logreg_predictions else 0.0,
            cd_transformer_proba=top1_proba
        )

        # update content fields (jak w starym score.py)
        content.update({
            "relevantProba": float(round(relevant_proba, 4)),
            "cdLogregPredictions": cd_logreg_predictions,
            "cdTransformerPredictions": cd_transformer_predictions,
        })

        if add_to_doc and cd_transformer_predictions:
            document_demand_predictions.add(cd_transformer_predictions[0]["label"])

    document["documentDemandPredictions"] = list(document_demand_predictions)

    latency = round(time.time() - start_time, 3)
    logger.info("Inference done in %ss", latency)

    return document


def run(raw_data):
    """
    Zostawiam wrapper jak w Twoim starym kodzie: zwracamy {"predictions": document}
    """
    try:
        if raw_data is None:
            return {"error": "Empty request body"}

        if isinstance(raw_data, (bytes, bytearray)):
            raw_data = raw_data.decode("utf-8", errors="replace")

        if isinstance(raw_data, str):
            raw_data = raw_data.strip()
            if not raw_data:
                return {"error": "Empty request body"}
            request_data = json.loads(raw_data)
        elif isinstance(raw_data, dict):
            request_data = raw_data
        else:
            return {"error": f"Unsupported request type: {type(raw_data)}"}

        # obsłuż oba warianty: {"document": ..., "num_preds": N} lub dokument bez wrappera
        if "document" in request_data:
            document = request_data["document"]
            num_pred = int(request_data.get("num_preds", request_data.get("numPreds", 3)) or 3)
        else:
            document = request_data
            num_pred = 3

        response = inference(document, num_pred)

        if isinstance(response, dict):
            return {"predictions": response}
        else:
            return {"predictions": response}

    except Exception as e:
        logger.exception("run() failed")
        # nie wywalaj kontenera — zwróć błąd, żeby dało się to zdebugować
        return {"error": "Internal server error", "detail": str(e)}
