import json
import os
import re
import time
import logging
from typing import Any, Dict, List, Tuple, Optional

import pandas as pd

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# -----------------------------
# Globals (initialized in init)
# -----------------------------
CLIENT = None
DEMANDS: List[Dict[str, str]] = []
DEMANDS_CONTEXT: str = ""

# Tuning knobs
RELEVANCE_THRESHOLD = float(os.getenv("RELEVANCE_THRESHOLD", "0.45"))
MAX_DEMANDS_PER_CHUNK = int(os.getenv("MAX_DEMANDS_PER_CHUNK", "3"))
MIN_DEMAND_PROB = float(os.getenv("MIN_DEMAND_PROB", "0.30"))

# Azure OpenAI config (your env name)
AOAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AOAI_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AOAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
AOAI_DEPLOYMENT = (
    os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")
    or os.getenv("AZURE_OPENAI_DEPLOYMENT")
)

_JSON_OBJ_RE = re.compile(r"\{.*\}", re.DOTALL)


def _here() -> str:
    return os.path.dirname(os.path.abspath(__file__))


def _safe_json_extract(text: str) -> Dict[str, Any]:
    """
    Extract first JSON object from model output (handles extra text/fences).
    """
    text = (text or "").strip()
    if not text:
        return {}

    # strip ``` fences
    if text.startswith("```"):
        text = text.strip("`").strip()
        parts = text.splitlines()
        if parts and re.match(r"^[a-zA-Z]+$", parts[0].strip()):
            text = "\n".join(parts[1:]).strip()

    try:
        return json.loads(text)
    except Exception:
        m = _JSON_OBJ_RE.search(text)
        if not m:
            return {}
        try:
            return json.loads(m.group(0))
        except Exception:
            return {}


def _load_demands_xlsx() -> List[Dict[str, str]]:
    """
    Only XLSX. Default: demands.xlsx next to score.py, or DEMANDS_PATH.
    Required columns (case-insensitive):
      - demand_id OR id
      - demand
      - description   (you said this is the column name)
    """
    p = os.getenv("DEMANDS_PATH", "").strip()
    if p:
        if not os.path.isabs(p):
            p = os.path.join(_here(), p)
    else:
        p = os.path.join(_here(), "demands.xlsx")

    if not os.path.exists(p):
        raise FileNotFoundError(
            f"demands.xlsx not found at {p}. Put demands.xlsx next to score.py or set DEMANDS_PATH."
        )

    df = pd.read_excel(p)
    if df is None or df.empty:
        raise ValueError(f"demands.xlsx is empty: {p}")

    df.columns = [str(c).strip().lower() for c in df.columns]

    id_col = "demand_id" if "demand_id" in df.columns else ("id" if "id" in df.columns else None)
    if not id_col:
        raise ValueError(f"Missing demand_id/id in demands.xlsx. Found: {list(df.columns)}")
    if "demand" not in df.columns:
        raise ValueError(f"Missing demand in demands.xlsx. Found: {list(df.columns)}")
    if "description" not in df.columns:
        raise ValueError(f"Missing description in demands.xlsx. Found: {list(df.columns)}")

    out: List[Dict[str, str]] = []
    for _, r in df.fillna("").iterrows():
        did = str(r.get(id_col, "")).strip()
        name = str(r.get("demand", "")).strip()
        desc = str(r.get("description", "")).strip()
        if did:
            out.append({"id": did, "demand": name, "description": desc})

    if not out:
        raise ValueError("No valid rows in demands.xlsx after parsing.")

    logger.info("Loaded %d demands from %s", len(out), p)
    return out


def _build_demands_context(demands: List[Dict[str, str]]) -> str:
    """
    CRITICAL: must include ID explicitly so the model can output demandIds.
    """
    lines = []
    for d in demands:
        desc = (d.get("description") or "").strip()
        # truncate to keep prompt size sane
        if len(desc) > 450:
            desc = desc[:450] + "â€¦"
        lines.append(
            f"- id: {d['id']}\n"
            f"  name: {d.get('demand','')}\n"
            f"  description: {desc}"
        )
    return "\n".join(lines)


def _init_openai_client():
    try:
        from openai import AzureOpenAI
    except Exception as e:
        raise RuntimeError("Missing python package 'openai' with AzureOpenAI client.") from e

    if not AOAI_ENDPOINT or not AOAI_KEY or not AOAI_DEPLOYMENT:
        raise RuntimeError(
            "Missing env vars. Need AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, "
            "AZURE_OPENAI_CHAT_DEPLOYMENT (or AZURE_OPENAI_DEPLOYMENT)."
        )

    return AzureOpenAI(
        azure_endpoint=AOAI_ENDPOINT,
        api_key=AOAI_KEY,
        api_version=AOAI_API_VERSION,
    )


def init():
    global CLIENT, DEMANDS, DEMANDS_CONTEXT
    logger.info("init(): starting")
    DEMANDS = _load_demands_xlsx()
    DEMANDS_CONTEXT = _build_demands_context(DEMANDS)
    CLIENT = _init_openai_client()
    logger.info(
        "init(): done. deployment=%s threshold=%.2f max_demands_per_chunk=%d",
        AOAI_DEPLOYMENT, RELEVANCE_THRESHOLD, MAX_DEMANDS_PER_CHUNK
    )


def _prompt_for_chunk(chunk_id: str, chunk_text: str, num_preds: int) -> Tuple[str, str]:
    """
    We explicitly model the old flow:
      - relevance (like logreg)
      - then demand classification (like transformer)
    """
    system = (
        "You are a strict requirements classifier for bid documentation. "
        "Return ONLY valid JSON. Never invent demand IDs."
    )

    user = f"""
You must do TWO steps:

Step 1) Relevance (like a relevance classifier):
- Decide if this chunk contains a concrete customer requirement/specification that should be highlighted.
- If NOT relevant: set relevantProbability < 0.30 and return empty demandIds.

Step 2) Demand matching:
- If relevant: choose up to {min(num_preds, MAX_DEMANDS_PER_CHUNK)} best matching demands from the list below.
- You MUST output demand IDs EXACTLY as listed (do not invent).
- Use demand DESCRIPTION to decide WHEN to match. Name is only a hint.

Demands:
{DEMANDS_CONTEXT}

Chunk:
chunkId: {chunk_id}
text: {chunk_text}

Return JSON ONLY in this exact format:
{{
  "chunkId": "{chunk_id}",
  "relevantProbability": 0.0,
  "demandIds": [{{"id":"<one_of_the_listed_ids>","probability":0.85}}],
  "explanation": "brief"
}}
""".strip()

    return system, user


def _classify_chunk(chunk_id: str, chunk_text: str, num_preds: int) -> Tuple[float, List[Dict[str, float]]]:
    assert CLIENT is not None

    sys_msg, user_msg = _prompt_for_chunk(chunk_id, chunk_text, num_preds)

    resp = CLIENT.chat.completions.create(
        model=AOAI_DEPLOYMENT,
        messages=[
            {"role": "system", "content": sys_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.0,
        max_tokens=900,
    )

    obj = _safe_json_extract((resp.choices[0].message.content or ""))
    rel = float(obj.get("relevantProbability") or 0.0)

    demand_ids = obj.get("demandIds") or []
    preds: List[Dict[str, float]] = []
    if isinstance(demand_ids, list):
        for d in demand_ids:
            if not isinstance(d, dict):
                continue
            did = str(d.get("id") or "").strip()
            if not did:
                continue
            try:
                p = float(d.get("probability") or 0.0)
            except Exception:
                p = 0.0
            if p >= MIN_DEMAND_PROB:
                preds.append({"id": did, "probability": p})

    preds.sort(key=lambda x: x["probability"], reverse=True)
    preds = preds[: min(num_preds, MAX_DEMANDS_PER_CHUNK)]

    # if not relevant => no labels
    if rel < 0.30:
        preds = []

    return rel, preds


def _iter_chunks(document: Dict[str, Any]) -> List[Tuple[str, Dict[str, Any]]]:
    cd = document.get("contentDomain") or {}
    by_id = cd.get("byId") or {}
    if not isinstance(by_id, dict):
        return []
    return [(str(k), v) for (k, v) in by_id.items() if isinstance(v, dict)]


def _apply_chunk_updates(content: Dict[str, Any], rel: float, preds: List[Dict[str, float]]):
    """
    This is the critical part for highlight:
    keep legacy fields and shapes.
    """
    content["relevantProba"] = float(rel)

    if rel >= RELEVANCE_THRESHOLD and preds:
        shaped = [{"label": p["id"], "proba": float(p["probability"])} for p in preds]
        # these names match the old solution shape
        content["cdTransformerPredictions"] = shaped
        content["cdLogregPredictions"] = shaped
    else:
        content["cdTransformerPredictions"] = []
        content["cdLogregPredictions"] = []


def inference(document: Dict[str, Any], num_preds: int) -> Dict[str, Any]:
    """
    Updates the same input document (in-place) and returns it,
    preserving the working app contract:
      return {"predictions": <document>}
    """
    start = time.time()
    global_ids = set()

    chunks = _iter_chunks(document)
    for chunk_id, content in chunks:
        text = str(content.get("text") or "").strip()

        # Ensure legacy keys exist (avoids UI null reference patterns)
        content.setdefault("relevantProba", 0.0)
        content.setdefault("cdTransformerPredictions", [])
        content.setdefault("cdLogregPredictions", [])

        if len(text) < 10:
            _apply_chunk_updates(content, 0.0, [])
            continue

        try:
            rel, preds = _classify_chunk(chunk_id, text, num_preds)
        except Exception as e:
            logger.exception("Chunk %s failed: %s", chunk_id, str(e))
            _apply_chunk_updates(content, 0.0, [])
            continue

        _apply_chunk_updates(content, rel, preds)

        if rel >= RELEVANCE_THRESHOLD and preds:
            for p in preds:
                global_ids.add(p["id"])

    # CRITICAL: must be JSON array (Python list), NOT string, NOT objects
    document["documentDemandPredictions"] = sorted(list(global_ids))

    logger.info(
        "inference done. chunks=%d global_ids=%d time=%.2fs",
        len(chunks), len(document["documentDemandPredictions"]), time.time() - start
    )
    return document


def run(raw_data):
    """
    Entry point.
    Must keep input/output identical to the known working solution:
      Input: {"document": <json>, "num_preds": <int>}
      Output: {"predictions": <document>}
    """
    try:
        if raw_data is None:
            return {"predictions": {"documentDemandPredictions": []}}

        if isinstance(raw_data, (bytes, bytearray)):
            raw_data = raw_data.decode("utf-8", errors="ignore")

        req = json.loads(raw_data) if isinstance(raw_data, str) else raw_data
        if not isinstance(req, dict):
            return {"predictions": {"documentDemandPredictions": []}}

        if "document" not in req or "num_preds" not in req:
            return {"predictions": {"documentDemandPredictions": []}}

        document = req["document"]
        num_preds = int(req.get("num_preds", 3))

        if isinstance(document, str):
            document = json.loads(document)

        if not isinstance(document, dict):
            return {"predictions": {"documentDemandPredictions": []}}

        out_doc = inference(document, num_preds)
        return {"predictions": out_doc}

    except Exception as e:
        logger.error("run() error: %s", str(e), exc_info=True)
        return {"predictions": {"documentDemandPredictions": []}}

