# score.py
import json
import logging
import os
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Globals (Azure ML calls init() once)
DEMANDS: List[Dict[str, str]] = []
CLIENT = None

# ----------------------------
# Demands loading (local рядом score.py)
# ----------------------------
def _normalize_demands(rows: List[Dict[str, Any]]) -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []
    for r in rows:
        # Support your Excel naming: demand_id, demand, demand_description
        did = r.get("id") or r.get("demand_id") or r.get("demandId")
        demand = r.get("demand") or r.get("name") or r.get("label")
        desc = r.get("description") or r.get("demand_description") or r.get("clarification") or ""

        if did is None:
            continue

        out.append(
            {
                "id": str(did).strip(),
                "demand": ("" if demand is None else str(demand).strip()),
                "description": ("" if desc is None else str(desc).strip()),
            }
        )
    return out


def _load_demands_from_xlsx(path: Path) -> List[Dict[str, str]]:
    try:
        import pandas as pd
    except Exception as e:
        raise RuntimeError("pandas is required to load demands.xlsx") from e

    df = pd.read_excel(path)
    df = df.fillna("")
    rows = df.to_dict(orient="records")
    return _normalize_demands(rows)


def _load_demands_from_csv(path: Path) -> List[Dict[str, str]]:
    try:
        import pandas as pd
    except Exception as e:
        raise RuntimeError("pandas is required to load demands.csv") from e

    df = pd.read_csv(path)
    df = df.fillna("")
    rows = df.to_dict(orient="records")
    return _normalize_demands(rows)


def _load_demands_from_json(path: Path) -> List[Dict[str, str]]:
    data = json.loads(path.read_text(encoding="utf-8"))
    if isinstance(data, dict) and "demands" in data and isinstance(data["demands"], list):
        rows = data["demands"]
    elif isinstance(data, list):
        rows = data
    else:
        rows = []
    return _normalize_demands(rows)


def _load_demands_table() -> List[Dict[str, str]]:
    # You said: demands are in the same folder as score.py (and .env), not in model_dir.
    here = Path(__file__).resolve().parent

    # Optional override
    dem_path = os.getenv("DEMANDS_PATH")
    candidates: List[Path] = []
    if dem_path:
        candidates.append(Path(dem_path))

    # Standard local candidates (same folder)
    candidates.extend(
        [
            here / "demands.xlsx",
            here / "demands.csv",
            here / "demands.json",
        ]
    )

    for p in candidates:
        if p.exists() and p.is_file():
            if p.suffix.lower() == ".xlsx":
                dem = _load_demands_from_xlsx(p)
            elif p.suffix.lower() == ".csv":
                dem = _load_demands_from_csv(p)
            elif p.suffix.lower() == ".json":
                dem = _load_demands_from_json(p)
            else:
                continue

            if dem:
                logger.info(f"Loaded demands from: {p} (count={len(dem)})")
                return dem

    raise RuntimeError(
        f"Could not find demands file. Put demands.xlsx/csv/json next to score.py "
        f"or set DEMANDS_PATH. Looked in: {here}"
    )


def _build_demands_context(demands: List[Dict[str, str]]) -> str:
    # Short but strong: id + demand name + description (you said description is key)
    lines = []
    for d in demands:
        did = d["id"]
        name = d.get("demand", "")
        desc = d.get("description", "")
        lines.append(f"- id: {did}\n  name: {name}\n  description: {desc}")
    return "\n".join(lines)


# ----------------------------
# Azure OpenAI client
# ----------------------------
def _init_openai_client():
    # Supports latest "openai" SDK style for Azure:
    # pip install openai
    try:
        from openai import AzureOpenAI
    except Exception as e:
        raise RuntimeError("Python package 'openai' is required in the environment.") from e

    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    api_key = os.getenv("AZURE_OPENAI_API_KEY")
    api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-06-01")

    if not endpoint or not api_key:
        raise RuntimeError("Missing AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY")

    return AzureOpenAI(azure_endpoint=endpoint, api_key=api_key, api_version=api_version)


def init():
    """
    Called once by Azure ML.
    """
    global DEMANDS, CLIENT
    logger.info("Initializing OpenAI-based scoring...")
    DEMANDS = _load_demands_table()
    if not DEMANDS:
        raise RuntimeError("Demands table is empty after loading.")
    CLIENT = _init_openai_client()

    if not os.getenv("AZURE_OPENAI_DEPLOYMENT"):
        raise RuntimeError("Missing AZURE_OPENAI_DEPLOYMENT (name of Azure OpenAI deployment).")

    logger.info("Initialization finished successfully.")


# ----------------------------
# Contract helpers: request/response must remain 1:1
# ----------------------------
def _parse_request(raw_data: Any) -> Tuple[Dict[str, Any], int]:
    """
    Expected request (from your C#):
      { "document": <JSON object>, "num_preds": <int> }
    """
    if raw_data is None:
        raise ValueError("Bad Request: body is empty")

    if isinstance(raw_data, (bytes, bytearray)):
        raw_data = raw_data.decode("utf-8")

    if isinstance(raw_data, str):
        raw_data = raw_data.strip()
        if not raw_data:
            raise ValueError("Bad Request: body is empty")
        req = json.loads(raw_data)
    elif isinstance(raw_data, dict):
        req = raw_data
    else:
        # AML sometimes passes already-parsed payload in unexpected type; fail fast
        raise ValueError(f"Bad Request: unsupported body type: {type(raw_data)}")

    if "document" not in req or "num_preds" not in req:
        raise ValueError("Bad Request: expected fields 'document' and 'num_preds'.")

    document = req["document"]
    if not isinstance(document, dict):
        raise ValueError("Bad Request: 'document' must be a JSON object.")

    try:
        num_preds = int(req["num_preds"])
    except Exception:
        num_preds = 3
    num_preds = max(1, min(10, num_preds))
    return document, num_preds


def _iter_chunks(document: Dict[str, Any]) -> List[Tuple[str, Dict[str, Any]]]:
    """
    Old code iterated: document['contentDomain']['byId'].values()
    Keep same structure.
    """
    cd = document.get("contentDomain") or {}
    by_id = cd.get("byId") or {}
    if not isinstance(by_id, dict):
        return []
    chunks: List[Tuple[str, Dict[str, Any]]] = []
    for cid, obj in by_id.items():
        if isinstance(obj, dict):
            chunks.append((str(cid), obj))
    return chunks


# ----------------------------
# LLM prompt: aligned with your screenshots (rules + demands + chunks)
# ----------------------------
def _prompt_for_chunk(demands_context: str, chunk_text: str, num_preds: int) -> str:
    # This prompt returns per-chunk demandIds with probabilities.
    # We will still convert global output to STRING IDs only (contract).
    return f"""
You are an assistant for bid engineers. You classify whether a given text chunk contains requirements that match any of the predefined demands.

**Rules & Constraints:**
- The demand **Name** tells WHAT equipment this applies to
- The **Description** tells WHEN to match (specific requirement criteria)
- Match ONLY when the chunk content aligns with the demand's description
- You MUST use only the predefined demands from the list provided
- Find maximum {num_preds} demands for the chunk
- If no relevant demands found, return empty demandIds array
- Probability scale (0.0 to 1.0):
  - 1.0: direct explicit match
  - 0.7-0.9: strong semantic relationship
  - 0.5-0.7: moderate relationship
  - 0.3-0.5: weak relationship
  - below 0.3: not relevant (exclude)

**Demands:**
{demands_context}

**Text Chunk to analyze:**
{chunk_text}

Respond with valid JSON in this format:
{{
  "demandIds": [{{"id":"demand_id","probability":0.85}}],
  "explanation": "brief explanation"
}}
""".strip()


def _call_openai_for_chunk(chunk_text: str, demands_context: str, num_preds: int) -> Dict[str, Any]:
    deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")
    assert CLIENT is not None

    prompt = _prompt_for_chunk(demands_context, chunk_text, num_preds)

    # Use chat.completions to be broadly compatible
    resp = CLIENT.chat.completions.create(
        model=deployment,
        messages=[
            {"role": "system", "content": "Return ONLY valid JSON. No markdown."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.0,
    )

    text = (resp.choices[0].message.content or "").strip()
    # Parse safely: ensure JSON object
    try:
        obj = json.loads(text)
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    # Hard fallback (never break contract)
    return {"demandIds": [], "explanation": "model returned non-JSON or invalid JSON"}


# ----------------------------
# Core inference: KEEP OUTPUT/INPUT NAMES 1:1
# ----------------------------
def inference(document: Dict[str, Any], num_preds: int) -> Dict[str, Any]:
    """
    MUST keep compatibility with old app:
    - document["documentDemandPredictions"] MUST BE JSON ARRAY OF STRINGS (DEMAND IDS)
    - per chunk update: relevantProba, cdLogregPredictions, cdTransformerPredictions
    """
    start = time.time()

    chunks = _iter_chunks(document)
    if not chunks:
        document["documentDemandPredictions"] = []
        return document

    demands_context = _build_demands_context(DEMANDS)

    global_ids = set()

    for cid, content in chunks:
        chunk_text = str(content.get("text") or "")
        if len(chunk_text.split()) <= 2:
            content.update(
                {
                    "relevantProba": 0.0,
                    "cdLogregPredictions": [],
                    "cdTransformerPredictions": [],
                }
            )
            continue

        llm = _call_openai_for_chunk(chunk_text, demands_context, num_preds)
        demand_ids = llm.get("demandIds", [])
        if not isinstance(demand_ids, list):
            demand_ids = []

        # Convert to old-style per-chunk predictions:
        # cdTransformerPredictions: [{label, proba}]
        cd_transformer_predictions: List[Dict[str, Any]] = []
        for x in demand_ids:
            if not isinstance(x, dict):
                continue
            did = x.get("id")
            proba = x.get("probability")
            if did is None:
                continue
            try:
                p = float(proba)
            except Exception:
                p = 0.0
            # Filter low relevance (below 0.3 per your rules)
            if p < 0.3:
                continue
            cd_transformer_predictions.append({"label": str(did), "proba": p})

        cd_transformer_predictions = cd_transformer_predictions[:num_preds]

        top_p = cd_transformer_predictions[0]["proba"] if cd_transformer_predictions else 0.0
        relevant_proba = float(top_p)

        # Keep cdLogregPredictions shape too (mirror)
        cd_logreg_predictions = list(cd_transformer_predictions)

        # Add to global list only if strong enough (conservative)
        if cd_transformer_predictions and relevant_proba >= 0.7:
            global_ids.add(cd_transformer_predictions[0]["label"])

        content.update(
            {
                "relevantProba": relevant_proba,
                "cdLogregPredictions": cd_logreg_predictions,
                "cdTransformerPredictions": cd_transformer_predictions,
            }
        )

    # CRITICAL: this must be ARRAY OF STRINGS (not objects!)
    document["documentDemandPredictions"] = sorted(list(global_ids))

    logger.info(f"Time to format results: {time.time() - start}")
    return document


def run(raw_data):
    """
    Azure ML entrypoint.
    Your C# reads either:
      json["predictions"]["documentDemandPredictions"]
    or
      json["documentDemandPredictions"]

    So we return wrapper: {"predictions": <document>}
    """
    try:
        document, num_pred = _parse_request(raw_data)
        result_doc = inference(document, num_pred)

        # Must be dict to preserve structure
        return {"predictions": result_doc}

    except Exception as e:
        logger.error(f"Scoring failed: {str(e)}", exc_info=True)
        # Never break downstream: return empty correct shape
        return {"error": str(e), "predictions": {"documentDemandPredictions": []}}
