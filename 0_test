import os
import json
import logging
import time
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from openai import OpenAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Globals (Azure ML style)
client: Optional[OpenAI] = None
DEMANDS: List[Dict[str, str]] = []
DEMAND_EMB: Optional[np.ndarray] = None

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_EMBED_MODEL = os.getenv("OPENAI_EMBED_MODEL", "text-embedding-3-small")

TOP_K_RAG = int(os.getenv("TOP_K_RAG", "8"))          # ile demandów podać do LLM jako kandydaci
MAX_LABELS_PER_CHUNK = int(os.getenv("MAX_LABELS_PER_CHUNK", "3"))
MIN_KEEP_PROBA = float(os.getenv("MIN_KEEP_PROBA", "0.30"))  # poniżej odrzucamy
MAX_TEXT_CHARS = int(os.getenv("MAX_TEXT_CHARS", "4000"))


def _l2_normalize(x: np.ndarray) -> np.ndarray:
    denom = (np.linalg.norm(x, axis=1, keepdims=True) + 1e-12)
    return x / denom


def _safe_float(v: Any, default: float = 0.0) -> float:
    try:
        return float(v)
    except Exception:
        return default


def _safe_json_loads(s: str) -> Optional[dict]:
    try:
        return json.loads(s)
    except Exception:
        return None


def init():
    """
    Called once per container start by Azure ML.
    Loads demands from Excel and embeds them for retrieval.
    """
    global client, DEMANDS, DEMAND_EMB

    logger.info("Initializing OpenAI client and loading demands...")

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("OPENAI_API_KEY is missing. init() will exit early.")
        return

    client = OpenAI(api_key=api_key)

    base_dir = os.path.dirname(os.path.abspath(__file__))
    demands_path = os.path.join(base_dir, "demands.xlsx")

    if not os.path.exists(demands_path):
        logger.error(f"demands.xlsx not found at: {demands_path}")
        return

    df = pd.read_excel(demands_path)

    # Your real columns: demand_id, demand, description
    required_cols = {"demand_id", "demand", "description"}
    if not required_cols.issubset(df.columns):
        logger.error(f"demands.xlsx missing required columns. Required: {required_cols}, got: {set(df.columns)}")
        return

    DEMANDS = []
    embed_inputs: List[str] = []

    for _, row in df.iterrows():
        did = str(row["demand_id"]).strip()
        name = str(row["demand"]).strip()
        desc = str(row["description"]).strip()

        if not did or did.lower() == "nan":
            continue
        if not name or name.lower() == "nan":
            continue

        DEMANDS.append({"id": did, "name": name, "description": desc})

        # Embed both name and description so retrieval works well
        embed_inputs.append(f"Name: {name}\nClarification: {desc}")

    if not DEMANDS:
        logger.error("No demands loaded from Excel (DEMANDS is empty).")
        return

    # Pre-compute embeddings
    t0 = time.time()
    emb = client.embeddings.create(model=OPENAI_EMBED_MODEL, input=embed_inputs)
    DEMAND_EMB = _l2_normalize(np.array([e.embedding for e in emb.data], dtype=np.float32))
    logger.info(f"Loaded {len(DEMANDS)} demands. Embedded in {time.time() - t0:.2f}s.")


def _retrieve_candidates(chunk_text: str, k: int) -> List[Dict[str, str]]:
    """
    RAG retrieval: pick top-k relevant demands using embeddings.
    """
    if client is None or DEMAND_EMB is None or not DEMANDS:
        return []

    emb = client.embeddings.create(model=OPENAI_EMBED_MODEL, input=[chunk_text])
    q = _l2_normalize(np.array([emb.data[0].embedding], dtype=np.float32))
    sims = DEMAND_EMB @ q[0]
    idx = np.argsort(-sims)[:k]
    return [DEMANDS[i] for i in idx]


def _llm_classify_chunk(chunk_id: str, chunk_text: str, candidates: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    Use LLM to decide which demands match and assign probability.
    Returns JSON with {chunkId, demandIds:[{id,probability}], explanation}
    """
    if client is None:
        return {"chunkId": chunk_id, "demandIds": [], "explanation": "OpenAI client not initialized."}

    # Provide candidates: id + clarification (description)
    demands_context = "\n".join(
        [f"- id: {d['id']}\n  name: {d['name']}\n  clarification: {d['description']}" for d in candidates]
    )

    system = (
        "You label customer requirements in product documentation.\n"
        "You MUST only choose from the provided demands.\n"
        "Match ONLY when the chunk clearly satisfies the demand's clarification.\n"
        "Return at most 3 demands.\n"
        "If nothing matches, return an empty demandIds array.\n"
        "Probabilities must be in [0.0, 1.0].\n"
        "Return JSON only."
    )

    user = f"""
Demands (id, name, clarification):
{demands_context}

ChunkId: {chunk_id}
Chunk text:
{chunk_text}

Return JSON in this format exactly:
{{
  "chunkId": "{chunk_id}",
  "demandIds": [{{"id":"<one of provided ids>","probability":0.85}}],
  "explanation": "brief reason"
}}
"""

    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            temperature=0,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
        )
        content = resp.choices[0].message.content or ""
        parsed = _safe_json_loads(content)
        if not parsed:
            return {"chunkId": chunk_id, "demandIds": [], "explanation": "LLM returned non-JSON."}
        return parsed
    except Exception as e:
        logger.exception("LLM classify failed")
        return {"chunkId": chunk_id, "demandIds": [], "explanation": f"LLM error: {str(e)}"}


def run(raw_data: Any) -> Dict[str, Any]:
    """
    Azure ML Online Endpoint entrypoint.
    Expects JSON like old solution:
      { "document": {...}, "num_preds": 3 }
    Returns:
      { "predictions": <document_with_updates> }
    """
    try:
        # Parse input
        request = json.loads(raw_data) if isinstance(raw_data, str) else raw_data
        if not isinstance(request, dict):
            raise ValueError("Request must be a JSON object/dict.")

        if "document" not in request or "num_preds" not in request:
            raise ValueError("Invalid input, expected 'document' and 'num_preds' in request body.")

        document = request["document"]
        num_pred = int(request["num_preds"])

        by_id = document.get("contentDomain", {}).get("byId", {})
        if not isinstance(by_id, dict):
            raise ValueError("document.contentDomain.byId must be an object/dict.")

        document_demand_predictions = set()

        # Process each chunk
        for chunk_id, content in by_id.items():
            text = str(content.get("text", "") or "")
            text = text[:MAX_TEXT_CHARS]

            # If init failed, fail-soft: keep contract, empty predictions
            if client is None or DEMAND_EMB is None or not DEMANDS:
                preds = []
                content.update({
                    "relevantProba": 0.0,
                    "cdLogregPredictions": [],
                    "cdTransformerPredictions": preds,
                })
                continue

            # RAG retrieval (candidates)
            candidates = _retrieve_candidates(text, TOP_K_RAG)

            # LLM scoring against candidates
            llm_out = _llm_classify_chunk(chunk_id, text, candidates)

            preds = []
            for item in llm_out.get("demandIds", [])[:MAX_LABELS_PER_CHUNK]:
                did = str(item.get("id", "")).strip()
                proba = _safe_float(item.get("probability", 0.0))
                if did and proba >= MIN_KEEP_PROBA:
                    preds.append({"label": did, "proba": proba})
                    document_demand_predictions.add(did)

            # Sort + truncate to requested top N
            preds = sorted(preds, key=lambda x: x["proba"], reverse=True)[:num_pred]

            relevant_proba = max([p["proba"] for p in preds], default=0.0)

            # Update chunk content exactly like old pipeline expected
            content.update({
                "relevantProba": relevant_proba,
                "cdLogregPredictions": [],          # logreg removed in PoC, keep empty for contract
                "cdTransformerPredictions": preds,  # keep same key naming as old code
            })

        # Document-level summary (used by app)
        document["documentDemandPredictions"] = list(document_demand_predictions)

        return {"predictions": document}

    except ValueError as ve:
        logger.warning(f"Bad request: {str(ve)}")
        raise ve
    except Exception as e:
        logger.exception("Internal server error")
        raise Exception("Internal server error")

