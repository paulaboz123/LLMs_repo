import json
import logging
import os
import re
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd

try:
    from openai import AzureOpenAI
except Exception:
    AzureOpenAI = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("score")

# =========================
# Tunables / ENV
# =========================
USE_MOCK = os.getenv("USE_MOCK", "0").strip() == "1"

MIN_CHUNK_LEN = int(os.getenv("MIN_CHUNK_LEN", "10"))
MIN_WORDS_IN_CHUNK = int(os.getenv("MIN_WORDS_IN_CHUNK", "5"))
MIN_WORDS_IN_SENTENCE = int(os.getenv("MIN_WORDS_IN_SENTENCE", "5"))

MAX_LINES_PER_CHUNK = int(os.getenv("MAX_LINES_PER_CHUNK", "18"))
MAX_LINES_TO_LLM = int(os.getenv("MAX_LINES_TO_LLM", "6"))

MAX_LABELS_PER_CHUNK = int(os.getenv("MAX_LABELS_PER_CHUNK", "3"))
CANDIDATES_TOPK = int(os.getenv("CANDIDATES_TOPK", "8"))

LABEL_MIN_PROB = float(os.getenv("LABEL_MIN_PROB", "0.75"))
HIGHLIGHT_MIN_PROB = float(os.getenv("HIGHLIGHT_MIN_PROB", "0.80"))

OVERLAP_MIN = int(os.getenv("OVERLAP_MIN", "1"))
CRITERIA_HITS_MIN = int(os.getenv("CRITERIA_HITS_MIN", "1"))

TOC_DOT_MIN = int(os.getenv("TOC_DOT_MIN", "10"))

# Embeddings
EMB_SIM_MIN = float(os.getenv("EMB_SIM_MIN", "0.35"))
EMB_BATCH = int(os.getenv("EMB_BATCH", "64"))

# =========================
# Globals
# =========================
client: Optional["AzureOpenAI"] = None
CHAT_DEPLOYMENT: Optional[str] = None
EMB_DEPLOYMENT: Optional[str] = None

DEMANDS: List[Dict[str, str]] = []
DEMAND_TEXT: Dict[str, str] = {}
DEMAND_TOKENSETS: Dict[str, set] = {}
DEMAND_EMB: Dict[str, List[float]] = {}

# =========================
# Regex / heuristics (ENGLISH ONLY)
# =========================
REQ_MODAL_RE = re.compile(
    r"\b(shall|must|required|requirement|should|need to|has to|may not|must not|shall not|prohibit|forbidden)\b",
    re.IGNORECASE,
)
REQ_NUM_RE = re.compile(r"\b\d+([.,]\d+)?\b")
UNIT_RE = re.compile(r"\b(mm|cm|m|kg|w|kw|mw|v|kv|a|ma|hz|rpm|bar|pa|°c|c|ip\d{2})\b", re.IGNORECASE)

STOPWORDS = {
    "shall","must","should","required","requirement","provide","provided","system","equipment",
    "the","and","for","with","this","that","are","is","to","of","in","on","by","as","be","will",
}

# =========================
# TOC / header-footer detection
# =========================
def _word_count(s: str) -> int:
    return len([w for w in re.split(r"\s+", (s or "").strip()) if w])

def _is_probable_toc_line(line: str) -> bool:
    t = (line or "").strip()
    if not t:
        return False
    dots_re = re.compile(r"\.{%d,}\s*\d+\s*$" % TOC_DOT_MIN)
    return bool(dots_re.search(t))

def _is_page_number_only(line: str) -> bool:
    return bool(re.fullmatch(r"\s*\d{1,4}\s*", (line or "").strip()))

def _is_probable_header_footer(line: str) -> bool:
    t = (line or "").strip()
    if not t:
        return False
    if _is_page_number_only(t):
        return True
    if re.match(r"^[A-Z][A-Za-z0-9 \-–—]{0,60}\s+\d{1,4}$", t) and _word_count(t) <= 10:
        return True
    return False

# =========================
# IO helpers
# =========================
def _json_load_maybe(x: Any) -> Any:
    if isinstance(x, (bytes, bytearray)):
        x = x.decode("utf-8", errors="replace")
    if isinstance(x, str):
        s = x.strip()
        if not s:
            return None
        try:
            return json.loads(s)
        except Exception:
            return x
    return x

def _find_content_byid(document: Dict[str, Any]) -> Dict[str, Any]:
    cd = document.get("contentDomain", {})
    by_id = cd.get("byId", {})
    return by_id if isinstance(by_id, dict) else {}

# =========================
# Demands loading
# =========================
def _load_demands_xlsx() -> List[Dict[str, str]]:
    here = os.path.dirname(os.path.abspath(__file__))
    candidates = [
        os.path.join(here, "demands.xlsx"),
        os.path.join(here, "assets", "demands.xlsx"),
    ]
    model_dir = os.getenv("AZUREML_MODEL_DIR")
    if model_dir:
        candidates += [
            os.path.join(model_dir, "demands.xlsx"),
            os.path.join(model_dir, "assets", "demands.xlsx"),
        ]

    for p in candidates:
        if os.path.exists(p):
            df = pd.read_excel(p)
            df.columns = [c.strip().lower() for c in df.columns]
            if "demand_id" not in df.columns and "id" in df.columns:
                df = df.rename(columns={"id": "demand_id"})
            if "description" not in df.columns and "demand_description" in df.columns:
                df = df.rename(columns={"demand_description": "description"})

            required = {"demand_id", "demand", "description"}
            missing = required - set(df.columns)
            if missing:
                raise RuntimeError(f"demands.xlsx missing required columns: {sorted(list(missing))}")

            out: List[Dict[str, str]] = []
            for _, r in df.fillna("").iterrows():
                did = str(r["demand_id"]).strip()
                if not did:
                    continue
                out.append(
                    {"id": did, "demand": str(r["demand"]).strip(), "description": str(r["description"]).strip()}
                )
            if not out:
                raise RuntimeError("demands.xlsx loaded but produced 0 rows")
            return out

    raise RuntimeError("demands.xlsx not found")

def _tokenize(s: str) -> set:
    s = (s or "").lower()
    s = re.sub(r"[^a-z0-9]+", " ", s)
    return {t for t in s.split() if len(t) >= 3 and t not in STOPWORDS}

def _build_demand_indexes() -> None:
    DEMAND_TOKENSETS.clear()
    DEMAND_TEXT.clear()
    for d in DEMANDS:
        did = d["id"]
        DEMAND_TOKENSETS[did] = _tokenize((d.get("demand") or "") + " " + (d.get("description") or ""))
        desc = d.get("description") or ""
        DEMAND_TEXT[did] = f'{(d.get("demand") or "")} | {desc[:240]}'

# =========================
# OpenAI init / embeddings
# =========================
def _init_openai() -> None:
    global client, CHAT_DEPLOYMENT, EMB_DEPLOYMENT

    if AzureOpenAI is None:
        raise RuntimeError("openai package not available in environment")

    endpoint = (os.getenv("AZURE_OPENAI_ENDPOINT") or "").strip()
    key = (os.getenv("AZURE_OPENAI_API_KEY") or "").strip()
    CHAT_DEPLOYMENT = (os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT") or "").strip()
    EMB_DEPLOYMENT = (os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT") or "").strip()

    if not endpoint or not key or not CHAT_DEPLOYMENT:
        raise RuntimeError("Missing AZURE_OPENAI_ENDPOINT / AZURE_OPENAI_API_KEY / AZURE_OPENAI_CHAT_DEPLOYMENT")

    client = AzureOpenAI(azure_endpoint=endpoint, api_key=key, api_version="2024-06-01")

def _embed(texts: List[str]) -> List[List[float]]:
    if not client or not EMB_DEPLOYMENT:
        return []
    out: List[List[float]] = []
    for i in range(0, len(texts), EMB_BATCH):
        resp = client.embeddings.create(model=EMB_DEPLOYMENT, input=texts[i : i + EMB_BATCH])
        out.extend([d.embedding for d in resp.data])
    return out

def _cos(a: List[float], b: List[float]) -> float:
    na = sum(x * x for x in a) ** 0.5
    nb = sum(x * x for x in b) ** 0.5
    if na == 0.0 or nb == 0.0:
        return 0.0
    return float(sum(x * y for x, y in zip(a, b)) / (na * nb))

def _build_demand_embeddings() -> None:
    DEMAND_EMB.clear()
    if not EMB_DEPLOYMENT:
        return
    texts = [f'{d.get("demand","")}\n{d.get("description","")}'.strip() for d in DEMANDS]
    vecs = _embed(texts)
    if len(vecs) != len(DEMANDS):
        DEMAND_EMB.clear()
        return
    for d, v in zip(DEMANDS, vecs):
        DEMAND_EMB[d["id"]] = v

# =========================
# Chunk -> sentence/line candidates (restored)
# =========================
def _split_lines_or_sentences(chunk_text: str) -> List[str]:
    t = (chunk_text or "").strip()
    if not t:
        return []

    raw_lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    tableish = (len(raw_lines) >= 6) or any(("|" in ln or "\t" in ln) for ln in raw_lines)

    candidates: List[str] = []
    if tableish:
        for ln in raw_lines:
            if len(ln) < MIN_CHUNK_LEN:
                continue
            if _is_probable_toc_line(ln) or _is_probable_header_footer(ln):
                continue
            if _word_count(ln) < MIN_WORDS_IN_SENTENCE:
                continue
            candidates.append(ln)
    else:
        parts = re.split(r"(?<=[.!?])\s+|\n+", t)
        for p in parts:
            p = (p or "").strip()
            if not p:
                continue
            if len(p) < MIN_CHUNK_LEN:
                continue
            if _is_probable_toc_line(p) or _is_probable_header_footer(p):
                continue
            if _word_count(p) < MIN_WORDS_IN_SENTENCE:
                continue
            candidates.append(p)

    return candidates[:MAX_LINES_PER_CHUNK]

def _line_priority_score(line: str) -> float:
    score = 0.0
    if REQ_MODAL_RE.search(line):
        score += 0.80
    if REQ_NUM_RE.search(line) or UNIT_RE.search(line):
        score += 0.20
    return score

# =========================
# Candidate retrieval
# =========================
def _candidate_retrieval(sentence: str) -> List[str]:
    s = (sentence or "").strip()
    if not s:
        return []

    if EMB_DEPLOYMENT and DEMAND_EMB and client:
        vec = _embed([s])
        if vec:
            sv = vec[0]
            scored: List[Tuple[str, float]] = []
            for did, dv in DEMAND_EMB.items():
                sim = _cos(sv, dv)
                if sim >= EMB_SIM_MIN:
                    scored.append((did, sim))
            scored.sort(key=lambda x: x[1], reverse=True)
            return [d for d, _ in scored[:CANDIDATES_TOPK]]

    toks = _tokenize(s)
    if not toks:
        return []
    scored2 = [(did, len(toks & dt)) for did, dt in DEMAND_TOKENSETS.items() if len(toks & dt) > 0]
    scored2.sort(key=lambda x: x[1], reverse=True)
    return [d for d, _ in scored2[:CANDIDATES_TOPK]]

# =========================
# LLM + parsing
# =========================
def _safe_json_from_model(text: str) -> Dict[str, Any]:
    t = (text or "").strip()
    if not t:
        return {}
    try:
        obj = json.loads(t)
        return obj if isinstance(obj, dict) else {}
    except Exception:
        pass
    m = re.search(r"\{.*\}", t, re.DOTALL)
    if not m:
        return {}
    try:
        obj = json.loads(m.group(0))
        return obj if isinstance(obj, dict) else {}
    except Exception:
        return {}

def _llm_predict(sentence: str, candidate_ids: List[str], top_k: int) -> Dict[str, Any]:
    if not client or not CHAT_DEPLOYMENT:
        return {}

    block = "\n".join([f"- {did}: {DEMAND_TEXT.get(did,'')}" for did in candidate_ids])

    system = (
        "You are a strict requirements classifier for bid/spec documents. "
        "Return STRICT JSON only. No markdown. No prose. "
        "Never invent demand IDs; use only candidate IDs. "
        "Only label when DEMAND DESCRIPTION conditions are satisfied."
    )

    user = f"""
You have ONE sentence/line.

If it is NOT a concrete requirement/specification to highlight:
- set isRequirement=false and labels=[]

If it IS a requirement:
- set isRequirement=true
- choose up to {min(int(top_k), MAX_LABELS_PER_CHUNK)} IDs from CANDIDATES only
For EACH label:
- probability >= {LABEL_MIN_PROB}
- evidence: exact quote copied from the sentence (5–25 words), MUST be a substring
- criteriaMatched: 1–3 short phrases reflecting DESCRIPTION conditions
- If you cannot find criteria from the description that are met here, DO NOT label.

CANDIDATES (id: demand | description):
{block}

SENTENCE:
{sentence}

Return JSON ONLY:
{{
  "isRequirement": true,
  "labels": [
    {{
      "id": "<id>",
      "probability": 0.0,
      "evidence": "<exact substring>",
      "criteriaMatched": ["<phrase>"]
    }}
  ]
}}
""".strip()

    resp = client.chat.completions.create(
        model=CHAT_DEPLOYMENT,
        messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
        temperature=0.0,
        max_tokens=900,
    )
    raw = (resp.choices[0].message.content or "").strip()
    return _safe_json_from_model(raw)

def _criteria_hits(sentence: str, crit: List[str]) -> int:
    s = (sentence or "").lower()
    hits = 0
    for c in crit:
        c = (c or "").strip()
        if not c:
            continue
        for t in c.lower().split():
            if len(t) >= 4 and t in s:
                hits += 1
                break
    return hits

def _sanitize(sentence: str, candidate_ids: List[str], data: Dict[str, Any], top_k: int) -> Optional[List[Dict[str, Any]]]:
    if not isinstance(data, dict):
        return None
    if not bool(data.get("isRequirement", False)):
        return None

    raw_labels = data.get("labels", [])
    if not isinstance(raw_labels, list) or not raw_labels:
        return None

    sent_lc = (sentence or "").lower()
    sent_tokens = _tokenize(sentence)

    out: List[Dict[str, Any]] = []
    for it in raw_labels:
        if not isinstance(it, dict):
            continue

        did = str(it.get("id", "")).strip()
        if not did or did not in candidate_ids:
            continue

        try:
            prob = float(it.get("probability", 0.0) or 0.0)
        except Exception:
            continue
        if prob < LABEL_MIN_PROB:
            continue

        evidence = str(it.get("evidence", "") or "").strip()
        if not evidence or evidence.lower() not in sent_lc:
            continue

        crit = it.get("criteriaMatched", [])
        if not isinstance(crit, list):
            crit = []
        if _criteria_hits(sentence, crit) < CRITERIA_HITS_MIN:
            continue

        # keep lightweight lexical sanity to prevent absurd labels
        if len(sent_tokens & DEMAND_TOKENSETS.get(did, set())) < OVERLAP_MIN:
            continue

        out.append({"label": did, "proba": float(prob)})

    out.sort(key=lambda x: x["proba"], reverse=True)
    out = out[: min(int(top_k), MAX_LABELS_PER_CHUNK)]
    return out if out else None

# =========================
# AML entrypoints
# =========================
def init():
    global DEMANDS
    DEMANDS = _load_demands_xlsx()
    _build_demand_indexes()

    if not USE_MOCK:
        _init_openai()
        _build_demand_embeddings()

def run(raw_data):
    try:
        req = _json_load_maybe(raw_data)
        if not isinstance(req, dict):
            return {"error": "Bad request: body must be JSON object", "predictions": {"documentDemandPredictions": []}}

        document = _json_load_maybe(req.get("document"))
        if not isinstance(document, dict):
            return {"error": "Bad request: 'document' must be JSON object", "predictions": {"documentDemandPredictions": []}}

        try:
            num_preds = int(req.get("num_preds", MAX_LABELS_PER_CHUNK))
        except Exception:
            num_preds = MAX_LABELS_PER_CHUNK
        num_preds = max(1, min(num_preds, MAX_LABELS_PER_CHUNK))

        by_id = _find_content_byid(document)
        doc_best: Dict[str, float] = {}

        for content in by_id.values():
            if not isinstance(content, dict):
                continue

            chunk_text = str(content.get("text", "") or "")

            # Required output fields (do not change)
            content["relevantProba"] = 0.0
            content["cdTransformerPredictions"] = []
            content["cdLogregPredictions"] = []
            content["highlightText"] = ""

            if _word_count(chunk_text) < MIN_WORDS_IN_CHUNK:
                continue

            candidates = _split_lines_or_sentences(chunk_text)
            if not candidates:
                continue

            ranked = sorted(candidates, key=_line_priority_score, reverse=True)[:MAX_LINES_TO_LLM]

            best_preds: Optional[List[Dict[str, Any]]] = None
            best_sentence = ""
            best_top1 = 0.0

            for sent in ranked:
                cand_ids = _candidate_retrieval(sent)
                if not cand_ids:
                    continue

                if USE_MOCK:
                    best_preds = [{"label": cand_ids[0], "proba": 0.99}]
                    best_sentence = sent
                    best_top1 = 0.99
                    break

                data = _llm_predict(sent, cand_ids, num_preds)
                preds = _sanitize(sent, cand_ids, data, num_preds)
                if not preds:
                    continue

                top1 = float(preds[0]["proba"])
                if top1 > best_top1:
                    best_top1 = top1
                    best_preds = preds
                    best_sentence = sent

            if not best_preds or best_top1 < HIGHLIGHT_MIN_PROB:
                continue

            content["highlightText"] = best_sentence
            content["relevantProba"] = float(best_top1)
            content["cdTransformerPredictions"] = best_preds
            content["cdLogregPredictions"] = list(best_preds)

            for p in best_preds:
                did = p["label"]
                pr = float(p["proba"])
                if did not in doc_best or pr > doc_best[did]:
                    doc_best[did] = pr

        # Keep output contract: list of strings
        document["documentDemandPredictions"] = list(doc_best.keys())
        return {"predictions": document}

    except Exception as e:
        logger.exception("run failed")
        return {"error": str(e), "predictions": {"documentDemandPredictions": []}}
