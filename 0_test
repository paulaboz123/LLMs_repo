# score.py
import json
import logging
import os
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Globals (Azure ML calls init() once)
DEMANDS: List[Dict[str, str]] = []
CLIENT = None

# ----------------------------
# Demands loading (local рядом score.py)
# ----------------------------
def _normalize_demands(rows: List[Dict[str, Any]]) -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []
    for r in rows:
        did = r.get("id") or r.get("demand_id") or r.get("demandId")
        demand = r.get("demand") or r.get("name") or r.get("label")
        desc = r.get("description") or r.get("demand_description") or r.get("clarification") or ""

        if did is None:
            continue

        out.append(
            {
                "id": str(did).strip(),
                "demand": ("" if demand is None else str(demand).strip()),
                "description": ("" if desc is None else str(desc).strip()),
            }
        )
    return out


def _load_demands_from_xlsx(path: Path) -> List[Dict[str, str]]:
    try:
        import pandas as pd
    except Exception as e:
        raise RuntimeError("pandas is required to load demands.xlsx") from e

    df = pd.read_excel(path)
    df = df.fillna("")
    rows = df.to_dict(orient="records")
    return _normalize_demands(rows)


def _load_demands_from_csv(path: Path) -> List[Dict[str, str]]:
    try:
        import pandas as pd
    except Exception as e:
        raise RuntimeError("pandas is required to load demands.csv") from e

    df = pd.read_csv(path)
    df = df.fillna("")
    rows = df.to_dict(orient="records")
    return _normalize_demands(rows)


def _load_demands_from_json(path: Path) -> List[Dict[str, str]]:
    data = json.loads(path.read_text(encoding="utf-8"))
    if isinstance(data, dict) and "demands" in data and isinstance(data["demands"], list):
        rows = data["demands"]
    elif isinstance(data, list):
        rows = data
    else:
        rows = []
    return _normalize_demands(rows)


def _load_demands_table() -> List[Dict[str, str]]:
    here = Path(__file__).resolve().parent

    dem_path = os.getenv("DEMANDS_PATH")
    candidates: List[Path] = []
    if dem_path:
        candidates.append(Path(dem_path))

    candidates.extend(
        [
            here / "demands.xlsx",
            here / "demands.csv",
            here / "demands.json",
        ]
    )

    for p in candidates:
        if p.exists() and p.is_file():
            if p.suffix.lower() == ".xlsx":
                dem = _load_demands_from_xlsx(p)
            elif p.suffix.lower() == ".csv":
                dem = _load_demands_from_csv(p)
            elif p.suffix.lower() == ".json":
                dem = _load_demands_from_json(p)
            else:
                continue

            if dem:
                logger.info(f"Loaded demands from: {p} (count={len(dem)})")
                return dem

    raise RuntimeError(
        f"Could not find demands file. Put demands.xlsx/csv/json next to score.py "
        f"or set DEMANDS_PATH. Looked in: {here}"
    )


def _build_demands_context(demands: List[Dict[str, str]]) -> str:
    lines = []
    for d in demands:
        did = d["id"]
        name = d.get("demand", "")
        desc = d.get("description", "")
        lines.append(f"- id: {did}\n  name: {name}\n  description: {desc}")
    return "\n".join(lines)


# ----------------------------
# Azure OpenAI client
# ----------------------------
def _init_openai_client():
    try:
        from openai import AzureOpenAI
    except Exception as e:
        raise RuntimeError("Python package 'openai' is required in the environment.") from e

    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    api_key = os.getenv("AZURE_OPENAI_API_KEY")
    api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-06-01")

    if not endpoint or not api_key:
        raise RuntimeError("Missing AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY")

    return AzureOpenAI(azure_endpoint=endpoint, api_key=api_key, api_version=api_version)


def init():
    global DEMANDS, CLIENT
    logger.info("Initializing OpenAI-based scoring...")
    DEMANDS = _load_demands_table()
    if not DEMANDS:
        raise RuntimeError("Demands table is empty after loading.")
    CLIENT = _init_openai_client()

    # IMPORTANT: your env uses AZURE_OPENAI_CHAT_DEPLOYMENT
    if not (os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT") or os.getenv("AZURE_OPENAI_DEPLOYMENT")):
        raise RuntimeError("Missing AZURE_OPENAI_CHAT_DEPLOYMENT (or AZURE_OPENAI_DEPLOYMENT).")

    logger.info("Initialization finished successfully.")


# ----------------------------
# Contract helpers: request/response must remain 1:1
# ----------------------------
def _parse_request(raw_data: Any) -> Tuple[Dict[str, Any], int]:
    if raw_data is None:
        raise ValueError("Bad Request: body is empty")

    if isinstance(raw_data, (bytes, bytearray)):
        raw_data = raw_data.decode("utf-8")

    if isinstance(raw_data, str):
        raw_data = raw_data.strip()
        if not raw_data:
            raise ValueError("Bad Request: body is empty")
        req = json.loads(raw_data)
    elif isinstance(raw_data, dict):
        req = raw_data
    else:
        raise ValueError(f"Bad Request: unsupported body type: {type(raw_data)}")

    if "document" not in req or "num_preds" not in req:
        raise ValueError("Bad Request: expected fields 'document' and 'num_preds'.")

    document = req["document"]
    if not isinstance(document, dict):
        raise ValueError("Bad Request: 'document' must be a JSON object.")

    try:
        num_preds = int(req["num_preds"])
    except Exception:
        num_preds = 3
    num_preds = max(1, min(10, num_preds))
    return document, num_preds


def _iter_chunks(document: Dict[str, Any]) -> List[Tuple[str, Dict[str, Any]]]:
    cd = document.get("contentDomain") or {}
    by_id = cd.get("byId") or {}
    if not isinstance(by_id, dict):
        return []
    chunks: List[Tuple[str, Dict[str, Any]]] = []
    for cid, obj in by_id.items():
        if isinstance(obj, dict):
            chunks.append((str(cid), obj))
    return chunks


# ----------------------------
# LLM prompt (fixed: includes chunkId + relevance + strict ID rule)
# ----------------------------
def _prompt_for_chunk(demands_context: str, chunk_id: str, chunk_text: str, num_preds: int) -> str:
    return f"""
You are an assistant for bid engineers. You must detect whether the text chunk contains a concrete customer requirement/specification (highlight-worthy) and map it to predefined demands.

**Rules & Constraints:**
- Use ONLY the demand IDs provided in the Demands list. Never invent IDs.
- The demand **Name** hints WHAT this is about.
- The **Description** tells WHEN to match.
- First decide relevance (is this a requirement/spec).
- If NOT relevant: set relevantProbability < 0.30 and return empty demandIds.
- If relevant: return up to {num_preds} demandIds with probabilities.
- Probability scale for demandIds:
  - 1.0: direct explicit match
  - 0.7-0.9: strong semantic relationship
  - 0.5-0.7: moderate
  - 0.3-0.5: weak
  - below 0.3: exclude

**Demands:**
{demands_context}

**Text Chunk to analyze:**
chunkId: {chunk_id}
text: {chunk_text}

Respond with valid JSON EXACTLY:
{{
  "chunkId": "{chunk_id}",
  "relevantProbability": 0.0,
  "demandIds": [{{"id":"<id_from_demands_list>","probability":0.85}}],
  "explanation": "brief explanation"
}}
""".strip()


def _call_openai_for_chunk(chunk_id: str, chunk_text: str, demands_context: str, num_preds: int) -> Dict[str, Any]:
    deployment = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT") or os.getenv("AZURE_OPENAI_DEPLOYMENT")
    if not deployment:
        return {"chunkId": chunk_id, "relevantProbability": 0.0, "demandIds": [], "explanation": "missing deployment env"}

    assert CLIENT is not None

    prompt = _prompt_for_chunk(demands_context, chunk_id, chunk_text, num_preds)

    resp = CLIENT.chat.completions.create(
        model=deployment,
        messages=[
            {"role": "system", "content": "Return ONLY valid JSON. No markdown."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.0,
    )

    text = (resp.choices[0].message.content or "").strip()

    # Parse safely
    try:
        obj = json.loads(text)
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    return {"chunkId": chunk_id, "relevantProbability": 0.0, "demandIds": [], "explanation": "model returned non-JSON or invalid JSON"}


# ----------------------------
# Core inference: KEEP OUTPUT/INPUT NAMES 1:1
# ----------------------------
def inference(document: Dict[str, Any], num_preds: int) -> Dict[str, Any]:
    start = time.time()

    chunks = _iter_chunks(document)
    if not chunks:
        document["documentDemandPredictions"] = []
        return document

    demands_context = _build_demands_context(DEMANDS)

    global_ids = set()

    for cid, content in chunks:
        chunk_text = str(content.get("text") or "")
        if len(chunk_text.split()) <= 2:
            content.update(
                {
                    "relevantProba": 0.0,
                    "cdLogregPredictions": [],
                    "cdTransformerPredictions": [],
                }
            )
            continue

        llm = _call_openai_for_chunk(cid, chunk_text, demands_context, num_preds)

        # relevance from model (fallback to 0)
        try:
            rel = float(llm.get("relevantProbability", 0.0) or 0.0)
        except Exception:
            rel = 0.0

        demand_ids = llm.get("demandIds", [])
        if not isinstance(demand_ids, list):
            demand_ids = []

        cd_transformer_predictions: List[Dict[str, Any]] = []
        for x in demand_ids:
            if not isinstance(x, dict):
                continue
            did = x.get("id")
            proba = x.get("probability")
            if did is None:
                continue
            try:
                p = float(proba)
            except Exception:
                p = 0.0
            if p < 0.3:
                continue
            cd_transformer_predictions.append({"label": str(did), "proba": p})

        cd_transformer_predictions = cd_transformer_predictions[:num_preds]

        top_label_proba = cd_transformer_predictions[0]["proba"] if cd_transformer_predictions else 0.0

        # IMPORTANT:
        # relevantProba drives highlighting in old flow. Use max(rel, top_label_proba)
        # so highlight can trigger if model provided label confidence but low rel field.
        relevant_proba = float(max(rel, top_label_proba))

        # Mirror old logreg/transformer shapes
        cd_logreg_predictions = list(cd_transformer_predictions)

        # Global IDs:
        # Do not block everything with 0.7 initially. Use 0.3 (same as your filter).
        for pred in cd_transformer_predictions:
            if float(pred.get("proba", 0.0)) >= 0.3:
                global_ids.add(pred["label"])

        content.update(
            {
                "relevantProba": relevant_proba,
                "cdLogregPredictions": cd_logreg_predictions,
                "cdTransformerPredictions": cd_transformer_predictions,
            }
        )

    # CRITICAL: ARRAY OF STRINGS
    document["documentDemandPredictions"] = sorted(list(global_ids))

    logger.info(f"Time to format results: {time.time() - start}")
    return document


def run(raw_data):
    try:
        document, num_pred = _parse_request(raw_data)
        result_doc = inference(document, num_pred)
        return {"predictions": result_doc}

    except Exception as e:
        logger.error(f"Scoring failed: {str(e)}", exc_info=True)
        return {"error": str(e), "predictions": {"documentDemandPredictions": []}}
