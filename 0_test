# score.py
import json
import logging
import os
import re
from typing import Any, Dict, List, Tuple

import pandas as pd

try:
    from openai import AzureOpenAI
except Exception:
    AzureOpenAI = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("score")

# -------------------------
# Safety-first knobs
# -------------------------
USE_MOCK = os.getenv("USE_MOCK", "1").strip() == "1"

MIN_CHUNK_LEN = int(os.getenv("MIN_CHUNK_LEN", "4"))
# Lower to get *any* highlights first; tune up later
MIN_PROBA = float(os.getenv("MIN_PROBA", "0.15"))
# If AOAI returns relevance below this, we still allow predictions if model provided them
AOAI_REL_THRESHOLD = float(os.getenv("AOAI_REL_THRESHOLD", "0.30"))
MAX_DEBUG_CHUNKS = int(os.getenv("MAX_DEBUG_CHUNKS", "0"))  # set 2-3 temporarily if needed

client = None
CHAT_DEPLOYMENT = None

DEMANDS: List[Dict[str, str]] = []         # [{id, demand, description}]
DEMAND_TOKENS: List[Tuple[str, set]] = []  # [(demand_id, tokenset)]


# -------------------------
# Helpers
# -------------------------
def _json_load_maybe(x: Any) -> Any:
    if isinstance(x, (bytes, bytearray)):
        x = x.decode("utf-8", errors="replace")
    if isinstance(x, str):
        s = x.strip()
        if not s:
            return None
        try:
            return json.loads(s)
        except Exception:
            return x
    return x


def _find_content_byid(document: Dict[str, Any]) -> Dict[str, Any]:
    cd = document.get("contentDomain", {})
    by_id = cd.get("byId", {})
    return by_id if isinstance(by_id, dict) else {}


def _load_demands_xlsx() -> List[Dict[str, str]]:
    here = os.path.dirname(os.path.abspath(__file__))
    candidates = [
        os.path.join(here, "demands.xlsx"),
        os.path.join(here, "assets", "demands.xlsx"),
    ]
    model_dir = os.getenv("AZUREML_MODEL_DIR")
    if model_dir:
        candidates += [
            os.path.join(model_dir, "demands.xlsx"),
            os.path.join(model_dir, "assets", "demands.xlsx"),
        ]

    for p in candidates:
        if os.path.exists(p):
            df = pd.read_excel(p)
            df.columns = [c.strip().lower() for c in df.columns]

            # accept id naming variants
            if "demand_id" not in df.columns and "id" in df.columns:
                df = df.rename(columns={"id": "demand_id"})
            if "description" not in df.columns and "demand_description" in df.columns:
                df = df.rename(columns={"demand_description": "description"})

            required = {"demand_id", "demand", "description"}
            missing = required - set(df.columns)
            if missing:
                raise RuntimeError(
                    f"demands.xlsx missing columns: {sorted(list(missing))}. Found: {list(df.columns)}"
                )

            out: List[Dict[str, str]] = []
            for _, r in df.fillna("").iterrows():
                did = str(r["demand_id"]).strip()
                if not did or did.lower() == "nan":
                    continue
                out.append(
                    {
                        "id": did,
                        "demand": str(r["demand"]).strip(),
                        "description": str(r["description"]).strip(),
                    }
                )
            logger.info("Loaded %d demands from %s", len(out), p)
            return out

    raise RuntimeError(f"demands.xlsx not found. Tried: {candidates}")


def _tokenize(s: str) -> set:
    s = (s or "").lower()
    s = re.sub(r"[^a-z0-9ąćęłńóśźż]+", " ", s)
    toks = [t for t in s.split() if len(t) >= 3]
    return set(toks)


def _build_demand_tokens():
    global DEMAND_TOKENS
    DEMAND_TOKENS = []
    for d in DEMANDS:
        toks = _tokenize(d["demand"] + " " + d["description"])
        DEMAND_TOKENS.append((d["id"], toks))


# -------------------------
# Predictors
# -------------------------
def _mock_predict(text: str, top_k: int) -> Tuple[float, List[Dict[str, Any]]]:
    """
    Token overlap against (demand+description). Returns:
      relevance float,
      preds [{label:<demand_id>, proba:<float>}]
    """
    t = (text or "").strip()
    if len(t) < MIN_CHUNK_LEN:
        return 0.0, []

    text_tokens = _tokenize(t)
    if not text_tokens:
        return 0.0, []

    scored = []
    for did, dtoks in DEMAND_TOKENS:
        if not dtoks:
            continue
        overlap = len(text_tokens & dtoks)
        if overlap <= 0:
            continue

        # permissive curve
        proba = 1.0 - (0.45 ** overlap)
        proba = max(0.0, min(0.99, float(proba)))
        if proba >= MIN_PROBA:
            scored.append((did, proba))

    if not scored:
        return 0.10, []

    scored.sort(key=lambda x: x[1], reverse=True)
    preds = [{"label": did, "proba": float(round(p, 4))} for did, p in scored[:max(1, int(top_k))]]
    relevance = float(round(max(p for _, p in scored), 4))
    return relevance, preds


def _aoai_predict(text: str, top_k: int) -> Tuple[float, List[Dict[str, Any]]]:
    """
    Returns:
      relevance float,
      preds [{label:<demand_id>, proba:<float>}]
    """
    t = (text or "").strip()
    if len(t) < MIN_CHUNK_LEN:
        return 0.0, []

    # Compact catalog
    lines = []
    for d in DEMANDS:
        desc = d["description"] or ""
        if len(desc) > 180:
            desc = desc[:180] + "…"
        lines.append(f'{d["id"]}: {d["demand"]} | {desc}')
    catalog = "\n".join(lines)

    system = "Return STRICT JSON only. No prose. No markdown."
    user = f"""
TEXT:
{t}

DEMANDS (id: demand | description):
{catalog}

Return JSON ONLY:
{{
  "relevantProba": 0.0,
  "predictions": [{{"id":"<demand_id>", "probability":0.0}}]
}}

Rules:
- relevantProba: 0..1
- Return up to {top_k} predictions sorted by probability desc
- prediction.id MUST be one of DEMANDS ids
- Do NOT invent ids
""".strip()

    resp = client.chat.completions.create(
        model=CHAT_DEPLOYMENT,
        messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
        temperature=0.0,
    )

    raw = (resp.choices[0].message.content or "").strip()
    try:
        data = json.loads(raw)
    except Exception:
        logger.warning("AOAI returned non-JSON (truncated): %r", raw[:500])
        return 0.0, []

    rel = float(data.get("relevantProba", 0.0) or 0.0)
    raw_preds = data.get("predictions", []) or []

    allowed = {d["id"] for d in DEMANDS}
    preds: List[Dict[str, Any]] = []

    if isinstance(raw_preds, list):
        for p in raw_preds[: max(1, int(top_k))]:
            if not isinstance(p, dict):
                continue
            did = str(p.get("id", "")).strip()
            try:
                prob = float(p.get("probability"))
            except Exception:
                continue
            if did in allowed:
                prob = max(0.0, min(1.0, prob))
                if prob >= MIN_PROBA:
                    preds.append({"label": did, "proba": prob})

    preds.sort(key=lambda x: x["proba"], reverse=True)

    # If model says very low relevance, we still keep preds if present (to enable highlight debugging)
    rel = max(0.0, min(1.0, rel))
    if rel < AOAI_REL_THRESHOLD and not preds:
        return rel, []

    # Relevance should not be lower than top prediction if there is one
    if preds:
        rel = max(rel, float(preds[0]["proba"]))

    return rel, preds


def _predict(text: str, top_k: int) -> Tuple[float, List[Dict[str, Any]]]:
    if USE_MOCK:
        return _mock_predict(text, top_k)
    return _aoai_predict(text, top_k)


# -------------------------
# AML entrypoints
# -------------------------
def init():
    global client, CHAT_DEPLOYMENT, DEMANDS, USE_MOCK

    USE_MOCK = os.getenv("USE_MOCK", "1").strip() == "1"
    DEMANDS = _load_demands_xlsx()
    _build_demand_tokens()

    logger.info("init(): USE_MOCK=%s demands=%d", USE_MOCK, len(DEMANDS))

    if not USE_MOCK:
        if AzureOpenAI is None:
            raise RuntimeError("openai package not available but USE_MOCK=0")

        aoai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "").strip()
        aoai_key = os.getenv("AZURE_OPENAI_API_KEY", "").strip()
        aoai_version = os.getenv("AZURE_OPENAI_API_VERSION", "").strip() or "2024-06-01"

        CHAT_DEPLOYMENT = (
            os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT", "").strip()
            or os.getenv("AZURE_OPENAI_DEPLOYMENT", "").strip()
        )

        if not (aoai_endpoint and aoai_key and CHAT_DEPLOYMENT):
            raise RuntimeError(
                "Missing AOAI env vars. Need: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, "
                "AZURE_OPENAI_CHAT_DEPLOYMENT (or AZURE_OPENAI_DEPLOYMENT)."
            )

        client = AzureOpenAI(
            azure_endpoint=aoai_endpoint,
            api_key=aoai_key,
            api_version=aoai_version,
        )


def run(raw_data):
    """
    Contract (must not crash app):
      { "predictions": <document> }
    document MUST include:
      documentDemandPredictions: ["id1","id2", ...]  (ARRAY OF STRINGS)
    and per chunk:
      relevantProba: float
      cdTransformerPredictions: [{label, proba}]
      cdLogregPredictions: SAME SHAPE (mirror)  <-- critical for your UI
    """
    try:
        req = _json_load_maybe(raw_data)
        if not isinstance(req, dict):
            return {"error": "Bad request: body must be JSON object", "predictions": {"documentDemandPredictions": []}}

        if "document" not in req:
            return {"error": "Bad request: missing 'document'", "predictions": {"documentDemandPredictions": []}}

        document = _json_load_maybe(req["document"])
        if not isinstance(document, dict):
            return {"error": "Bad request: 'document' must be JSON object", "predictions": {"documentDemandPredictions": []}}

        num_preds = int(req.get("num_preds", 3))
        num_preds = max(1, min(10, num_preds))

        by_id = _find_content_byid(document)

        if MAX_DEBUG_CHUNKS > 0:
            logger.info("run(): USE_MOCK=%s chunks=%d num_preds=%d", USE_MOCK, len(by_id), num_preds)

        best_doc_probs: Dict[str, float] = {}
        debug_left = MAX_DEBUG_CHUNKS

        for cid, content in by_id.items():
            if not isinstance(content, dict):
                continue

            text = content.get("text", "")
            if debug_left > 0:
                logger.info("chunk=%s text_sample=%r", cid, (str(text)[:180] if text is not None else None))
                debug_left -= 1

            rel, preds = _predict(text, top_k=num_preds)

            # Always set expected keys/types
            content["relevantProba"] = float(rel)
            content["cdTransformerPredictions"] = preds if isinstance(preds, list) else []
            # CRITICAL: mirror to logreg for UI selection logic
            content["cdLogregPredictions"] = list(content["cdTransformerPredictions"])

            for p in content["cdTransformerPredictions"]:
                try:
                    did = str(p.get("label"))
                    prob = float(p.get("proba", 0.0))
                except Exception:
                    continue
                if did and (did not in best_doc_probs or prob > best_doc_probs[did]):
                    best_doc_probs[did] = prob

        # Document-level: ARRAY OF STRINGS (IDs only), sorted by confidence
        ids_sorted = [k for k, v in sorted(best_doc_probs.items(), key=lambda kv: kv[1], reverse=True)]
        document["documentDemandPredictions"] = ids_sorted[:num_preds]

        return {"predictions": document}

    except Exception as e:
        logger.exception("run() failed")
        return {"error": str(e), "predictions": {"documentDemandPredictions": []}}
