import json
import logging
import os
import re
from typing import Any, Dict, List, Tuple, Optional

import pandas as pd

try:
    from openai import AzureOpenAI
except Exception:
    AzureOpenAI = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("score")

# -------------------------
# Tunables (env)
# -------------------------
USE_MOCK = os.getenv("USE_MOCK", "0").strip() == "1"

MIN_CHUNK_LEN = int(os.getenv("MIN_CHUNK_LEN", "10"))
MAX_LABELS_PER_CHUNK = int(os.getenv("MAX_LABELS_PER_CHUNK", "3"))
CANDIDATES_TOPK = int(os.getenv("CANDIDATES_TOPK", "10"))

LABEL_MIN_PROB = float(os.getenv("LABEL_MIN_PROB", "0.75"))
HIGHLIGHT_MIN_PROB = float(os.getenv("HIGHLIGHT_MIN_PROB", "0.80"))
RELEVANCE_MIN = float(os.getenv("RELEVANCE_MIN", "0.60"))

MAX_SENTENCES_PER_CHUNK = int(os.getenv("MAX_SENTENCES_PER_CHUNK", "10"))
MAX_SENTENCES_TO_LLM = int(os.getenv("MAX_SENTENCES_TO_LLM", "4"))

USE_LLM = os.getenv("USE_LLM", "1").strip() == "1" and not USE_MOCK
MAX_CANDIDATES_TO_LLM = int(os.getenv("MAX_CANDIDATES_TO_LLM", "10"))

# Gate policy
REQUIRE_MODAL_OR_STRUCTURE = os.getenv("REQUIRE_MODAL_OR_STRUCTURE", "1").strip() == "1"
ALLOW_REQUIREMENTS_WITHOUT_MODAL = os.getenv("ALLOW_REQUIREMENTS_WITHOUT_MODAL", "0").strip() == "1"

# Validation looseness
CRITERIA_MIN_OVERLAP = int(os.getenv("CRITERIA_MIN_OVERLAP", "2"))
DESC_MATCH_MIN_OVERLAP = int(os.getenv("DESC_MATCH_MIN_OVERLAP", "2"))

# Retrieval fallback
ALWAYS_RETURN_CANDIDATES = os.getenv("ALWAYS_RETURN_CANDIDATES", "1").strip() == "1"
NGRAM_N = int(os.getenv("NGRAM_N", "3"))

MAX_DEBUG_CHUNKS = int(os.getenv("MAX_DEBUG_CHUNKS", "0"))

client = None
CHAT_DEPLOYMENT = None

# -------------------------
# Demands cache
# -------------------------
DEMANDS: List[Dict[str, str]] = []
DEMAND_NAME: Dict[str, str] = {}
DEMAND_DESC: Dict[str, str] = {}
DEMAND_TEXT: Dict[str, str] = {}

DEMAND_TOKS_ALL: Dict[str, set] = {}
DEMAND_TOKS_DESC: Dict[str, set] = {}
DEMAND_TOKS_NAME: Dict[str, set] = {}
DEMAND_NGRAMS_ALL: Dict[str, set] = {}  # id -> char ngrams of (demand + description)

# -------------------------
# Stopwords (English ONLY), no domain stopwords per your request
# Keep requirement signal words.
# -------------------------
STOPWORDS = {
    "the", "and", "or", "to", "of", "in", "on", "for", "with", "a", "an", "as", "at", "by",
    "from", "into", "over", "under", "than", "then", "that", "this", "these", "those",
    "be", "is", "are", "was", "were", "been", "being",
    "it", "its", "they", "them", "their", "we", "our", "you", "your",
    "can", "could", "may", "might", "will", "would",
    "not", "no", "yes",
}

# -------------------------
# Requirement heuristics (EN)
# -------------------------
REQ_MODAL_RE = re.compile(
    r"\b(shall|must|required|requirement|should|need to|has to|may not|must not|shall not|prohibit|forbidden)\b",
    re.IGNORECASE,
)
REQ_STRUCT_RE = re.compile(
    r"\b(acceptance|test(ed|ing)?|verify|verification|criteria|tolerance|limit|threshold|conform|compliance)\b",
    re.IGNORECASE,
)
REQ_NUM_RE = re.compile(r"\b\d+([.,]\d+)?\b")
UNIT_RE = re.compile(r"\b(mm|cm|m|kg|w|kw|mw|v|kv|a|ma|hz|rpm|bar|pa|degc|°c|ip\d{2}\b)", re.IGNORECASE)

HEADING_RE = re.compile(r"^\s*([A-Z0-9][A-Z0-9 \-/,:._]{3,}|[0-9]+(\.[0-9]+)+\s+.+)\s*$")


# -------------------------
# Helpers
# -------------------------
def _json_load_maybe(x: Any) -> Any:
    if isinstance(x, (bytes, bytearray)):
        x = x.decode("utf-8", errors="replace")
    if isinstance(x, str):
        s = x.strip()
        if not s:
            return None
        try:
            return json.loads(s)
        except Exception:
            return x
    return x


def _find_content_byid(document: Dict[str, Any]) -> Dict[str, Any]:
    cd = document.get("contentDomain", {})
    by_id = cd.get("byId", {})
    return by_id if isinstance(by_id, dict) else {}


def _load_demands_xlsx() -> List[Dict[str, str]]:
    here = os.path.dirname(os.path.abspath(__file__))
    candidates = [
        os.path.join(here, "demands.xlsx"),
        os.path.join(here, "assets", "demands.xlsx"),
    ]
    model_dir = os.getenv("AZUREML_MODEL_DIR")
    if model_dir:
        candidates += [
            os.path.join(model_dir, "demands.xlsx"),
            os.path.join(model_dir, "assets", "demands.xlsx"),
        ]

    for p in candidates:
        if os.path.exists(p):
            df = pd.read_excel(p)
            df.columns = [c.strip().lower() for c in df.columns]
            if "demand_id" not in df.columns and "id" in df.columns:
                df = df.rename(columns={"id": "demand_id"})
            if "description" not in df.columns and "demand_description" in df.columns:
                df = df.rename(columns={"demand_description": "description"})

            required = {"demand_id", "demand", "description"}
            missing = required - set(df.columns)
            if missing:
                raise RuntimeError(f"demands.xlsx missing columns: {sorted(list(missing))}. Found: {list(df.columns)}")

            out: List[Dict[str, str]] = []
            for _, r in df.fillna("").iterrows():
                did = str(r["demand_id"]).strip()
                if not did or did.lower() == "nan":
                    continue
                out.append({"id": did, "demand": str(r["demand"]).strip(), "description": str(r["description"]).strip()})
            logger.info("Loaded %d demands from %s", len(out), p)
            return out

    raise RuntimeError(f"demands.xlsx not found. Tried: {candidates}")


def _tokenize(s: str) -> set:
    s = (s or "").lower()
    s = re.sub(r"[^a-z0-9]+", " ", s)
    toks = []
    for t in s.split():
        if len(t) < 3:
            continue
        if t in STOPWORDS:
            continue
        toks.append(t)
    return set(toks)


def _normalize_for_ngrams(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"[^a-z0-9 ]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _char_ngrams(s: str, n: int) -> set:
    s = _normalize_for_ngrams(s)
    if len(s) < n:
        return {s} if s else set()
    grams = set()
    # include spaces (important for phrase-like matching)
    for i in range(0, len(s) - n + 1):
        grams.add(s[i : i + n])
    return grams


def _jaccard(a: set, b: set) -> float:
    if not a or not b:
        return 0.0
    inter = len(a & b)
    if inter == 0:
        return 0.0
    union = len(a | b)
    return float(inter) / float(union) if union else 0.0


def _build_demand_indexes():
    global DEMAND_NAME, DEMAND_DESC, DEMAND_TEXT
    global DEMAND_TOKS_ALL, DEMAND_TOKS_DESC, DEMAND_TOKS_NAME, DEMAND_NGRAMS_ALL

    DEMAND_NAME = {}
    DEMAND_DESC = {}
    DEMAND_TEXT = {}
    DEMAND_TOKS_ALL = {}
    DEMAND_TOKS_DESC = {}
    DEMAND_TOKS_NAME = {}
    DEMAND_NGRAMS_ALL = {}

    for d in DEMANDS:
        did = d["id"]
        name = d.get("demand", "") or ""
        desc = d.get("description", "") or ""

        DEMAND_NAME[did] = name
        DEMAND_DESC[did] = desc

        DEMAND_TOKS_NAME[did] = _tokenize(name)
        DEMAND_TOKS_DESC[did] = _tokenize(desc)
        DEMAND_TOKS_ALL[did] = set(DEMAND_TOKS_NAME[did] | DEMAND_TOKS_DESC[did])

        # ngrams on full text improve retrieval robustness when token overlap fails
        full = (name + " " + desc).strip()
        DEMAND_NGRAMS_ALL[did] = _char_ngrams(full, NGRAM_N)

        desc_short = desc if len(desc) <= 220 else (desc[:220] + "…")
        DEMAND_TEXT[did] = f"{name} | {desc_short}"


def _split_into_sentences_or_lines(chunk_text: str) -> List[str]:
    t = (chunk_text or "").strip()
    if not t:
        return []
    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    tableish = (len(lines) >= 6) or any(("|" in ln or "\t" in ln) for ln in lines)
    if tableish:
        out = [ln for ln in lines if len(ln) >= MIN_CHUNK_LEN]
        return out[:MAX_SENTENCES_PER_CHUNK]
    parts = re.split(r"(?<=[.!?])\s+|\n+", t)
    parts = [p.strip() for p in parts if p and p.strip()]
    return parts[:MAX_SENTENCES_PER_CHUNK]


def _is_heading_like(s: str) -> bool:
    t = (s or "").strip()
    if len(t) < 4:
        return False
    if HEADING_RE.match(t) and len(t.split()) <= 12:
        return True
    upp = sum(1 for c in t if c.isalpha() and c.isupper())
    low = sum(1 for c in t if c.isalpha() and c.islower())
    if upp > 10 and low < upp / 3:
        return True
    return False


def _is_requirement_fast(text: str) -> Tuple[bool, float]:
    t = (text or "").strip()
    if len(t) < MIN_CHUNK_LEN:
        return False, 0.0
    if _is_heading_like(t):
        return False, 0.10

    has_modal = bool(REQ_MODAL_RE.search(t))
    has_num = bool(REQ_NUM_RE.search(t))
    has_unit = bool(UNIT_RE.search(t))
    has_struct = bool(REQ_STRUCT_RE.search(t))

    if has_modal and (has_num or has_unit or has_struct):
        return True, 0.88
    if has_modal:
        return True, 0.72
    if (has_num and has_unit) or has_struct:
        return True, 0.65
    return False, 0.25


# -------------------------
# Retrieval (NEVER return empty when ALWAYS_RETURN_CANDIDATES=1)
# -------------------------
def _candidate_retrieval(sentence: str, k: int) -> List[str]:
    stoks = _tokenize(sentence)

    scored_tok: List[Tuple[str, int]] = []
    if stoks:
        for did, dtoks in DEMAND_TOKS_ALL.items():
            if not dtoks:
                continue
            overlap = len(stoks & dtoks)
            if overlap > 0:
                scored_tok.append((did, overlap))
        scored_tok.sort(key=lambda x: x[1], reverse=True)

    # Primary: token overlap
    if scored_tok:
        return [did for did, _ in scored_tok[: max(1, k)]]

    # Fallback: char ngram similarity (robust to tokenization/OCR/form variations)
    if ALWAYS_RETURN_CANDIDATES:
        sgrams = _char_ngrams(sentence, NGRAM_N)
        scored_ng: List[Tuple[str, float]] = []
        for did, grams in DEMAND_NGRAMS_ALL.items():
            if not grams:
                continue
            sim = _jaccard(sgrams, grams)
            if sim > 0.0:
                scored_ng.append((did, sim))
        scored_ng.sort(key=lambda x: x[1], reverse=True)
        if scored_ng:
            return [did for did, _ in scored_ng[: max(1, k)]]

        # Last resort: return first k demand IDs deterministically (still table-only)
        return [d["id"] for d in DEMANDS[: max(1, k)]]

    return []


def _extract_desc_key_phrases(desc: str, max_phrases: int = 3) -> List[str]:
    d = (desc or "").strip()
    if not d:
        return []
    parts = re.split(r"(?<=[.!?;:])\s+|\n+", d)
    parts = [p.strip() for p in parts if p and len(p.strip()) >= 8]

    scored: List[Tuple[int, str]] = []
    for p in parts:
        score = 0
        if REQ_MODAL_RE.search(p):
            score += 3
        if REQ_STRUCT_RE.search(p):
            score += 2
        if REQ_NUM_RE.search(p):
            score += 1
        if UNIT_RE.search(p):
            score += 1
        scored.append((score, p))

    scored.sort(key=lambda x: x[0], reverse=True)
    out = [p for sc, p in scored if sc > 0][:max_phrases]
    if not out:
        out = parts[:max_phrases]
    return out


def _build_candidates_block(candidate_ids: List[str]) -> str:
    lines = []
    for did in candidate_ids:
        name = DEMAND_NAME.get(did, "")
        desc = DEMAND_DESC.get(did, "")
        keys = _extract_desc_key_phrases(desc, max_phrases=3)
        keys_block = "\n    - " + "\n    - ".join(keys) if keys else ""
        lines.append(
            f"- id: {did}\n"
            f"  demand: {name}\n"
            f"  description: {desc}\n"
            f"  keyPhrases:{keys_block}"
        )
    return "\n".join(lines)


def _criteria_supported_by_description(criteria_quotes: List[str], did: str) -> bool:
    desc = (DEMAND_DESC.get(did, "") or "")
    if not desc:
        return False
    desc_lower = desc.lower()
    desc_toks = DEMAND_TOKS_DESC.get(did, set())

    for q in criteria_quotes:
        q = (q or "").strip()
        if len(q) < 4:
            continue
        q_lower = q.lower()
        if q_lower in desc_lower:
            return True
        q_toks = _tokenize(q)
        if len(q_toks & desc_toks) >= CRITERIA_MIN_OVERLAP:
            return True
    return False


# -------------------------
# LLM
# -------------------------
def _llm_gate_and_match(sentence: str, candidate_ids: List[str], top_k: int) -> Dict[str, Any]:
    if client is None or not CHAT_DEPLOYMENT:
        return {"isRequirement": False, "relevance": 0.0, "labels": [], "notes": "no_client"}

    if not candidate_ids:
        return {"isRequirement": False, "relevance": 0.0, "labels": [], "notes": "no_candidates"}

    candidate_ids = candidate_ids[:MAX_CANDIDATES_TO_LLM]
    candidates_block = _build_candidates_block(candidate_ids)

    system = (
        "You are a strict requirements-to-demand matcher for technical specifications. "
        "You MUST base your decision ONLY on the provided candidates (demand + description + keyPhrases) and the sentence. "
        "Return STRICT JSON only."
    )

    user = f"""
TASK A — REQUIREMENT GATE:
Decide if the SENTENCE is a concrete requirement/specification that should be highlighted.
If it is just a topic mention, heading, context, or general narrative, it is NOT a requirement.

TASK B — MATCHING (only if isRequirement=true):
Select up to {min(int(top_k), MAX_LABELS_PER_CHUNK)} candidate IDs that truly apply.

CRITICAL RULES FOR EACH SELECTED LABEL:
1) evidenceQuote: exact substring copied from the SENTENCE (5–25 words).
2) criteriaQuotes: 1–3 short phrases that come from the candidate's DESCRIPTION or keyPhrases.
   Keep each criteria quote short (max ~12 words).
3) probability: 0..1, calibrated. Do NOT output 1.0 unless the match is extremely specific.

CANDIDATES:
{candidates_block}

SENTENCE:
{sentence}

Return JSON ONLY:
{{
  "isRequirement": true,
  "relevance": 0.0,
  "labels": [
    {{
      "id": "candidate_id",
      "probability": 0.0,
      "evidenceQuote": "exact substring from sentence",
      "criteriaQuotes": ["short phrase from description or keyPhrases"]
    }}
  ],
  "notes": "short"
}}

Hard constraints:
- If the sentence only contains a keyword but does not satisfy description conditions, DO NOT select it.
- If you cannot justify using the description/keyPhrases, DO NOT select it.
""".strip()

    resp = client.chat.completions.create(
        model=CHAT_DEPLOYMENT,
        messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
        temperature=0.0,
        max_tokens=900,
    )

    raw = (resp.choices[0].message.content or "").strip()
    try:
        data = json.loads(raw)
    except Exception:
        return {"isRequirement": False, "relevance": 0.0, "labels": [], "notes": "non_json"}

    is_req = bool(data.get("isRequirement", False))
    try:
        rel = float(data.get("relevance", 0.0) or 0.0)
    except Exception:
        rel = 0.0

    if not is_req or rel < RELEVANCE_MIN:
        return {"isRequirement": False, "relevance": float(rel), "labels": [], "notes": "gate_failed"}

    labels_raw = data.get("labels", []) or []
    if not isinstance(labels_raw, list):
        labels_raw = []

    sent_lower = (sentence or "").lower()
    stoks = _tokenize(sentence)

    out: List[Dict[str, Any]] = []
    for item in labels_raw[: min(int(top_k), MAX_LABELS_PER_CHUNK)]:
        if not isinstance(item, dict):
            continue

        did = str(item.get("id", "")).strip()
        if did not in candidate_ids:
            continue

        try:
            prob = float(item.get("probability", 0.0) or 0.0)
        except Exception:
            continue
        if prob < LABEL_MIN_PROB:
            continue
        prob = min(prob, 0.97)

        ev = str(item.get("evidenceQuote", "") or "").strip()
        if not ev or ev.lower() not in sent_lower:
            continue

        cq = item.get("criteriaQuotes", [])
        if not isinstance(cq, list):
            cq = []
        cq = [str(x).strip() for x in cq if str(x).strip()]
        if not cq:
            continue

        if not _criteria_supported_by_description(cq, did):
            continue

        # zero lexical support reject (still table-only)
        dtoks = DEMAND_TOKS_ALL.get(did, set())
        if not dtoks or len(stoks & dtoks) == 0:
            continue

        out.append({"id": did, "probability": float(max(0.0, min(1.0, prob)))})

    out.sort(key=lambda x: x["probability"], reverse=True)
    out = out[:MAX_LABELS_PER_CHUNK]

    if not out:
        return {"isRequirement": True, "relevance": float(rel), "labels": [], "notes": "no_labels_after_validation"}

    rel = max(float(rel), float(out[0]["probability"]))
    return {"isRequirement": True, "relevance": float(rel), "labels": out, "notes": "ok"}


# -------------------------
# Deterministic fallback (description-first)
# -------------------------
def _deterministic_fallback(sentence: str, candidate_ids: List[str], top_k: int) -> Dict[str, Any]:
    is_req, rel = _is_requirement_fast(sentence)
    if not is_req or rel < RELEVANCE_MIN:
        return {"isRequirement": False, "relevance": float(rel), "labels": [], "notes": "fallback_gate_failed"}

    stoks = _tokenize(sentence)
    if not stoks:
        return {"isRequirement": True, "relevance": float(rel), "labels": [], "notes": "fallback_no_tokens"}

    # If retrieval returned nothing (should not happen with ALWAYS_RETURN_CANDIDATES=1),
    # still ensure we have some candidates.
    if not candidate_ids:
        candidate_ids = [d["id"] for d in DEMANDS[: max(1, CANDIDATES_TOPK)]]

    scored: List[Tuple[str, int, int]] = []
    for did in candidate_ids:
        desc_toks = DEMAND_TOKS_DESC.get(did, set())
        name_toks = DEMAND_TOKS_NAME.get(did, set())
        if not desc_toks:
            continue

        desc_overlap = len(stoks & desc_toks)
        name_overlap = len(stoks & name_toks)

        # require description overlap so we don't match on demand name keyword alone
        if desc_overlap < DESC_MATCH_MIN_OVERLAP:
            continue

        scored.append((did, desc_overlap, name_overlap))

    if not scored:
        return {"isRequirement": True, "relevance": float(rel), "labels": [], "notes": "fallback_no_desc_match"}

    scored.sort(key=lambda x: (x[1], x[2]), reverse=True)

    labels: List[Dict[str, Any]] = []
    for did, d_ov, n_ov in scored[:min(top_k, MAX_LABELS_PER_CHUNK)]:
        p = min(0.95, 0.70 + 0.05 * float(d_ov - DESC_MATCH_MIN_OVERLAP) + 0.02 * float(n_ov))
        if p >= LABEL_MIN_PROB:
            labels.append({"id": did, "probability": float(p)})

    labels.sort(key=lambda x: x["probability"], reverse=True)
    labels = labels[:MAX_LABELS_PER_CHUNK]

    if not labels:
        return {"isRequirement": True, "relevance": float(rel), "labels": [], "notes": "fallback_probs_too_low"}

    rel = max(float(rel), float(labels[0]["probability"]))
    return {"isRequirement": True, "relevance": float(rel), "labels": labels, "notes": "fallback_ok"}


def _predict_sentence(sentence: str, top_k: int) -> Dict[str, Any]:
    cands = _candidate_retrieval(sentence, CANDIDATES_TOPK)

    if USE_MOCK or not USE_LLM:
        return _deterministic_fallback(sentence, cands, top_k)

    pred = _llm_gate_and_match(sentence, cands, top_k)

    # If LLM says requirement but yields no labels, fallback
    if bool(pred.get("isRequirement", False)) and not (pred.get("labels") or []):
        fb = _deterministic_fallback(sentence, cands, top_k)
        if fb.get("labels"):
            return fb
    return pred


# -------------------------
# AML entrypoints
# -------------------------
def init():
    global client, CHAT_DEPLOYMENT, DEMANDS, USE_MOCK

    USE_MOCK = os.getenv("USE_MOCK", "0").strip() == "1"
    DEMANDS = _load_demands_xlsx()
    _build_demand_indexes()

    logger.info(
        "init(): USE_MOCK=%s USE_LLM=%s demands=%d CANDIDATES_TOPK=%d ALWAYS_RETURN_CANDIDATES=%s NGRAM_N=%d "
        "LABEL_MIN_PROB=%.2f HIGHLIGHT_MIN_PROB=%.2f RELEVANCE_MIN=%.2f",
        USE_MOCK, USE_LLM, len(DEMANDS), CANDIDATES_TOPK, ALWAYS_RETURN_CANDIDATES, NGRAM_N,
        LABEL_MIN_PROB, HIGHLIGHT_MIN_PROB, RELEVANCE_MIN
    )

    if USE_LLM and not USE_MOCK:
        if AzureOpenAI is None:
            raise RuntimeError("openai package not available but USE_LLM=1")

        aoai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "").strip()
        aoai_key = os.getenv("AZURE_OPENAI_API_KEY", "").strip()
        aoai_version = os.getenv("AZURE_OPENAI_API_VERSION", "").strip() or "2024-06-01"

        CHAT_DEPLOYMENT = (
            os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT", "").strip()
            or os.getenv("AZURE_OPENAI_DEPLOYMENT", "").strip()
        )

        if not (aoai_endpoint and aoai_key and CHAT_DEPLOYMENT):
            raise RuntimeError(
                "Missing AOAI env vars. Need: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, "
                "AZURE_OPENAI_CHAT_DEPLOYMENT (or AZURE_OPENAI_DEPLOYMENT)."
            )

        client = AzureOpenAI(
            azure_endpoint=aoai_endpoint,
            api_key=aoai_key,
            api_version=aoai_version,
        )


def run(raw_data):
    """
    Contract (do not break):
      { "predictions": <document> }

    document MUST include:
      documentDemandPredictions: ["id1","id2", ...]  (ARRAY OF STRINGS)

    per chunk:
      relevantProba: float
      cdTransformerPredictions: [{label, proba}]
      cdLogregPredictions: SAME SHAPE (mirror)
    """
    try:
        req = _json_load_maybe(raw_data)
        if not isinstance(req, dict):
            return {"error": "Bad request: body must be JSON object", "predictions": {"documentDemandPredictions": []}}

        if "document" not in req:
            return {"error": "Bad request: missing 'document'", "predictions": {"documentDemandPredictions": []}}

        document = _json_load_maybe(req["document"])
        if not isinstance(document, dict):
            return {"error": "Bad request: 'document' must be JSON object", "predictions": {"documentDemandPredictions": []}}

        num_preds = int(req.get("num_preds", MAX_LABELS_PER_CHUNK))
        num_preds = max(1, min(10, num_preds))
        num_preds = min(num_preds, MAX_LABELS_PER_CHUNK)

        by_id = _find_content_byid(document)

        best_doc_probs: Dict[str, float] = {}
        debug_left = MAX_DEBUG_CHUNKS

        for cid, content in by_id.items():
            if not isinstance(content, dict):
                continue

            chunk_text = (str(content.get("text", "") or "")).strip()

            content["relevantProba"] = 0.0
            content["cdTransformerPredictions"] = []
            content["cdLogregPredictions"] = []
            content["highlightText"] = ""

            if debug_left > 0:
                logger.info("chunk=%s sample=%r", cid, chunk_text[:180])
                debug_left -= 1

            sentences = _split_into_sentences_or_lines(chunk_text)
            if not sentences:
                continue

            # Preselect likely requirements
            scored_sents: List[Tuple[float, str]] = []
            for s in sentences:
                if _is_heading_like(s):
                    scored_sents.append((0.0, s))
                    continue
                is_req, rel_fast = _is_requirement_fast(s)
                scored_sents.append((rel_fast if is_req else 0.0, s))

            scored_sents.sort(key=lambda x: x[0], reverse=True)
            candidates_for_model = [s for sc, s in scored_sents[:MAX_SENTENCES_TO_LLM] if s.strip()]

            best_chunk_pred: Optional[Dict[str, Any]] = None
            best_top1 = 0.0
            best_sentence = ""

            for s in candidates_for_model:
                pred = _predict_sentence(s, top_k=num_preds)

                is_req = bool(pred.get("isRequirement", False))
                labels = pred.get("labels", []) or []
                if not is_req or not isinstance(labels, list) or not labels:
                    continue

                labels_sorted = sorted(labels, key=lambda x: float(x.get("probability", 0.0) or 0.0), reverse=True)
                top1 = float(labels_sorted[0].get("probability", 0.0) or 0.0)

                if top1 > best_top1:
                    best_top1 = top1
                    best_chunk_pred = pred
                    best_sentence = s

            if best_chunk_pred is None:
                continue

            labels = best_chunk_pred.get("labels", []) or []
            labels_sorted = sorted(labels, key=lambda x: float(x.get("probability", 0.0) or 0.0), reverse=True)
            labels_sorted = labels_sorted[:num_preds]

            final_labels = [{"label": str(lb["id"]), "proba": float(lb["probability"])} for lb in labels_sorted]

            top1 = float(final_labels[0]["proba"]) if final_labels else 0.0
            is_req = bool(best_chunk_pred.get("isRequirement", False))
            try:
                rel = float(best_chunk_pred.get("relevance", 0.0) or 0.0)
            except Exception:
                rel = 0.0

            # Highlight guard: avoid topic mentions
            if REQUIRE_MODAL_OR_STRUCTURE and not ALLOW_REQUIREMENTS_WITHOUT_MODAL:
                has_modal = bool(REQ_MODAL_RE.search(best_sentence))
                has_struct = bool(REQ_STRUCT_RE.search(best_sentence))
                has_num_unit = bool(REQ_NUM_RE.search(best_sentence) and (UNIT_RE.search(best_sentence) is not None))
                if not (has_modal or has_struct or has_num_unit):
                    is_req = False

            should_highlight = bool(is_req and top1 >= HIGHLIGHT_MIN_PROB)

            if should_highlight and final_labels:
                content["relevantProba"] = float(max(rel, top1))
                content["cdTransformerPredictions"] = final_labels
                content["cdLogregPredictions"] = list(final_labels)
                content["highlightText"] = best_sentence

                for p in final_labels:
                    did = p["label"]
                    prob = float(p["proba"])
                    if did not in best_doc_probs or prob > best_doc_probs[did]:
                        best_doc_probs[did] = prob

        ids_sorted = [k for k, v in sorted(best_doc_probs.items(), key=lambda kv: kv[1], reverse=True)]
        document["documentDemandPredictions"] = ids_sorted[:num_preds]

        return {"predictions": document}

    except Exception as e:
        logger.exception("run() failed")
        return {"error": str(e), "predictions": {"documentDemandPredictions": []}}

