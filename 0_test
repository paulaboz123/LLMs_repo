import os
import json
import time
import logging
from typing import Any, Dict, List, Optional

import pandas as pd
from openai import AzureOpenAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure OpenAI client
_client: Optional[AzureOpenAI] = None
_deployment: Optional[str] = None

# Demands (loaded from local file next to score.py)
_demands: List[Dict[str, str]] = []  # {"id":..., "name":..., "description":...}


# ----------------------------
# Utilities (hardening)
# ----------------------------
def _this_dir() -> str:
    return os.path.dirname(os.path.abspath(__file__))


def _ensure_list_of_strings(x: Any) -> List[str]:
    """
    CRITICAL: application expects JSON array of strings.
    If x is list of dicts -> extract "id".
    If x is a single string -> wrap into [string].
    Otherwise -> [].
    """
    if x is None:
        return []
    if isinstance(x, str):
        s = x.strip()
        return [s] if s else []
    if isinstance(x, list):
        out: List[str] = []
        for item in x:
            if isinstance(item, str):
                s = item.strip()
                if s:
                    out.append(s)
            elif isinstance(item, dict):
                # if someone mistakenly returns [{"id": "...", "probability": ...}]
                s = str(item.get("id", "")).strip()
                if s:
                    out.append(s)
        return out
    # sets, tuples, etc.
    try:
        return [str(v).strip() for v in list(x) if str(v).strip()]
    except Exception:
        return []


def _safe_json_loads(s: str) -> Optional[Any]:
    try:
        return json.loads(s)
    except Exception:
        return None


def _find_demands_file() -> str:
    base = _this_dir()
    # Demands obok score.py (jak u Ciebie)
    for name in ["demands.xlsx", "demands.csv", "demands.json"]:
        p = os.path.join(base, name)
        if os.path.isfile(p):
            return p
    raise FileNotFoundError("Missing demands file next to score.py (demands.xlsx/csv/json).")


def _load_demands() -> List[Dict[str, str]]:
    """
    Your Excel: demand_id, demand, demand_description
    We use demand_description for context, but output always uses demand_id.
    """
    path = _find_demands_file()
    logger.info(f"Loading demands from: {path}")

    if path.endswith(".xlsx"):
        df = pd.read_excel(path)
    elif path.endswith(".csv"):
        df = pd.read_csv(path)
    elif path.endswith(".json"):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict) and "demands" in data:
            data = data["demands"]
        out = []
        for d in data if isinstance(data, list) else []:
            if not isinstance(d, dict):
                continue
            did = str(d.get("demand_id") or d.get("id") or "").strip()
            name = str(d.get("demand") or d.get("name") or "").strip()
            desc = str(d.get("demand_description") or d.get("description") or d.get("clarification") or "").strip()
            if did:
                out.append({"id": did, "name": name, "description": desc})
        return out
    else:
        raise ValueError("Unsupported demands file type.")

    # Normalize DF
    cols = {c.strip() for c in df.columns}
    req = {"demand_id", "demand", "demand_description"}
    missing = req - cols
    if missing:
        raise ValueError(f"Demands missing columns: {sorted(missing)}")

    out: List[Dict[str, str]] = []
    for _, row in df.iterrows():
        did = str(row["demand_id"]).strip()
        name = str(row["demand"]).strip()
        desc = str(row["demand_description"]).strip()
        if did:
            out.append({"id": did, "name": name, "description": desc})
    return out


def _demands_context(demands: List[Dict[str, str]]) -> str:
    # Keep compact; description is key for match
    return "\n".join(
        f"- ID: {d['id']}\n  Name: {d['name']}\n  Clarification: {d['description']}"
        for d in demands
    )


def _prompt(demands_ctx: str, chunk_id: str, chunk_text: str, topn: int) -> str:
    # Single-chunk prompt: less token bloat, less chaos
    # Still based on your rules: match only when Clarification aligns.
    return f"""
Rules:
- Use ONLY demand IDs from the list below
- Match ONLY when chunk aligns with demand Clarification (WHEN)
- Max {topn} demands
- If none, return empty list
- Return JSON ONLY

Demands:
{demands_ctx}

ChunkId: "{chunk_id}"
Text: "{chunk_text}"

Return JSON:
{{
  "demandIds": [{{"id":"demand_id","probability":0.85}}],
  "explanation": "brief"
}}
""".strip()


def _call_llm(prompt: str) -> Dict[str, Any]:
    assert _client is not None and _deployment is not None
    resp = _client.chat.completions.create(
        model=_deployment,
        messages=[
            {"role": "system", "content": "Return ONLY valid JSON. No markdown, no prose."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.0,
        max_tokens=600,
    )
    content = (resp.choices[0].message.content or "").strip()
    # robust parse
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        s = content.find("{")
        e = content.rfind("}")
        if s >= 0 and e > s:
            return json.loads(content[s:e+1])
        return {"demandIds": [], "explanation": "parse_error"}


def init():
    global _client, _deployment, _demands
    logger.info("init() starting")

    _demands = _load_demands()
    if not _demands:
        raise RuntimeError("Demands list is empty.")

    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "").strip()
    api_key = os.getenv("AZURE_OPENAI_API_KEY", "").strip()
    api_version = os.getenv("AZURE_OPENAI_API_VERSION", "").strip()
    _deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "").strip()

    missing = [k for k, v in {
        "AZURE_OPENAI_ENDPOINT": endpoint,
        "AZURE_OPENAI_API_KEY": api_key,
        "AZURE_OPENAI_API_VERSION": api_version,
        "AZURE_OPENAI_DEPLOYMENT": _deployment,
    }.items() if not v]
    if missing:
        raise RuntimeError(f"Missing env vars: {missing}")

    _client = AzureOpenAI(
        azure_endpoint=endpoint,
        api_key=api_key,
        api_version=api_version,
    )
    logger.info("init() completed")


def run(raw_data):
    """
    HARD RULES:
    - never throw (avoid 500 -> app crash)
    - always return JSON object
    - always ensure documentDemandPredictions is JSON array of strings
    """
    t0 = time.time()

    # Default minimal safe response (if anything goes wrong)
    safe_doc = {"documentDemandPredictions": []}
    safe_response = {"predictions": safe_doc, "documentDemandPredictions": []}

    try:
        # Parse request
        if raw_data is None:
            logger.warning("Empty request body")
            return safe_response

        if isinstance(raw_data, (bytes, bytearray)):
            raw_data = raw_data.decode("utf-8", errors="ignore")

        if isinstance(raw_data, str):
            req = _safe_json_loads(raw_data)
            if req is None:
                logger.warning("Invalid JSON request")
                return safe_response
        elif isinstance(raw_data, dict):
            req = raw_data
        else:
            logger.warning(f"Unsupported request type: {type(raw_data)}")
            return safe_response

        document = req.get("document", None)
        num_preds = req.get("num_preds", None)

        if document is None or num_preds is None:
            logger.warning("Missing 'document' or 'num_preds'")
            return safe_response

        try:
            topn = int(num_preds)
        except Exception:
            topn = 3

        # Document can be stringified JSON
        if isinstance(document, str):
            parsed = _safe_json_loads(document)
            if isinstance(parsed, dict):
                document = parsed

        if not isinstance(document, dict):
            logger.warning("document is not an object")
            return safe_response

        # Ensure structure exists
        by_id = document.get("contentDomain", {}).get("byId", {})
        if not isinstance(by_id, dict):
            # Still return with empty array to not break app
            document["documentDemandPredictions"] = []
            out = {"predictions": document, "documentDemandPredictions": []}
            return out

        demands_ctx = _demands_context(_demands)

        global_ids: List[str] = []
        global_set = set()

        # Iterate each content chunk (1:1 with legacy)
        for _, content in by_id.items():
            if not isinstance(content, dict):
                continue
            cid = str(content.get("id", "")).strip()
            txt = str(content.get("text", "") or "").replace("\n", " ").strip()

            # Always attach fields expected by UI (even if empty)
            content.setdefault("relevantProba", 0.0)
            content.setdefault("cdLogregPredictions", [])
            content.setdefault("cdTransformerPredictions", [])

            if not cid or not txt:
                continue

            # Call LLM per chunk (simple, stable)
            llm = _call_llm(_prompt(demands_ctx, cid, txt, topn))

            demand_ids = llm.get("demandIds", [])
            # Normalize demandIds -> predictions lists
            normalized = []
            if isinstance(demand_ids, list):
                for di in demand_ids:
                    if not isinstance(di, dict):
                        continue
                    did = str(di.get("id", "")).strip()
                    try:
                        prob = float(di.get("probability", 0.0))
                    except Exception:
                        prob = 0.0
                    if not did or prob < 0.3:
                        continue
                    normalized.append({"label": did, "proba": max(0.0, min(1.0, prob))})

            normalized.sort(key=lambda x: x["proba"], reverse=True)
            normalized = normalized[: max(1, topn)]

            # Attach per chunk (UI/debug)
            content["cdTransformerPredictions"] = normalized
            content["cdLogregPredictions"] = [{"label": p["label"], "proba": p["proba"] * 0.85} for p in normalized]
            content["relevantProba"] = float(normalized[0]["proba"]) if normalized else 0.0

            # Global IDs: ONLY STRINGS (CRITICAL)
            if normalized:
                did = str(normalized[0]["label"]).strip()
                if did and did not in global_set:
                    global_set.add(did)

        global_ids = list(global_set)

        # CRITICAL: documentDemandPredictions MUST be array of strings
        document["documentDemandPredictions"] = _ensure_list_of_strings(global_ids)

        out = {
            "predictions": document,
            # extra top-level fallback (doesn't hurt, helps)
            "documentDemandPredictions": document["documentDemandPredictions"],
        }

        logger.info(f"run() OK in {time.time()-t0:.3f}s, ids={len(document['documentDemandPredictions'])}")
        return out

    except Exception as e:
        # Never fail the endpoint contract; return safe structure
        logger.error(f"run() failed but returning safe response: {str(e)}", exc_info=True)
        return safe_response


